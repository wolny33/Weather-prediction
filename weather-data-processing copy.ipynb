{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install numpy\n",
    "# %pip install scikit-learn\n",
    "# %pip install kagglehub\n",
    "# %pip install cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:24.574362Z",
     "iopub.status.busy": "2024-12-29T16:54:24.573960Z",
     "iopub.status.idle": "2024-12-29T16:54:25.867960Z",
     "shell.execute_reply": "2024-12-29T16:54:25.866721Z",
     "shell.execute_reply.started": "2024-12-29T16:54:24.574325Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\patry\\anaconda3\\envs\\weather\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "import cupy as cp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patry\\AppData\\Local\\Temp\\ipykernel_7224\\263827200.py:30: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  combined_data = combined_data.ffill().bfill().interpolate()\n"
     ]
    }
   ],
   "source": [
    "historical_hourly_weather_data_path = kagglehub.dataset_download('selfishgene/historical-hourly-weather-data')\n",
    "\n",
    "city_attributes = pd.read_csv(f\"{historical_hourly_weather_data_path}/city_attributes.csv\")\n",
    "humidity = pd.read_csv(f\"{historical_hourly_weather_data_path}/humidity.csv\")\n",
    "pressure = pd.read_csv(f\"{historical_hourly_weather_data_path}/pressure.csv\")\n",
    "temperature = pd.read_csv(f\"{historical_hourly_weather_data_path}/temperature.csv\")\n",
    "weather_description = pd.read_csv(f\"{historical_hourly_weather_data_path}/weather_description.csv\")\n",
    "wind_speed = pd.read_csv(f\"{historical_hourly_weather_data_path}/wind_speed.csv\")\n",
    "wind_direction = pd.read_csv(f\"{historical_hourly_weather_data_path}/wind_direction.csv\")\n",
    "\n",
    "data_frames = []\n",
    "for city in city_attributes['City']:\n",
    "    city_data = pd.DataFrame({\n",
    "        'datetime': pd.to_datetime(humidity['datetime']),\n",
    "        'humidity': humidity[city],\n",
    "        'pressure': pressure[city],\n",
    "        'temperature': temperature[city],\n",
    "        'weather_description': weather_description[city],\n",
    "        'wind_speed': wind_speed[city],\n",
    "        'wind_direction': wind_direction[city],\n",
    "        'latitude': city_attributes.loc[city_attributes['City'] == city, 'Latitude'].values[0],\n",
    "        'longitude': city_attributes.loc[city_attributes['City'] == city, 'Longitude'].values[0],\n",
    "        'city': city\n",
    "    })\n",
    "    city_data.set_index('datetime', inplace=True)\n",
    "    data_frames.append(city_data)\n",
    "\n",
    "combined_data = pd.concat(data_frames)\n",
    "\n",
    "combined_data = combined_data.ffill().bfill().interpolate()\n",
    "\n",
    "aggregated_data = combined_data.groupby(['city']).resample('D').agg({\n",
    "    'temperature': 'mean',\n",
    "    'humidity': 'mean',\n",
    "    'wind_speed': ['max', 'mean'],\n",
    "    'pressure': 'mean',\n",
    "    'weather_description': lambda x: x.mode()[0] if not x.mode().empty else np.nan,\n",
    "    'wind_direction': 'mean',\n",
    "    'latitude': 'mean',\n",
    "    'longitude': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "aggregated_data.columns = [\n",
    "    '_'.join(col).strip('_') if isinstance(col, tuple) else col for col in aggregated_data.columns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_weather_description = pd.get_dummies(\n",
    "    aggregated_data['weather_description_<lambda>'],\n",
    "    prefix='weather_desc',\n",
    "    drop_first=False\n",
    ")\n",
    "\n",
    "aggregated_data = pd.concat([aggregated_data, one_hot_weather_description], axis=1)\n",
    "aggregated_data.drop(columns=['weather_description_<lambda>'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_output = True # True better\n",
    "two_models = False # True = 2.165, Flase = 2.151\n",
    "standarize_2nd_data = False # False better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_weather_data(data, binary_output=True, window_size=3, two_models=False, standarize_2nd_data=False):\n",
    "    X, y, X_2, y_2 = [], [], [], []\n",
    "    for i in range(window_size, len(data) - 1):\n",
    "        X_window = data.iloc[i-window_size:i][[\n",
    "            'temperature_mean', 'humidity_mean', 'wind_speed_max',\n",
    "            'wind_speed_mean', 'pressure_mean', 'wind_direction_mean',\n",
    "            'weather_desc_broken clouds', 'weather_desc_dust',\n",
    "            'weather_desc_few clouds', 'weather_desc_fog',\n",
    "            'weather_desc_freezing rain', 'weather_desc_haze',\n",
    "            'weather_desc_heavy intensity rain', 'weather_desc_light rain',\n",
    "            'weather_desc_mist', 'weather_desc_moderate rain',\n",
    "            'weather_desc_overcast clouds', 'weather_desc_scattered clouds',\n",
    "            'weather_desc_sky is clear', 'weather_desc_smoke', 'weather_desc_snow'\n",
    "        ]].values\n",
    "        if two_models is False:\n",
    "            y_target = data.iloc[i + 1][['temperature_mean', 'wind_speed_max']].values\n",
    "        else:\n",
    "            y_target = data.iloc[i][['temperature_mean', 'wind_speed_max']].values\n",
    "            y_target2 = data.iloc[i + 1][['temperature_mean', 'wind_speed_max']].values\n",
    "\n",
    "        if binary_output:\n",
    "            # encode wind speed > 6 as binary\n",
    "            y_target[1] = 1 if y_target[1] >= 6 else 0\n",
    "            if two_models:\n",
    "                y_target2[1] = 1 if y_target2[1] >= 6 else 0\n",
    "        \n",
    "        X.append(X_window)\n",
    "        y.append(y_target)\n",
    "        if two_models:\n",
    "            X_2.append(y_target)\n",
    "            y_2.append(y_target2)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X_2 = np.array(X_2)\n",
    "    y_2 = np.array(y_2)\n",
    "\n",
    "    # split into train/test (0.7 or 0.8)\n",
    "    train_size = int(0.8 * len(X))\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    continuous_indices = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "    X_train_continuous = X_train[:, :, continuous_indices].astype(float) \n",
    "    X_test_continuous = X_test[:, :, continuous_indices].astype(float)\n",
    "\n",
    "    X_mean = X_train_continuous.mean(axis=(0, 1), keepdims=True)\n",
    "    X_std = X_train_continuous.std(axis=(0, 1), keepdims=True)\n",
    "\n",
    "    X_train[:, :, continuous_indices] = (X_train_continuous - X_mean) / (X_std + 1e-9)\n",
    "    X_test[:, :, continuous_indices] = (X_test_continuous - X_mean) / (X_std + 1e-9)\n",
    "\n",
    "    if two_models:\n",
    "        X_train_2, X_test_2 = X_2[:train_size], X_2[train_size:]\n",
    "        y_train_2, y_test_2 = y_2[:train_size], y_2[train_size:]\n",
    "\n",
    "        X_train_2_continuous = X_train_2[:, 0].astype(float) \n",
    "        X_test_2_continuous = X_test_2[:, 0].astype(float)\n",
    "\n",
    "        if standarize_2nd_data:\n",
    "            X_2_mean = X_train_2_continuous.mean(keepdims=True)\n",
    "            X_2_std = X_train_2_continuous.std(keepdims=True)\n",
    "\n",
    "            X_train_2[:, 0] = (X_train_2_continuous - X_2_mean) / (X_2_std + 1e-9)\n",
    "            X_test_2[:, 0] = (X_test_2_continuous - X_2_mean) / (X_2_std + 1e-9)\n",
    "        else:\n",
    "            X_2_mean = None\n",
    "            X_2_std = None\n",
    "    else:\n",
    "        X_train_2 = None\n",
    "        X_test_2 = None\n",
    "        y_train_2 = None\n",
    "        y_test_2 = None\n",
    "        X_2_mean = None\n",
    "        X_2_std = None\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, X_train_2, X_test_2, y_train_2, y_test_2, X_2_mean, X_2_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, X_train_2, X_test_2, y_train_2, y_test_2, X_2_mean, X_2_std = preprocess_weather_data(\n",
    "    aggregated_data, binary_output=binary_output, window_size=3, two_models=two_models, standarize_2nd_data=standarize_2nd_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54342, 3, 21)\n",
      "(54342, 2)\n",
      "(13586, 3, 21)\n",
      "(13586, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "if two_models:\n",
    "    print(X_train_2.shape)\n",
    "    print(y_train_2.shape)\n",
    "    \n",
    "    print(X_test_2.shape)\n",
    "    print(y_test_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=1000, learning_rate=0.001, rate = [500], \n",
    "                       batch_size=256, binary_output=True, model2=None, X_train2=None, X_test2=None, y_train2=None, y_test2=None,\n",
    "                       X_2_std=None, X_2_mean=None, standarize_2nd_data=False, add_noise=False):\n",
    "    X_train_cp = cp.array(X_train.reshape(X_train.shape[0], -1), dtype=cp.float32)\n",
    "    y_train_cp = cp.array(y_train, dtype=cp.float32)\n",
    "    X_test_cp = cp.array(X_test.reshape(X_test.shape[0], -1), dtype=cp.float32)\n",
    "    y_test_cp = cp.array(y_test, dtype=cp.float32)\n",
    "\n",
    "    model.train(X_train_cp, y_train_cp, X_test_cp, y_test_cp, epochs, learning_rate, rate, batch_size=batch_size)\n",
    "\n",
    "    predictions = model.predict(X_test_cp)\n",
    "    \n",
    "    mae = cp.mean(cp.abs(predictions[:, 0] - y_test_cp[:, 0]))\n",
    "\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    if binary_output:\n",
    "        auc = roc_auc_score(cp.asnumpy(y_test_cp[:, 1]), cp.asnumpy(predictions[:, 1]))\n",
    "    else:\n",
    "        auc = roc_auc_score((cp.asnumpy(y_test_cp[:, 1]) >= 6), cp.asnumpy(predictions[:, 1]))\n",
    "\n",
    "    print(f\"Test Regression MAE: {mae}\")\n",
    "    print(f\"Test Classification AUC: {auc}\")\n",
    "\n",
    "    # print(\"Predictions:\" , predictions[:5, :])\n",
    "    # print(\"True values:\", y_test_cp[:5, :])\n",
    "\n",
    "    if model2 is not None:\n",
    "        if add_noise:\n",
    "            predictions_train = model.predict(X_train_cp)\n",
    "            error_mean = cp.mean(cp.abs(predictions_train - y_train_cp), axis=0)\n",
    "            error_std = cp.std(cp.abs(predictions_train - y_train_cp), axis=0)\n",
    "            noise = cp.random.normal(loc=error_mean, scale=error_std, size=X_train_2.shape)\n",
    "\n",
    "        X_train_cp = cp.array(X_train2, dtype=cp.float32)\n",
    "        if add_noise:\n",
    "            X_train_cp = X_train_cp + noise\n",
    "        X_test_cp = cp.array(X_test2, dtype=cp.float32)\n",
    "        y_train_cp = cp.array(y_train2, dtype=cp.float32)\n",
    "        y_test_cp = cp.array(y_test2, dtype=cp.float32)\n",
    "        \n",
    "        model2.train(X_train_cp, y_train_cp, X_test_cp, y_test_cp, epochs, learning_rate * 10, rate, batch_size=batch_size)\n",
    "\n",
    "        X_2_mean_cp = cp.array(X_2_mean, dtype=cp.float32)\n",
    "        X_2_std_cp = cp.array(X_2_std, dtype=cp.float32)\n",
    "        if standarize_2nd_data:\n",
    "            predictions = (predictions - X_2_mean_cp) / (X_2_std_cp + 1e-9)\n",
    "        predictions = model2.predict(cp.array(predictions, dtype=cp.float32))\n",
    "\n",
    "        mae = cp.mean(cp.abs(predictions[:, 0] - y_test_cp[:, 0]))\n",
    "\n",
    "        if binary_output:\n",
    "            auc = roc_auc_score(cp.asnumpy(y_test_cp[:, 1]), cp.asnumpy(predictions[:, 1]))\n",
    "        else:\n",
    "            auc = roc_auc_score((cp.asnumpy(y_test_cp[:, 1]) >= 6), cp.asnumpy(predictions[:, 1]))\n",
    "    \n",
    "        print(f\"Test Regression MAE: {mae}\")\n",
    "        print(f\"Test Classification AUC: {auc}\")\n",
    "    \n",
    "    return mae, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Regression Loss: 279.34344482421875, Classification AUC: 0.56592076773337, Learning Rate: 0.0001\n",
      "Test Regression MAE: 278.7571716308594, Test Classification AUC: 0.587793424085129\n",
      "Test Regression MAE: 2.1387667655944824\n",
      "Test Classification AUC: 0.7361923168891688\n"
     ]
    }
   ],
   "source": [
    "from weather_prediction import WeatherPredictionNetwork\n",
    "# Best parameters: layers=config_1, activations=sigmoid_linear, learning_rate=0.00035369824683149367, batch_size=256, epochs=50 Best MAE: 2.1866440773010254\n",
    "# layers=(63, 1024, 512, 2), activations=('sigmoid', 'relu'), learning_rate=0.00362561763457623, batch_size=166, epochs=723\n",
    "# layers = [X_train.shape[1] * X_train.shape[2], 512, 512, 2]\n",
    "\n",
    "\n",
    "# Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.0001, batch_size=256, epochs=1000\n",
    "# Epoch 0, Regression Loss: 279.34344482421875, Classification AUC: 0.56592076773337, Learning Rate: 0.0001\n",
    "# Test Regression MAE: 278.7571716308594, Test Classification AUC: 0.587793424085129\n",
    "# Test Regression MAE: 2.1387107372283936\n",
    "# Test Classification AUC: 0.7361937317908783\n",
    "# Finished testing with MAE: 2.1387107372283936, AUC: 0.7361937317908783\n",
    "\n",
    "# \"config_0\": (X_train.shape[1] * X_train.shape[2], 64, 32, 2)\n",
    "# Best parameters: layers=config_0, activations=sigmoid_linear, learning_rate=0.0001, batch_size=256, epochs=1000\n",
    "# Best MAE: 2.156092643737793\n",
    "\n",
    "\n",
    "# layers = [X_train.shape[1] * X_train.shape[2], 1024, 512, 2]\n",
    "# activations = [\"sigmoid\", \"relu\"]\n",
    "\n",
    "layers = [X_train.shape[1] * X_train.shape[2], 128, 64, 2]\n",
    "activations = [\"sigmoid\", \"relu\"]\n",
    "model = WeatherPredictionNetwork(layers, activations, binary_output=binary_output, seed=42)\n",
    "\n",
    "if two_models:\n",
    "    model2 = WeatherPredictionNetwork([X_train_2.shape[1], 64, 32, 2], [\"linear\", \"linear\"], binary_output=binary_output, seed=42)\n",
    "    train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=1000, learning_rate=0.0001, rate = [500, 750, 900], \n",
    "                    batch_size=245, binary_output=binary_output, \n",
    "                    model2=model2, X_train2=X_train_2, X_test2=X_test_2, y_train2=y_train_2, y_test2=y_test_2, \n",
    "                    X_2_std=X_2_std, X_2_mean=X_2_mean, standarize_2nd_data=standarize_2nd_data, add_noise=False)\n",
    "else:\n",
    "    # train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=1000, learning_rate=0.0001, rate = [500, 750, 900], \n",
    "    #                 batch_size=245, binary_output=binary_output)\n",
    "    train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=500, learning_rate=0.0001, rate = [500, 750, 900, 1200], \n",
    "                    batch_size=256, binary_output=binary_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import cupy as cp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def hyperparameter_optimization(train_and_evaluate, X_train, y_train, X_test, y_test, binary_output):\n",
    "    # Zakres hiperparametrów do optymalizacji\n",
    "    layer_options = [\n",
    "        [X_train.shape[1] * X_train.shape[2], 128, 64, 2]\n",
    "    ]\n",
    "    activation_options = [\n",
    "        [\"sigmoid\", \"relu\"]\n",
    "    ]\n",
    "    learning_rate_options = [0.001, 0.0005, 0.0001]\n",
    "    batch_size_options = [64, 128, 256]\n",
    "    epochs_options = [100, 500, 1000, 1500]\n",
    "    random_state = [11, 42, 69, 100, 200, 420, 666, 777, 999]\n",
    "\n",
    "    # Lista wszystkich kombinacji hiperparametrów\n",
    "    search_space = list(product(layer_options, activation_options, learning_rate_options, batch_size_options, epochs_options, random_state))\n",
    "\n",
    "    best_mae = float(\"inf\")\n",
    "    best_auc = 0\n",
    "    best_params = None\n",
    "\n",
    "    # Iteracja po wszystkich kombinacjach hiperparametrów\n",
    "    for layers, activations, learning_rate, batch_size, epochs, seed in search_space:\n",
    "        print(f\"Testing layers={layers}, activations={activations}, learning_rate={learning_rate}, batch_size={batch_size}, epochs={epochs}, seed={seed}\")\n",
    "        \n",
    "        # Tworzenie modelu\n",
    "        model = WeatherPredictionNetwork(layers, activations, binary_output=binary_output, seed=seed)\n",
    "\n",
    "        # Trening i ewaluacja\n",
    "        mae, auc = train_and_evaluate(\n",
    "            model, X_train, y_train, X_test, y_test,\n",
    "            epochs=epochs, learning_rate=learning_rate, rate=[500, 750, 900],\n",
    "            batch_size=batch_size, binary_output=binary_output\n",
    "        )\n",
    "\n",
    "        # Aktualizacja najlepszych hiperparametrów\n",
    "        if mae < best_mae or (mae == best_mae and auc > best_auc):\n",
    "            best_mae = mae\n",
    "            best_auc = auc\n",
    "            best_params = (layers, activations, learning_rate, batch_size, epochs)\n",
    "\n",
    "        print(f\"Finished testing with MAE: {mae}, AUC: {auc}\\n\")\n",
    "\n",
    "    print(f\"Best params: layers={best_params[0]}, activations={best_params[1]}, learning_rate={best_params[2]}, batch_size={best_params[3]}, epochs={best_params[4]}\")\n",
    "    print(f\"Best MAE: {best_mae}, Best AUC: {best_auc}\")\n",
    "\n",
    "    return best_params, best_mae, best_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.001, batch_size=64, epochs=100, seed=11\n",
      "Epoch 0, Regression Loss: 3.626224994659424, Classification AUC: 0.5204835948260332, Learning Rate: 0.001\n",
      "Test Regression MAE: 3.351290464401245, Test Classification AUC: 0.5017093449659478\n",
      "Test Regression MAE: 2.558445930480957\n",
      "Test Classification AUC: 0.73350846942477\n",
      "Finished testing with MAE: 2.558445930480957, AUC: 0.73350846942477\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.001, batch_size=64, epochs=100, seed=42\n",
      "Epoch 0, Regression Loss: 4.574060916900635, Classification AUC: 0.6006473312337299, Learning Rate: 0.001\n",
      "Test Regression MAE: 4.118930339813232, Test Classification AUC: 0.590231940857816\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m best_params, best_mae, best_auc \u001b[38;5;241m=\u001b[39m \u001b[43mhyperparameter_optimization\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbinary_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary_output\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 33\u001b[0m, in \u001b[0;36mhyperparameter_optimization\u001b[1;34m(train_and_evaluate, X_train, y_train, X_test, y_test, binary_output)\u001b[0m\n\u001b[0;32m     30\u001b[0m model \u001b[38;5;241m=\u001b[39m WeatherPredictionNetwork(layers, activations, binary_output\u001b[38;5;241m=\u001b[39mbinary_output, seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Trening i ewaluacja\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m mae, auc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m750\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m900\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary_output\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Aktualizacja najlepszych hiperparametrów\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mae \u001b[38;5;241m<\u001b[39m best_mae \u001b[38;5;129;01mor\u001b[39;00m (mae \u001b[38;5;241m==\u001b[39m best_mae \u001b[38;5;129;01mand\u001b[39;00m auc \u001b[38;5;241m>\u001b[39m best_auc):\n",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(model, X_train, y_train, X_test, y_test, epochs, learning_rate, rate, batch_size, binary_output, model2, X_train2, X_test2, y_train2, y_test2, X_2_std, X_2_mean, standarize_2nd_data, add_noise)\u001b[0m\n\u001b[0;32m      6\u001b[0m X_test_cp \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39marray(X_test\u001b[38;5;241m.\u001b[39mreshape(X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mcp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      7\u001b[0m y_test_cp \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39marray(y_test, dtype\u001b[38;5;241m=\u001b[39mcp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_cp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_cp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_cp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_cp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_cp)\n\u001b[0;32m     13\u001b[0m mae \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mmean(cp\u001b[38;5;241m.\u001b[39mabs(predictions[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m y_test_cp[:, \u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\patry\\Documents\\GitHub\\Weather-prediction\\weather_prediction.py:125\u001b[0m, in \u001b[0;36mWeatherPredictionNetwork.train\u001b[1;34m(self, X, y, X_test, y_test, epochs, learning_rate, lower_rate, batch_size)\u001b[0m\n\u001b[0;32m    123\u001b[0m X_batch \u001b[38;5;241m=\u001b[39m X_shuffled[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m    124\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_shuffled[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m--> 125\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward(X_batch, y_batch, output, learning_rate)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# self.clip_weights()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\patry\\Documents\\GitHub\\Weather-prediction\\weather_prediction.py:51\u001b[0m, in \u001b[0;36mWeatherPredictionNetwork.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_values \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m---> 51\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivations_values\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases[i]\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_values\u001b[38;5;241m.\u001b[39mappend(z)\n\u001b[0;32m     53\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_activation(z, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations[i])\n",
      "File \u001b[1;32mc:\\Users\\patry\\anaconda3\\envs\\weather\\lib\\site-packages\\cupy\\linalg\\_product.py:63\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(a, b, out)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a dot product of two arrays.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03mFor arrays with more than one axis, it computes the dot product along the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# TODO(okuta): check type\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_params, best_mae, best_auc = hyperparameter_optimization(\n",
    "    train_and_evaluate,\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    binary_output=binary_output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scikit-optimize\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "import numpy as np\n",
    "\n",
    "def bayesian_optimization(train_and_evaluate, X_train, y_train, X_test, y_test, binary_output):\n",
    "    # Przestrzeń wyszukiwania dla hiperparametrów\n",
    "    space = [\n",
    "        Categorical([\"config_0\", \"config_1\", \"config_2\", \"config_3\"], name=\"layers\"),\n",
    "        Categorical([\"sigmoid_relu\", \"sigmoid_linear\"], name=\"activations\"),  # Zakodowane krotki jako stringi\n",
    "        Real(1e-4, 1e-3, prior=\"log-uniform\", name=\"learning_rate\"),\n",
    "        Categorical([64, 128, 256], name=\"batch_size\"),\n",
    "        Categorical([10, 50, 100], name=\"epochs\")\n",
    "    ]\n",
    "\n",
    "    # Mapowanie z ciągów znaków na rzeczywiste konfiguracje warstw\n",
    "    layers_mapping = {\n",
    "        \"config_0\": (X_train.shape[1] * X_train.shape[2], 64, 32, 2),\n",
    "        \"config_1\": (X_train.shape[1] * X_train.shape[2], 128, 64, 2),\n",
    "        \"config_2\": (X_train.shape[1] * X_train.shape[2], 256, 128, 2),\n",
    "        \"config_3\": (X_train.shape[1] * X_train.shape[2], 512, 256, 2)\n",
    "    }\n",
    "\n",
    "    # Mapowanie string `activations` na rzeczywistą krotkę\n",
    "    activations_mapping = {\n",
    "        \"sigmoid_relu\": (\"sigmoid\", \"relu\"),\n",
    "        \"sigmoid_linear\": (\"sigmoid\", \"linear\")\n",
    "    }\n",
    "\n",
    "    @use_named_args(space)\n",
    "    def objective(layers, activations, learning_rate, batch_size, epochs):\n",
    "        # Mapowanie string `layers` na rzeczywistą konfigurację\n",
    "        mapped_layers = layers_mapping[layers]\n",
    "        # Mapowanie string `activations` na rzeczywistą krotkę\n",
    "        mapped_activations = activations_mapping[activations]\n",
    "        print(f\"Testing: layers={mapped_layers}, activations={mapped_activations}, learning_rate={learning_rate}, batch_size={batch_size}, epochs={epochs}\")\n",
    "\n",
    "        # Tworzenie modelu\n",
    "        model = WeatherPredictionNetwork(list(mapped_layers), list(mapped_activations), binary_output=binary_output, seed=42)\n",
    "\n",
    "        # Trening i ewaluacja\n",
    "        mae, auc = train_and_evaluate(\n",
    "            model, X_train, y_train, X_test, y_test,\n",
    "            epochs=epochs, learning_rate=learning_rate, rate=[500, 750, 900],\n",
    "            batch_size=batch_size, binary_output=binary_output\n",
    "        )\n",
    "\n",
    "        print(f\"MAE: {mae}, AUC: {auc}\")\n",
    "\n",
    "        # Zwróć MAE jako wartość skalarną\n",
    "        return float(mae)\n",
    "\n",
    "    # Optymalizacja przy użyciu Bayesian Optimization\n",
    "    result = gp_minimize(\n",
    "        func=objective,\n",
    "        dimensions=space,\n",
    "        n_calls=30,  # Liczba iteracji optymalizacji (tutaj 20)\n",
    "        n_initial_points=10,\n",
    "        random_state=11,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Najlepsze hiperparametry\n",
    "    best_params = result.x\n",
    "    print(f\"Best parameters: layers={best_params[0]}, activations={best_params[1]}, learning_rate={best_params[2]}, batch_size={best_params[3]}, epochs={best_params[4]}\")\n",
    "    print(f\"Best MAE: {result.fun}\")\n",
    "\n",
    "    return best_params, result.fun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "Testing: layers=(63, 64, 32, 2), activations=('sigmoid', 'linear'), learning_rate=0.0001563563853264126, batch_size=256, epochs=10\n",
      "Epoch 0, Regression Loss: 279.7014465332031, Classification AUC: 0.3740077330856904, Learning Rate: 0.0001563563853264126\n",
      "Test Regression MAE: 279.0731506347656, Test Classification AUC: 0.32755887627091335\n",
      "Test Regression MAE: 5.3514323234558105\n",
      "Test Classification AUC: 0.48150290152124925\n",
      "MAE: 5.3514323234558105, AUC: 0.48150290152124925\n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 4.0550\n",
      "Function value obtained: 5.3514\n",
      "Current minimum: 5.3514\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "Testing: layers=(63, 64, 32, 2), activations=('sigmoid', 'linear'), learning_rate=0.0006701643967251029, batch_size=64, epochs=100\n",
      "Epoch 0, Regression Loss: 3.945028066635132, Classification AUC: 0.46012882260776355, Learning Rate: 0.0006701643967251029\n",
      "Test Regression MAE: 4.092808246612549, Test Classification AUC: 0.4212409444079812\n",
      "Test Regression MAE: 2.4013960361480713\n",
      "Test Classification AUC: 0.7251491859097721\n",
      "MAE: 2.4013960361480713, AUC: 0.7251491859097721\n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 155.0448\n",
      "Function value obtained: 2.4014\n",
      "Current minimum: 2.4014\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "Testing: layers=(63, 256, 128, 2), activations=('sigmoid', 'linear'), learning_rate=0.0006100827168448158, batch_size=128, epochs=100\n",
      "Epoch 0, Regression Loss: 3.4476075172424316, Classification AUC: 0.6525084735261725, Learning Rate: 0.0006100827168448158\n",
      "Test Regression MAE: 3.21465802192688, Test Classification AUC: 0.6329516591756464\n",
      "Test Regression MAE: 3.085024833679199\n",
      "Test Classification AUC: 0.7329939978985172\n",
      "MAE: 3.085024833679199, AUC: 0.7329939978985172\n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 78.8200\n",
      "Function value obtained: 3.0850\n",
      "Current minimum: 2.4014\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "Testing: layers=(63, 256, 128, 2), activations=('sigmoid', 'linear'), learning_rate=0.0009740452025559091, batch_size=256, epochs=10\n",
      "Epoch 0, Regression Loss: 4.053400993347168, Classification AUC: 0.6462431322587998, Learning Rate: 0.0009740452025559091\n",
      "Test Regression MAE: 3.9631614685058594, Test Classification AUC: 0.6289098155799426\n",
      "Test Regression MAE: 3.522167205810547\n",
      "Test Classification AUC: 0.7181837906030364\n",
      "MAE: 3.522167205810547, AUC: 0.7181837906030364\n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 4.2345\n",
      "Function value obtained: 3.5222\n",
      "Current minimum: 2.4014\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "Testing: layers=(63, 512, 256, 2), activations=('sigmoid', 'relu'), learning_rate=0.00015477567507535283, batch_size=64, epochs=100\n",
      "Epoch 0, Regression Loss: 3.4965615272521973, Classification AUC: 0.46946651328409394, Learning Rate: 0.00015477567507535283\n",
      "Test Regression MAE: 3.303788423538208, Test Classification AUC: 0.4802119695201874\n",
      "Test Regression MAE: 2.188033103942871\n",
      "Test Classification AUC: 0.7360833694575425\n",
      "MAE: 2.188033103942871, AUC: 0.7360833694575425\n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 199.7265\n",
      "Function value obtained: 2.1880\n",
      "Current minimum: 2.1880\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "Testing: layers=(63, 64, 32, 2), activations=('sigmoid', 'linear'), learning_rate=0.00030416289532950955, batch_size=256, epochs=10\n",
      "Epoch 0, Regression Loss: 268.52862548828125, Classification AUC: 0.3631592796791875, Learning Rate: 0.00030416289532950955\n",
      "Test Regression MAE: 267.8394775390625, Test Classification AUC: 0.32385056259144906\n",
      "Test Regression MAE: 3.4394235610961914\n",
      "Test Classification AUC: 0.512446835068268\n",
      "MAE: 3.4394235610961914, AUC: 0.512446835068268\n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 4.7539\n",
      "Function value obtained: 3.4394\n",
      "Current minimum: 2.1880\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "Testing: layers=(63, 256, 128, 2), activations=('sigmoid', 'relu'), learning_rate=0.0007900654608497371, batch_size=64, epochs=50\n",
      "Epoch 0, Regression Loss: 3.5220415592193604, Classification AUC: 0.5591857190018117, Learning Rate: 0.0007900654608497371\n",
      "Test Regression MAE: 3.1217427253723145, Test Classification AUC: 0.5343909325670225\n",
      "Test Regression MAE: 4.247553825378418\n",
      "Test Classification AUC: 0.7318005061987729\n",
      "MAE: 4.247553825378418, AUC: 0.7318005061987729\n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 93.5688\n",
      "Function value obtained: 4.2476\n",
      "Current minimum: 2.1880\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "Testing: layers=(63, 128, 64, 2), activations=('sigmoid', 'linear'), learning_rate=0.00030560384759576207, batch_size=128, epochs=50\n",
      "Epoch 0, Regression Loss: 105.98841857910156, Classification AUC: 0.5190748840886297, Learning Rate: 0.00030560384759576207\n",
      "Test Regression MAE: 105.40506744384766, Test Classification AUC: 0.5511736123307269\n",
      "Test Regression MAE: 2.2258920669555664\n",
      "Test Classification AUC: 0.7318478722442798\n",
      "MAE: 2.2258920669555664, AUC: 0.7318478722442798\n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 43.7778\n",
      "Function value obtained: 2.2259\n",
      "Current minimum: 2.1880\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "Testing: layers=(63, 64, 32, 2), activations=('sigmoid', 'relu'), learning_rate=0.00029953885483505383, batch_size=64, epochs=10\n",
      "Epoch 0, Regression Loss: 182.26620483398438, Classification AUC: 0.3723066001205425, Learning Rate: 0.00029953885483505383\n",
      "Test Regression MAE: 181.6501922607422, Test Classification AUC: 0.377862998339436\n",
      "Test Regression MAE: 2.616434097290039\n",
      "Test Classification AUC: 0.5449483322111395\n",
      "MAE: 2.616434097290039, AUC: 0.5449483322111395\n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 18.9467\n",
      "Function value obtained: 2.6164\n",
      "Current minimum: 2.1880\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "Testing: layers=(63, 512, 256, 2), activations=('sigmoid', 'linear'), learning_rate=0.0009733225732961977, batch_size=128, epochs=100\n",
      "Epoch 0, Regression Loss: 5.466731548309326, Classification AUC: 0.438867650369367, Learning Rate: 0.0009733225732961977\n",
      "Test Regression MAE: 5.255596160888672, Test Classification AUC: 0.45755329138857215\n",
      "Test Regression MAE: 6.093013763427734\n",
      "Test Classification AUC: 0.7353299011355117\n",
      "MAE: 6.093013763427734, AUC: 0.7353299011355117\n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 86.0863\n",
      "Function value obtained: 6.0930\n",
      "Current minimum: 2.1880\n",
      "Iteration No: 11 started. Searching for the next optimal point.\n",
      "Testing: layers=(63, 128, 64, 2), activations=('sigmoid', 'relu'), learning_rate=0.001, batch_size=128, epochs=50\n",
      "Epoch 0, Regression Loss: 3.8202707767486572, Classification AUC: 0.6157449523601044, Learning Rate: 0.001\n",
      "Test Regression MAE: 3.590301513671875, Test Classification AUC: 0.6112271146280852\n",
      "Test Regression MAE: 2.505718469619751\n",
      "Test Classification AUC: 0.7196795848961364\n",
      "MAE: 2.505718469619751, AUC: 0.7196795848961364\n",
      "Iteration No: 11 ended. Search finished for the next optimal point.\n",
      "Time taken: 53.2155\n",
      "Function value obtained: 2.5057\n",
      "Current minimum: 2.1880\n",
      "Iteration No: 12 started. Searching for the next optimal point.\n",
      "Testing: layers=(63, 128, 64, 2), activations=('sigmoid', 'linear'), learning_rate=0.00035369824683149367, batch_size=256, epochs=50\n",
      "Epoch 0, Regression Loss: 210.3661651611328, Classification AUC: 0.5037192610155813, Learning Rate: 0.00035369824683149367\n",
      "Test Regression MAE: 209.65135192871094, Test Classification AUC: 0.5416424254209731\n",
      "Test Regression MAE: 2.1866440773010254\n",
      "Test Classification AUC: 0.7223117663403019\n",
      "MAE: 2.1866440773010254, AUC: 0.7223117663403019\n",
      "Iteration No: 12 ended. Search finished for the next optimal point.\n",
      "Time taken: 23.8181\n",
      "Function value obtained: 2.1866\n",
      "Current minimum: 2.1866\n",
      "Iteration No: 13 started. Searching for the next optimal point.\n",
      "Testing: layers=(63, 256, 128, 2), activations=('sigmoid', 'linear'), learning_rate=0.000864202957870249, batch_size=64, epochs=10\n",
      "Epoch 0, Regression Loss: 3.857231378555298, Classification AUC: 0.6875382983612557, Learning Rate: 0.000864202957870249\n",
      "Test Regression MAE: 3.441286087036133, Test Classification AUC: 0.6757858739927447\n",
      "Test Regression MAE: 3.4401614665985107\n",
      "Test Classification AUC: 0.7245397280523542\n",
      "MAE: 3.4401614665985107, AUC: 0.7245397280523542\n",
      "Iteration No: 13 ended. Search finished for the next optimal point.\n",
      "Time taken: 18.2455\n",
      "Function value obtained: 3.4402\n",
      "Current minimum: 2.1866\n",
      "Iteration No: 14 started. Searching for the next optimal point.\n",
      "Testing: layers=(63, 512, 256, 2), activations=('sigmoid', 'linear'), learning_rate=0.0009760271476713135, batch_size=64, epochs=10\n",
      "Epoch 0, Regression Loss: 5.687140464782715, Classification AUC: 0.49663525172936673, Learning Rate: 0.0009760271476713135\n",
      "Test Regression MAE: 5.542597770690918, Test Classification AUC: 0.5424787870861213\n",
      "Test Regression MAE: 4.888155937194824\n",
      "Test Classification AUC: 0.7258859185975212\n",
      "MAE: 4.888155937194824, AUC: 0.7258859185975212\n",
      "Iteration No: 14 ended. Search finished for the next optimal point.\n",
      "Time taken: 17.8924\n",
      "Function value obtained: 4.8882\n",
      "Current minimum: 2.1866\n",
      "Iteration No: 15 started. Searching for the next optimal point.\n",
      "Testing: layers=(63, 512, 256, 2), activations=('sigmoid', 'linear'), learning_rate=0.0001, batch_size=128, epochs=10\n",
      "Epoch 0, Regression Loss: 3.0877602100372314, Classification AUC: 0.37102872081307237, Learning Rate: 0.0001\n",
      "Test Regression MAE: 2.5930306911468506, Test Classification AUC: 0.3860544501930722\n",
      "Test Regression MAE: 2.318650245666504\n",
      "Test Classification AUC: 0.7272581190155185\n",
      "MAE: 2.318650245666504, AUC: 0.7272581190155185\n",
      "Iteration No: 15 ended. Search finished for the next optimal point.\n",
      "Time taken: 9.4528\n",
      "Function value obtained: 2.3187\n",
      "Current minimum: 2.1866\n",
      "Iteration No: 16 started. Searching for the next optimal point.\n",
      "Testing: layers=(63, 64, 32, 2), activations=('sigmoid', 'relu'), learning_rate=0.001, batch_size=64, epochs=10\n",
      "Epoch 0, Regression Loss: 3.950857639312744, Classification AUC: 0.43204152345053304, Learning Rate: 0.001\n",
      "Test Regression MAE: 3.9897263050079346, Test Classification AUC: 0.43942885370146245\n",
      "Test Regression MAE: 2.2317867279052734\n",
      "Test Classification AUC: 0.635482354583919\n",
      "MAE: 2.2317867279052734, AUC: 0.635482354583919\n",
      "Iteration No: 16 ended. Search finished for the next optimal point.\n",
      "Time taken: 17.3247\n",
      "Function value obtained: 2.2318\n",
      "Current minimum: 2.1866\n",
      "Iteration No: 17 started. Searching for the next optimal point.\n",
      "Testing: layers=(63, 128, 64, 2), activations=('sigmoid', 'relu'), learning_rate=0.0003711765429359786, batch_size=256, epochs=100\n",
      "Epoch 0, Regression Loss: 242.57049560546875, Classification AUC: 0.6183476286667423, Learning Rate: 0.0003711765429359786\n",
      "Test Regression MAE: 241.9052734375, Test Classification AUC: 0.6368146724599817\n",
      "Test Regression MAE: 2.2119228839874268\n",
      "Test Classification AUC: 0.7272466450469686\n",
      "MAE: 2.2119228839874268, AUC: 0.7272466450469686\n",
      "Iteration No: 17 ended. Search finished for the next optimal point.\n",
      "Time taken: 44.6951\n",
      "Function value obtained: 2.2119\n",
      "Current minimum: 2.1866\n",
      "Iteration No: 18 started. Searching for the next optimal point.\n",
      "Testing: layers=(63, 512, 256, 2), activations=('sigmoid', 'relu'), learning_rate=0.0001, batch_size=64, epochs=50\n",
      "Epoch 0, Regression Loss: 3.4729905128479004, Classification AUC: 0.4496321824780236, Learning Rate: 0.0001\n",
      "Test Regression MAE: 3.3491291999816895, Test Classification AUC: 0.4553250422746101\n",
      "Test Regression MAE: 2.1976375579833984\n",
      "Test Classification AUC: 0.7321762841957425\n",
      "MAE: 2.1976375579833984, AUC: 0.7321762841957425\n",
      "Iteration No: 18 ended. Search finished for the next optimal point.\n",
      "Time taken: 87.3894\n",
      "Function value obtained: 2.1976\n",
      "Current minimum: 2.1866\n",
      "Iteration No: 19 started. Searching for the next optimal point.\n",
      "Testing: layers=(63, 512, 256, 2), activations=('sigmoid', 'relu'), learning_rate=0.00013562247353901842, batch_size=64, epochs=100\n",
      "Epoch 0, Regression Loss: 3.482011556625366, Classification AUC: 0.45400123393735337, Learning Rate: 0.00013562247353901842\n",
      "Test Regression MAE: 3.3250198364257812, Test Classification AUC: 0.4652417788462644\n",
      "Test Regression MAE: 2.2046093940734863\n",
      "Test Classification AUC: 0.7354979870370242\n",
      "MAE: 2.2046093940734863, AUC: 0.7354979870370242\n",
      "Iteration No: 19 ended. Search finished for the next optimal point.\n",
      "Time taken: 174.6904\n",
      "Function value obtained: 2.2046\n",
      "Current minimum: 2.1866\n",
      "Iteration No: 20 started. Searching for the next optimal point.\n",
      "Testing: layers=(63, 64, 32, 2), activations=('sigmoid', 'relu'), learning_rate=0.0005635361538977361, batch_size=64, epochs=10\n",
      "Epoch 0, Regression Loss: 4.928178310394287, Classification AUC: 0.43111261912970267, Learning Rate: 0.0005635361538977361\n",
      "Test Regression MAE: 5.7440714836120605, Test Classification AUC: 0.45635292415083345\n",
      "Test Regression MAE: 2.412377119064331\n",
      "Test Classification AUC: 0.6153963104315574\n",
      "MAE: 2.412377119064331, AUC: 0.6153963104315574\n",
      "Iteration No: 20 ended. Search finished for the next optimal point.\n",
      "Time taken: 17.1271\n",
      "Function value obtained: 2.4124\n",
      "Current minimum: 2.1866\n",
      "Iteration No: 21 started. Searching for the next optimal point.\n",
      "Testing: layers=(63, 512, 256, 2), activations=('sigmoid', 'relu'), learning_rate=0.00015036718612076224, batch_size=256, epochs=10\n",
      "Epoch 0, Regression Loss: 209.41087341308594, Classification AUC: 0.4320331556489495, Learning Rate: 0.00015036718612076224\n",
      "Test Regression MAE: 208.9885711669922, Test Classification AUC: 0.4459068706388971\n",
      "Test Regression MAE: 2.4786899089813232\n",
      "Test Classification AUC: 0.6454019099050725\n",
      "MAE: 2.4786899089813232, AUC: 0.6454019099050725\n",
      "Iteration No: 21 ended. Search finished for the next optimal point.\n",
      "Time taken: 5.0506\n",
      "Function value obtained: 2.4787\n",
      "Current minimum: 2.1866\n",
      "Iteration No: 22 started. Searching for the next optimal point.\n",
      "Testing: layers=(63, 512, 256, 2), activations=('sigmoid', 'relu'), learning_rate=0.001, batch_size=128, epochs=50\n",
      "Epoch 0, Regression Loss: 4.240937232971191, Classification AUC: 0.48112676508520824, Learning Rate: 0.001\n",
      "Test Regression MAE: 3.9751758575439453, Test Classification AUC: 0.5022307915154711\n",
      "Test Regression MAE: 3.81072735786438\n",
      "Test Classification AUC: 0.7318841512084233\n",
      "MAE: 3.81072735786438, AUC: 0.7318841512084233\n",
      "Iteration No: 22 ended. Search finished for the next optimal point.\n",
      "Time taken: 48.6025\n",
      "Function value obtained: 3.8107\n",
      "Current minimum: 2.1866\n",
      "Iteration No: 23 started. Searching for the next optimal point.\n",
      "Testing: layers=(63, 128, 64, 2), activations=('sigmoid', 'linear'), learning_rate=0.0003986473591965726, batch_size=256, epochs=50\n",
      "Epoch 0, Regression Loss: 195.19900512695312, Classification AUC: 0.5097493313978092, Learning Rate: 0.0003986473591965726\n",
      "Test Regression MAE: 194.45799255371094, Test Classification AUC: 0.5471599120143371\n",
      "Test Regression MAE: 2.2189931869506836\n",
      "Test Classification AUC: 0.7234733564280666\n",
      "MAE: 2.2189931869506836, AUC: 0.7234733564280666\n",
      "Iteration No: 23 ended. Search finished for the next optimal point.\n",
      "Time taken: 20.8749\n",
      "Function value obtained: 2.2190\n",
      "Current minimum: 2.1866\n",
      "Iteration No: 24 started. Searching for the next optimal point.\n",
      "Testing: layers=(63, 64, 32, 2), activations=('sigmoid', 'linear'), learning_rate=0.001, batch_size=128, epochs=10\n",
      "Epoch 0, Regression Loss: 4.152115821838379, Classification AUC: 0.47844515819812705, Learning Rate: 0.001\n",
      "Test Regression MAE: 4.294140815734863, Test Classification AUC: 0.44996569305511414\n",
      "Test Regression MAE: 2.508922815322876\n",
      "Test Classification AUC: 0.6124795016114846\n",
      "MAE: 2.508922815322876, AUC: 0.6124795016114846\n",
      "Iteration No: 24 ended. Search finished for the next optimal point.\n",
      "Time taken: 8.6216\n",
      "Function value obtained: 2.5089\n",
      "Current minimum: 2.1866\n",
      "Iteration No: 25 started. Searching for the next optimal point.\n",
      "Testing: layers=(63, 512, 256, 2), activations=('sigmoid', 'relu'), learning_rate=0.0001744693651761564, batch_size=256, epochs=50\n",
      "Epoch 0, Regression Loss: 191.9982147216797, Classification AUC: 0.4258584960329198, Learning Rate: 0.0001744693651761564\n",
      "Test Regression MAE: 191.64317321777344, Test Classification AUC: 0.4385362275855251\n",
      "Test Regression MAE: 2.3253633975982666\n",
      "Test Classification AUC: 0.7274470747172452\n",
      "MAE: 2.3253633975982666, AUC: 0.7274470747172452\n",
      "Iteration No: 25 ended. Search finished for the next optimal point.\n",
      "Time taken: 23.6209\n",
      "Function value obtained: 2.3254\n",
      "Current minimum: 2.1866\n",
      "Iteration No: 26 started. Searching for the next optimal point.\n",
      "Testing: layers=(63, 64, 32, 2), activations=('sigmoid', 'relu'), learning_rate=0.001, batch_size=64, epochs=100\n",
      "Epoch 0, Regression Loss: 3.950857639312744, Classification AUC: 0.43204152345053304, Learning Rate: 0.001\n",
      "Test Regression MAE: 3.9897263050079346, Test Classification AUC: 0.43942885370146245\n",
      "Test Regression MAE: 2.260023355484009\n",
      "Test Classification AUC: 0.7106210083190915\n",
      "MAE: 2.260023355484009, AUC: 0.7106210083190915\n",
      "Iteration No: 26 ended. Search finished for the next optimal point.\n",
      "Time taken: 171.0670\n",
      "Function value obtained: 2.2600\n",
      "Current minimum: 2.1866\n",
      "Iteration No: 27 started. Searching for the next optimal point.\n",
      "Testing: layers=(63, 128, 64, 2), activations=('sigmoid', 'linear'), learning_rate=0.0003150294148982799, batch_size=256, epochs=50\n",
      "Epoch 0, Regression Loss: 222.39332580566406, Classification AUC: 0.4985378531249379, Learning Rate: 0.0003150294148982799\n",
      "Test Regression MAE: 221.70130920410156, Test Classification AUC: 0.5364612101159105\n",
      "Test Regression MAE: 2.3218603134155273\n",
      "Test Classification AUC: 0.7233675151478492\n",
      "MAE: 2.3218603134155273, AUC: 0.7233675151478492\n",
      "Iteration No: 27 ended. Search finished for the next optimal point.\n",
      "Time taken: 19.2073\n",
      "Function value obtained: 2.3219\n",
      "Current minimum: 2.1866\n",
      "Iteration No: 28 started. Searching for the next optimal point.\n",
      "Testing: layers=(63, 512, 256, 2), activations=('sigmoid', 'linear'), learning_rate=0.0001, batch_size=64, epochs=50\n",
      "Epoch 0, Regression Loss: 3.360775947570801, Classification AUC: 0.44926543897052773, Learning Rate: 0.0001\n",
      "Test Regression MAE: 2.935760498046875, Test Classification AUC: 0.4931059577443142\n",
      "Test Regression MAE: 2.1930510997772217\n",
      "Test Classification AUC: 0.7335499547850473\n",
      "MAE: 2.1930510997772217, AUC: 0.7335499547850473\n",
      "Iteration No: 28 ended. Search finished for the next optimal point.\n",
      "Time taken: 74.2926\n",
      "Function value obtained: 2.1931\n",
      "Current minimum: 2.1866\n",
      "Iteration No: 29 started. Searching for the next optimal point.\n",
      "Testing: layers=(63, 64, 32, 2), activations=('sigmoid', 'linear'), learning_rate=0.001, batch_size=64, epochs=10\n",
      "Epoch 0, Regression Loss: 3.659457206726074, Classification AUC: 0.46623334003417205, Learning Rate: 0.001\n",
      "Test Regression MAE: 3.4761698246002197, Test Classification AUC: 0.42714644568731774\n",
      "Test Regression MAE: 2.9483070373535156\n",
      "Test Classification AUC: 0.6388533136909604\n",
      "MAE: 2.9483070373535156, AUC: 0.6388533136909604\n",
      "Iteration No: 29 ended. Search finished for the next optimal point.\n",
      "Time taken: 15.3418\n",
      "Function value obtained: 2.9483\n",
      "Current minimum: 2.1866\n",
      "Iteration No: 30 started. Searching for the next optimal point.\n",
      "Testing: layers=(63, 512, 256, 2), activations=('sigmoid', 'relu'), learning_rate=0.0001726972450036532, batch_size=64, epochs=50\n",
      "Epoch 0, Regression Loss: 3.297821044921875, Classification AUC: 0.47916353767515796, Learning Rate: 0.0001726972450036532\n",
      "Test Regression MAE: 3.0350406169891357, Test Classification AUC: 0.4901089306296684\n",
      "Test Regression MAE: 2.221066474914551\n",
      "Test Classification AUC: 0.7344449132780213\n",
      "MAE: 2.221066474914551, AUC: 0.7344449132780213\n",
      "Iteration No: 30 ended. Search finished for the next optimal point.\n",
      "Time taken: 85.4831\n",
      "Function value obtained: 2.2211\n",
      "Current minimum: 2.1866\n",
      "Best parameters: layers=config_1, activations=sigmoid_linear, learning_rate=0.00035369824683149367, batch_size=256, epochs=50\n",
      "Best MAE: 2.1866440773010254\n"
     ]
    }
   ],
   "source": [
    "best_params, best_mae = bayesian_optimization(\n",
    "    train_and_evaluate,\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    binary_output=binary_output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_majority_class_percentage(y, binary_output=True):\n",
    "    if binary_output:\n",
    "        binary_labels = y[:, 1]\n",
    "    else:\n",
    "        binary_labels = y[:, 1] >=6\n",
    "    \n",
    "    unique, counts = np.unique(binary_labels, return_counts=True)\n",
    "    class_counts = dict(zip(unique, counts))\n",
    "    \n",
    "    majority_class_count = max(class_counts.values())\n",
    "    total_samples = len(binary_labels)\n",
    "    majority_class_percentage = (majority_class_count / total_samples) * 100\n",
    "    \n",
    "    return majority_class_percentage, class_counts\n",
    "\n",
    "majority_percentage_train, train_class_counts = compute_majority_class_percentage(y_train, binary_output=binary_output)\n",
    "majority_percentage_test, test_class_counts = compute_majority_class_percentage(y_test, binary_output=binary_output)\n",
    "\n",
    "print(f\"Majority class percentage in training data: {majority_percentage_train:.2f}%\")\n",
    "print(f\"Class distribution in training data: {train_class_counts}\")\n",
    "print(f\"Majority class percentage in test data: {majority_percentage_test:.2f}%\")\n",
    "print(f\"Class distribution in test data: {test_class_counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7752,
     "sourceId": 11375,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "weather",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
