{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install numpy\n",
    "# %pip install scikit-learn\n",
    "# %pip install kagglehub\n",
    "# %pip install cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:24.574362Z",
     "iopub.status.busy": "2024-12-29T16:54:24.573960Z",
     "iopub.status.idle": "2024-12-29T16:54:25.867960Z",
     "shell.execute_reply": "2024-12-29T16:54:25.866721Z",
     "shell.execute_reply.started": "2024-12-29T16:54:24.574325Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\patry\\anaconda3\\envs\\weather\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "import kagglehub\n",
    "import cupy as cp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:25.869990Z",
     "iopub.status.busy": "2024-12-29T16:54:25.869419Z",
     "iopub.status.idle": "2024-12-29T16:54:29.158623Z",
     "shell.execute_reply": "2024-12-29T16:54:29.157504Z",
     "shell.execute_reply.started": "2024-12-29T16:54:25.869950Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "historical_hourly_weather_data_path = kagglehub.dataset_download('selfishgene/historical-hourly-weather-data')\n",
    "\n",
    "city = \"Portland\"\n",
    "\n",
    "city_attributes = pd.read_csv(f\"{historical_hourly_weather_data_path}/city_attributes.csv\")\n",
    "humidity = pd.read_csv(f\"{historical_hourly_weather_data_path}/humidity.csv\")\n",
    "pressure = pd.read_csv(f\"{historical_hourly_weather_data_path}/pressure.csv\")\n",
    "temperature = pd.read_csv(f\"{historical_hourly_weather_data_path}/temperature.csv\")\n",
    "weather_description = pd.read_csv(f\"{historical_hourly_weather_data_path}/weather_description.csv\")\n",
    "wind_speed = pd.read_csv(f\"{historical_hourly_weather_data_path}/wind_speed.csv\")\n",
    "wind_direction = pd.read_csv(f\"{historical_hourly_weather_data_path}/wind_direction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:29.161394Z",
     "iopub.status.busy": "2024-12-29T16:54:29.160941Z",
     "iopub.status.idle": "2024-12-29T16:54:29.178700Z",
     "shell.execute_reply": "2024-12-29T16:54:29.177526Z",
     "shell.execute_reply.started": "2024-12-29T16:54:29.161353Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# encoder = LabelEncoder()\n",
    "# aggregated_data['weather_description'] = encoder.fit_transform(aggregated_data['weather_description'])\n",
    "# weather_mapping = dict(enumerate(encoder.classes_))\n",
    "# print(\"Mapping for weather_description:\", weather_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patry\\AppData\\Local\\Temp\\ipykernel_11752\\263827200.py:30: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  combined_data = combined_data.ffill().bfill().interpolate()\n"
     ]
    }
   ],
   "source": [
    "historical_hourly_weather_data_path = kagglehub.dataset_download('selfishgene/historical-hourly-weather-data')\n",
    "\n",
    "city_attributes = pd.read_csv(f\"{historical_hourly_weather_data_path}/city_attributes.csv\")\n",
    "humidity = pd.read_csv(f\"{historical_hourly_weather_data_path}/humidity.csv\")\n",
    "pressure = pd.read_csv(f\"{historical_hourly_weather_data_path}/pressure.csv\")\n",
    "temperature = pd.read_csv(f\"{historical_hourly_weather_data_path}/temperature.csv\")\n",
    "weather_description = pd.read_csv(f\"{historical_hourly_weather_data_path}/weather_description.csv\")\n",
    "wind_speed = pd.read_csv(f\"{historical_hourly_weather_data_path}/wind_speed.csv\")\n",
    "wind_direction = pd.read_csv(f\"{historical_hourly_weather_data_path}/wind_direction.csv\")\n",
    "\n",
    "data_frames = []\n",
    "for city in city_attributes['City']:\n",
    "    city_data = pd.DataFrame({\n",
    "        'datetime': pd.to_datetime(humidity['datetime']),\n",
    "        'humidity': humidity[city],\n",
    "        'pressure': pressure[city],\n",
    "        'temperature': temperature[city],\n",
    "        'weather_description': weather_description[city],\n",
    "        'wind_speed': wind_speed[city],\n",
    "        'wind_direction': wind_direction[city],\n",
    "        'latitude': city_attributes.loc[city_attributes['City'] == city, 'Latitude'].values[0],\n",
    "        'longitude': city_attributes.loc[city_attributes['City'] == city, 'Longitude'].values[0],\n",
    "        'city': city\n",
    "    })\n",
    "    city_data.set_index('datetime', inplace=True)\n",
    "    data_frames.append(city_data)\n",
    "\n",
    "combined_data = pd.concat(data_frames)\n",
    "\n",
    "combined_data = combined_data.ffill().bfill().interpolate()\n",
    "\n",
    "aggregated_data = combined_data.groupby(['city']).resample('D').agg({\n",
    "    'temperature': 'mean',\n",
    "    'humidity': 'mean',\n",
    "    'wind_speed': ['max', 'mean'],\n",
    "    'pressure': 'mean',\n",
    "    'weather_description': lambda x: x.mode()[0] if not x.mode().empty else np.nan,\n",
    "    'wind_direction': 'mean',\n",
    "    'latitude': 'mean',\n",
    "    'longitude': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "aggregated_data.columns = [\n",
    "    '_'.join(col).strip('_') if isinstance(col, tuple) else col for col in aggregated_data.columns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_weather_data_for_single_city(data, window_size=3):\n",
    "#     X, y = [], []\n",
    "#     for i in range(window_size, len(data) - 1):\n",
    "#         X_window = data.iloc[i-window_size:i][[\n",
    "#             'temperature_mean', 'humidity_mean', 'pressure_mean', 'wind_speed_max', 'wind_speed_mean', 'wind_direction_mean'\n",
    "#         ]].values\n",
    "#         y_target = data.iloc[i + 1][['temperature_mean', 'wind_speed_max']].values\n",
    "\n",
    "#         # encode wind speed >= 6 as binary\n",
    "#         y_target[1] = 1 if y_target[1] >= 6 else 0\n",
    "#         X.append(X_window)\n",
    "#         y.append(y_target)\n",
    "\n",
    "#     X = np.array(X)\n",
    "#     y = np.array(y)\n",
    "\n",
    "#     # normalize X\n",
    "#     X_mean = X.mean(axis=(0, 1), keepdims=True)\n",
    "#     X_std = X.std(axis=(0, 1), keepdims=True)\n",
    "#     X_normalized = (X - X_mean) / (X_std + 1e-9)\n",
    "\n",
    "#     return X_normalized, y\n",
    "\n",
    "# train_size = int(0.8 * len(X))\n",
    "# X_train, X_test = X[:train_size], X[train_size:]\n",
    "# y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:52.861230Z",
     "iopub.status.busy": "2024-12-29T16:54:52.860846Z",
     "iopub.status.idle": "2024-12-29T16:54:52.868867Z",
     "shell.execute_reply": "2024-12-29T16:54:52.867778Z",
     "shell.execute_reply.started": "2024-12-29T16:54:52.861203Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(\"Training data windowed:\", X_train.shape, y_train.shape)\n",
    "# print(\"Test data windowed:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_output = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_weather_data(data, binary_output=True, window_size=3):\n",
    "    X, y = [], []\n",
    "    for i in range(window_size, len(data) - 1):\n",
    "        X_window = data.iloc[i-window_size:i][[\n",
    "            'temperature_mean', 'humidity_mean', 'pressure_mean', 'wind_speed_max', 'wind_speed_mean', 'wind_direction_mean'\n",
    "        ]].values\n",
    "        y_target = data.iloc[i + 1][['temperature_mean', 'wind_speed_max']].values\n",
    "\n",
    "        if binary_output:\n",
    "            # encode wind speed > 6 as binary\n",
    "            y_target[1] = 1 if y_target[1] >= 6 else 0\n",
    "        X.append(X_window)\n",
    "        y.append(y_target)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # split into train/test (0.7 or 0.8)\n",
    "    train_size = int(0.7 * len(X))\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    X_mean = X_train.mean(axis=(0, 1), keepdims=True)\n",
    "    X_std = X_train.std(axis=(0, 1), keepdims=True)\n",
    "    X_train = (X_train - X_mean) / (X_std + 1e-9)\n",
    "    X_test = (X_test - X_mean) / (X_std + 1e-9)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_weather_data(aggregated_data, binary_output=binary_output, window_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47549, 3, 6)\n",
      "(47549, 2)\n",
      "(20379, 3, 6)\n",
      "(20379, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=1000, learning_rate=0.001, rate = [500], binary_output=True):\n",
    "    X_train_cp = cp.array(X_train.reshape(X_train.shape[0], -1), dtype=cp.float32)\n",
    "    y_train_cp = cp.array(y_train, dtype=cp.float32)\n",
    "    X_test_cp = cp.array(X_test.reshape(X_test.shape[0], -1), dtype=cp.float32)\n",
    "    y_test_cp = cp.array(y_test, dtype=cp.float32)\n",
    "\n",
    "    model.train(X_train_cp, y_train_cp, X_test_cp, y_test_cp, epochs, learning_rate, rate, batch_size=256)\n",
    "\n",
    "    predictions = model.predict(X_test_cp)\n",
    "    \n",
    "    mae = cp.mean(cp.abs(predictions[:, 0] - y_test_cp[:, 0]))\n",
    "\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    if binary_output:\n",
    "        auc = roc_auc_score(cp.asnumpy(y_test_cp[:, 1]), cp.asnumpy(predictions[:, 1]))\n",
    "    else:\n",
    "        auc = roc_auc_score((cp.asnumpy(y_test_cp[:, 1]) >= 6), cp.asnumpy(predictions[:, 1]))\n",
    "\n",
    "    print(f\"Test Regression MAE: {mae}\")\n",
    "    print(f\"Test Classification AUC: {auc}\")\n",
    "\n",
    "    print(\"Predictions:\" , predictions[:5, :])\n",
    "    print(\"True values:\", y_test_cp[:5, :])\n",
    "\n",
    "    return mae, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Regression Loss: 217.2998809814453, Classification AUC: 0.6126248842427497, Learning Rate: 0.0001\n",
      "Test Regression MAE: 216.2659149169922, Test Classification AUC: 0.6216077195989598\n",
      "Epoch 10, Regression Loss: 2.8194234371185303, Classification AUC: 0.7089153381735274, Learning Rate: 0.0001\n",
      "Epoch 20, Regression Loss: 2.791374444961548, Classification AUC: 0.7221854050011771, Learning Rate: 0.0001\n",
      "Epoch 30, Regression Loss: 2.875065803527832, Classification AUC: 0.72830587853698, Learning Rate: 0.0001\n",
      "Epoch 40, Regression Loss: 2.8084299564361572, Classification AUC: 0.7305391199068036, Learning Rate: 0.0001\n",
      "Epoch 50, Regression Loss: 2.7614238262176514, Classification AUC: 0.7315252570477777, Learning Rate: 0.0001\n",
      "Epoch 60, Regression Loss: 2.9276649951934814, Classification AUC: 0.7318344583675775, Learning Rate: 0.0001\n",
      "Epoch 70, Regression Loss: 2.7557687759399414, Classification AUC: 0.7327100671273852, Learning Rate: 0.0001\n",
      "Epoch 80, Regression Loss: 2.911717414855957, Classification AUC: 0.7336046252408923, Learning Rate: 0.0001\n",
      "Epoch 90, Regression Loss: 2.765193462371826, Classification AUC: 0.7345651599691378, Learning Rate: 0.0001\n",
      "Epoch 100, Regression Loss: 2.7744483947753906, Classification AUC: 0.7350145652750462, Learning Rate: 0.0001\n",
      "Test Regression MAE: 2.434812545776367, Test Classification AUC: 0.7354290076991237\n",
      "Epoch 110, Regression Loss: 2.8362574577331543, Classification AUC: 0.7356644472853905, Learning Rate: 0.0001\n",
      "Epoch 120, Regression Loss: 2.726062774658203, Classification AUC: 0.7360540268001061, Learning Rate: 0.0001\n",
      "Epoch 130, Regression Loss: 2.761143684387207, Classification AUC: 0.7371279284766716, Learning Rate: 0.0001\n",
      "Epoch 140, Regression Loss: 2.737610101699829, Classification AUC: 0.7377809684152759, Learning Rate: 0.0001\n",
      "Epoch 150, Regression Loss: 2.8278584480285645, Classification AUC: 0.7381218783678435, Learning Rate: 0.0001\n",
      "Epoch 160, Regression Loss: 2.734248399734497, Classification AUC: 0.7385295954524406, Learning Rate: 0.0001\n",
      "Epoch 170, Regression Loss: 2.7192697525024414, Classification AUC: 0.739078045520733, Learning Rate: 0.0001\n",
      "Epoch 180, Regression Loss: 2.7736992835998535, Classification AUC: 0.7393831968421432, Learning Rate: 0.0001\n",
      "Epoch 190, Regression Loss: 2.7530360221862793, Classification AUC: 0.7398338055506566, Learning Rate: 0.0001\n",
      "Epoch 200, Regression Loss: 2.726546049118042, Classification AUC: 0.7400185113472659, Learning Rate: 0.0001\n",
      "Test Regression MAE: 2.4219284057617188, Test Classification AUC: 0.738729796669223\n",
      "Epoch 210, Regression Loss: 2.7899553775787354, Classification AUC: 0.7400373135093941, Learning Rate: 0.0001\n",
      "Epoch 220, Regression Loss: 2.7629997730255127, Classification AUC: 0.7404290322082812, Learning Rate: 0.0001\n",
      "Epoch 230, Regression Loss: 2.768810749053955, Classification AUC: 0.7409168000980564, Learning Rate: 0.0001\n",
      "Epoch 240, Regression Loss: 2.880298137664795, Classification AUC: 0.7416604252980028, Learning Rate: 0.0001\n",
      "Epoch 250, Regression Loss: 2.6970512866973877, Classification AUC: 0.7418201861375691, Learning Rate: 0.0001\n",
      "Epoch 260, Regression Loss: 2.6952719688415527, Classification AUC: 0.742338460149577, Learning Rate: 0.0001\n",
      "Epoch 270, Regression Loss: 2.7739241123199463, Classification AUC: 0.7424041075904374, Learning Rate: 0.0001\n",
      "Epoch 280, Regression Loss: 2.6843817234039307, Classification AUC: 0.7428820350546094, Learning Rate: 0.0001\n",
      "Epoch 290, Regression Loss: 2.6898558139801025, Classification AUC: 0.7431459887932799, Learning Rate: 0.0001\n",
      "Epoch 300, Regression Loss: 2.7376608848571777, Classification AUC: 0.7433499945271498, Learning Rate: 0.0001\n",
      "Test Regression MAE: 2.4402475357055664, Test Classification AUC: 0.7415459207325168\n",
      "Epoch 310, Regression Loss: 2.7110865116119385, Classification AUC: 0.743723599742049, Learning Rate: 0.0001\n",
      "Epoch 320, Regression Loss: 2.7259862422943115, Classification AUC: 0.7443696030314684, Learning Rate: 0.0001\n",
      "Epoch 330, Regression Loss: 2.681096076965332, Classification AUC: 0.744424429661653, Learning Rate: 0.0001\n",
      "Epoch 340, Regression Loss: 2.7767157554626465, Classification AUC: 0.744763288744992, Learning Rate: 0.0001\n",
      "Epoch 350, Regression Loss: 2.6793413162231445, Classification AUC: 0.7453282073199529, Learning Rate: 0.0001\n",
      "Epoch 360, Regression Loss: 2.788362741470337, Classification AUC: 0.745146753118967, Learning Rate: 0.0001\n",
      "Epoch 370, Regression Loss: 2.6721155643463135, Classification AUC: 0.7458176121584164, Learning Rate: 0.0001\n",
      "Epoch 380, Regression Loss: 2.668303966522217, Classification AUC: 0.745847281518867, Learning Rate: 0.0001\n",
      "Epoch 390, Regression Loss: 2.699209451675415, Classification AUC: 0.7458457212882099, Learning Rate: 0.0001\n",
      "Epoch 400, Regression Loss: 2.6904714107513428, Classification AUC: 0.7462426693021987, Learning Rate: 0.0001\n",
      "Test Regression MAE: 2.4195497035980225, Test Classification AUC: 0.744011313003903\n",
      "Epoch 410, Regression Loss: 2.6663477420806885, Classification AUC: 0.7462501421736769, Learning Rate: 0.0001\n",
      "Epoch 420, Regression Loss: 2.6703457832336426, Classification AUC: 0.746923725595303, Learning Rate: 0.0001\n",
      "Epoch 430, Regression Loss: 2.683354377746582, Classification AUC: 0.7469136550156064, Learning Rate: 0.0001\n",
      "Epoch 440, Regression Loss: 2.681056499481201, Classification AUC: 0.7471367314247137, Learning Rate: 0.0001\n",
      "Epoch 450, Regression Loss: 2.733510971069336, Classification AUC: 0.7471454870930387, Learning Rate: 0.0001\n",
      "Epoch 460, Regression Loss: 2.767468214035034, Classification AUC: 0.7471756702858844, Learning Rate: 0.0001\n",
      "Epoch 470, Regression Loss: 2.737902879714966, Classification AUC: 0.7474689704556186, Learning Rate: 0.0001\n",
      "Epoch 480, Regression Loss: 2.67582106590271, Classification AUC: 0.7474389398067651, Learning Rate: 0.0001\n",
      "Epoch 490, Regression Loss: 2.6604881286621094, Classification AUC: 0.7479882212844187, Learning Rate: 0.0001\n",
      "Epoch 500, Regression Loss: 2.6443710327148438, Classification AUC: 0.7484555134885018, Learning Rate: 1e-05\n",
      "Test Regression MAE: 2.400942087173462, Test Classification AUC: 0.7446861987725035\n",
      "Epoch 510, Regression Loss: 2.641920328140259, Classification AUC: 0.7485113213958292, Learning Rate: 1e-05\n",
      "Epoch 520, Regression Loss: 2.655971050262451, Classification AUC: 0.7485310878857678, Learning Rate: 1e-05\n",
      "Epoch 530, Regression Loss: 2.6570029258728027, Classification AUC: 0.7485806727118691, Learning Rate: 1e-05\n",
      "Epoch 540, Regression Loss: 2.6395998001098633, Classification AUC: 0.7486342843425368, Learning Rate: 1e-05\n",
      "Epoch 550, Regression Loss: 2.64363169670105, Classification AUC: 0.748714041656642, Learning Rate: 1e-05\n",
      "Epoch 560, Regression Loss: 2.645127773284912, Classification AUC: 0.7487476361256644, Learning Rate: 1e-05\n",
      "Epoch 570, Regression Loss: 2.6429340839385986, Classification AUC: 0.7487946696311942, Learning Rate: 1e-05\n",
      "Epoch 580, Regression Loss: 2.6550400257110596, Classification AUC: 0.7487448439461579, Learning Rate: 1e-05\n",
      "Epoch 590, Regression Loss: 2.6457278728485107, Classification AUC: 0.7488803681323646, Learning Rate: 1e-05\n",
      "Epoch 600, Regression Loss: 2.6384925842285156, Classification AUC: 0.748902611901054, Learning Rate: 1e-05\n",
      "Test Regression MAE: 2.4051132202148438, Test Classification AUC: 0.7444642853091472\n",
      "Epoch 610, Regression Loss: 2.638284206390381, Classification AUC: 0.7488983397772019, Learning Rate: 1e-05\n",
      "Epoch 620, Regression Loss: 2.6411221027374268, Classification AUC: 0.7490281779084006, Learning Rate: 1e-05\n",
      "Epoch 630, Regression Loss: 2.6377034187316895, Classification AUC: 0.7489494286335421, Learning Rate: 1e-05\n",
      "Epoch 640, Regression Loss: 2.637495279312134, Classification AUC: 0.7489369048609882, Learning Rate: 1e-05\n",
      "Epoch 650, Regression Loss: 2.644958257675171, Classification AUC: 0.7491246276387769, Learning Rate: 1e-05\n",
      "Epoch 660, Regression Loss: 2.636781930923462, Classification AUC: 0.7490134533988363, Learning Rate: 1e-05\n",
      "Epoch 670, Regression Loss: 2.6363492012023926, Classification AUC: 0.7491313904224318, Learning Rate: 1e-05\n",
      "Epoch 680, Regression Loss: 2.6350769996643066, Classification AUC: 0.7491277070648654, Learning Rate: 1e-05\n",
      "Epoch 690, Regression Loss: 2.646106719970703, Classification AUC: 0.7492025383677133, Learning Rate: 1e-05\n",
      "Epoch 700, Regression Loss: 2.644085168838501, Classification AUC: 0.7492880227720526, Learning Rate: 1e-05\n",
      "Test Regression MAE: 2.4075498580932617, Test Classification AUC: 0.7445070356872748\n",
      "Epoch 710, Regression Loss: 2.6339471340179443, Classification AUC: 0.7493204165146803, Learning Rate: 1e-05\n",
      "Epoch 720, Regression Loss: 2.640998601913452, Classification AUC: 0.7492921762505775, Learning Rate: 1e-05\n",
      "Epoch 730, Regression Loss: 2.6482667922973633, Classification AUC: 0.7493649638207578, Learning Rate: 1e-05\n",
      "Epoch 740, Regression Loss: 2.6434273719787598, Classification AUC: 0.7493687015946021, Learning Rate: 1e-05\n",
      "Epoch 750, Regression Loss: 2.64509654045105, Classification AUC: 0.7494748320700282, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 760, Regression Loss: 2.636483669281006, Classification AUC: 0.7494560397206714, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 770, Regression Loss: 2.636221408843994, Classification AUC: 0.7494495962981231, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 780, Regression Loss: 2.63653564453125, Classification AUC: 0.7494524081031726, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 790, Regression Loss: 2.6410391330718994, Classification AUC: 0.7494550067034611, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 800, Regression Loss: 2.635857582092285, Classification AUC: 0.7494316068118831, Learning Rate: 1.0000000000000002e-06\n",
      "Test Regression MAE: 2.4029221534729004, Test Classification AUC: 0.7447352083641483\n",
      "Epoch 810, Regression Loss: 2.6379261016845703, Classification AUC: 0.7494370207855017, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 820, Regression Loss: 2.638399124145508, Classification AUC: 0.7494410618631901, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 830, Regression Loss: 2.637005090713501, Classification AUC: 0.7494349806211145, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 840, Regression Loss: 2.6393017768859863, Classification AUC: 0.749440628317107, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 850, Regression Loss: 2.6363143920898438, Classification AUC: 0.749445991442728, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 860, Regression Loss: 2.63675856590271, Classification AUC: 0.749445345583954, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 870, Regression Loss: 2.6369268894195557, Classification AUC: 0.749442743415385, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 880, Regression Loss: 2.6356306076049805, Classification AUC: 0.7494482903074529, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 890, Regression Loss: 2.6368422508239746, Classification AUC: 0.7494594795430924, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 900, Regression Loss: 2.6383211612701416, Classification AUC: 0.7494587052262196, Learning Rate: 1.0000000000000002e-07\n",
      "Test Regression MAE: 2.4048171043395996, Test Classification AUC: 0.7448113830428459\n",
      "Epoch 910, Regression Loss: 2.6383183002471924, Classification AUC: 0.749461035313399, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 920, Regression Loss: 2.636814594268799, Classification AUC: 0.7494620638702587, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 930, Regression Loss: 2.6376543045043945, Classification AUC: 0.7494614215797653, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 940, Regression Loss: 2.637474536895752, Classification AUC: 0.74946090507116, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 950, Regression Loss: 2.6372499465942383, Classification AUC: 0.7494619416566509, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 960, Regression Loss: 2.6376960277557373, Classification AUC: 0.7494588934530171, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 970, Regression Loss: 2.6375622749328613, Classification AUC: 0.7494582841691183, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 980, Regression Loss: 2.6378772258758545, Classification AUC: 0.7494597873072872, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 990, Regression Loss: 2.637186288833618, Classification AUC: 0.7494622012490588, Learning Rate: 1.0000000000000002e-07\n",
      "Test Regression MAE: 2.4041919708251953\n",
      "Test Classification AUC: 0.744797152994473\n",
      "Predictions: [[2.9245050e+02 1.3614365e-01]\n",
      " [2.9204739e+02 1.1311164e-01]\n",
      " [2.9314651e+02 1.0880973e-01]\n",
      " [2.9233646e+02 1.2109037e-01]\n",
      " [2.9202518e+02 1.0751853e-01]]\n",
      "True values: [[293.89233   0.     ]\n",
      " [293.2227    0.     ]\n",
      " [291.94113   0.     ]\n",
      " [291.89948   0.     ]\n",
      " [292.71707   0.     ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(2.404192, dtype=float32), np.float64(0.744797152994473))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from weather_prediction import WeatherPredictionNetwork\n",
    "\n",
    "layers = [X_train.shape[1] * X_train.shape[2],512, 512, 2]\n",
    "activations = [\"sigmoid\", \"relu\"]\n",
    "model = WeatherPredictionNetwork(layers, activations, binary_output=binary_output, seed=42)\n",
    "\n",
    "train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=1000, learning_rate=0.0001, rate = [500, 750, 900], binary_output=binary_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority class percentage in training data: 54.58%\n",
      "Class distribution in training data: {0: np.int64(25950), 1: np.int64(21599)}\n",
      "Majority class percentage in test data: 60.07%\n",
      "Class distribution in test data: {0: np.int64(12242), 1: np.int64(8137)}\n"
     ]
    }
   ],
   "source": [
    "def compute_majority_class_percentage(y, binary_output=True):\n",
    "    if binary_output:\n",
    "        binary_labels = y[:, 1]\n",
    "    else:\n",
    "        binary_labels = y[:, 1] >=6\n",
    "    \n",
    "    unique, counts = np.unique(binary_labels, return_counts=True)\n",
    "    class_counts = dict(zip(unique, counts))\n",
    "    \n",
    "    majority_class_count = max(class_counts.values())\n",
    "    total_samples = len(binary_labels)\n",
    "    majority_class_percentage = (majority_class_count / total_samples) * 100\n",
    "    \n",
    "    return majority_class_percentage, class_counts\n",
    "\n",
    "majority_percentage_train, train_class_counts = compute_majority_class_percentage(y_train, binary_output=binary_output)\n",
    "majority_percentage_test, test_class_counts = compute_majority_class_percentage(y_test, binary_output=binary_output)\n",
    "\n",
    "print(f\"Majority class percentage in training data: {majority_percentage_train:.2f}%\")\n",
    "print(f\"Class distribution in training data: {train_class_counts}\")\n",
    "print(f\"Majority class percentage in test data: {majority_percentage_test:.2f}%\")\n",
    "print(f\"Class distribution in test data: {test_class_counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_data(X, y):\n",
    "#     X = X.reshape(X.shape[0], -1)  # Flattening the time-series data\n",
    "#     X = cp.array(X, dtype=cp.float32)\n",
    "#     y = cp.array(y, dtype=cp.float32)\n",
    "#     return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from network_test import Network\n",
    "# # Dataset preparation\n",
    "# X_train = np.random.rand(1504, 7, 5)  # Training data (1504 samples, 7 days, 5 features per day)\n",
    "# y_train = np.random.rand(1504, 2)     # Training targets (1504 samples, 2 output features)\n",
    "# X_test = np.random.rand(377, 7, 5)    # Test data\n",
    "# y_test = np.random.rand(377, 2)       # Test targets\n",
    "\n",
    "# # Preprocess the data\n",
    "# X_train, y_train = preprocess_data(X_train, y_train)\n",
    "# X_test, y_test = preprocess_data(X_test, y_test)\n",
    "\n",
    "# # Initialize and train the network\n",
    "# activations = [\"relu\", \"relu\", \"relu\"] \n",
    "# nn = Network([X_train.shape[1], 64, 32, y_train.shape[1]], activations, loss_function=\"mse\", seed=123)\n",
    "# nn.train(X_train, y_train, epochs=500, learning_rate=0.001)\n",
    "\n",
    "# # Predict and evaluate\n",
    "# predictions = nn.predict(X_test)\n",
    "# print(\"Predictions (first 5):\", predictions[:5].get())\n",
    "# print(\"Actual (first 5):\", y_test[:5].get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cupy as cp\n",
    "# from network_test import Network\n",
    "\n",
    "# # Dataset preparation\n",
    "# X_train = np.random.rand(1504, 7, 5)  # Training data (1504 samples, 7 days, 5 features per day)\n",
    "# y_train = np.random.rand(1504, 2)     # Training targets (1504 samples, 2 output features)\n",
    "# X_test = np.random.rand(377, 7, 5)    # Test data\n",
    "# y_test = np.random.rand(377, 2)       # Test targets\n",
    "\n",
    "# # Preprocess the data (no flattening for time-series)\n",
    "# def preprocess_data(X, y):\n",
    "#     # Flattening the time-series data to (samples, days*features)\n",
    "#     X = X.reshape(X.shape[0], -1)  # Now shape is (samples, days*features)\n",
    "#     X = cp.array(X, dtype=cp.float32)\n",
    "#     y = cp.array(y, dtype=cp.float32)\n",
    "#     return X, y\n",
    "\n",
    "# # Preprocess training and testing data\n",
    "# X_train, y_train = preprocess_data(X_train, y_train)\n",
    "# X_test, y_test = preprocess_data(X_test, y_test)\n",
    "\n",
    "# # Initialize and train the network\n",
    "# activations = [\"relu\", \"relu\", \"relu\"]  # Relu for hidden layers\n",
    "# nn = Network([X_train.shape[1], 64, 32, y_train.shape[1]], activations, loss_function=\"mse\", seed=123)\n",
    "\n",
    "# # Train the network\n",
    "# nn.train(X_train, y_train, epochs=500, learning_rate=0.001)\n",
    "\n",
    "# # Predict and evaluate\n",
    "# predictions = nn.predict(X_test)\n",
    "# print(\"Predictions (first 5):\", predictions[:5].get())\n",
    "# print(\"Actual (first 5):\", y_test[:5].get())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import cupy as cp\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# class NeuralNetwork:\n",
    "#     def __init__(self, layers, activation=\"relu\", loss=\"mse\"):\n",
    "#         self.layers = layers\n",
    "#         self.activation = activation\n",
    "#         self.loss = loss\n",
    "#         self.weights = []\n",
    "#         self.biases = []\n",
    "#         self.initialize_weights()\n",
    "\n",
    "#     def initialize_weights(self):\n",
    "#         for i in range(len(self.layers) - 1):\n",
    "#             weight = cp.random.randn(self.layers[i], self.layers[i + 1]) * cp.sqrt(2. / self.layers[i])\n",
    "#             bias = cp.zeros((1, self.layers[i + 1]))\n",
    "#             self.weights.append(weight)\n",
    "#             self.biases.append(bias)\n",
    "\n",
    "#     def activate(self, z, activation):\n",
    "#         if activation == \"sigmoid\":\n",
    "#             return 1 / (1 + cp.exp(-z))\n",
    "#         elif activation == \"relu\":\n",
    "#             return cp.maximum(0, z)\n",
    "#         elif activation == \"linear\":\n",
    "#             return z\n",
    "#         else:\n",
    "#             raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "#     def activate_derivative(self, a, activation):\n",
    "#         if activation == \"sigmoid\":\n",
    "#             return a * (1 - a)\n",
    "#         elif activation == \"relu\":\n",
    "#             return (a > 0).astype(cp.float32)\n",
    "#         elif activation == \"linear\":\n",
    "#             return cp.ones_like(a)\n",
    "#         else:\n",
    "#             raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         activations = [X]\n",
    "#         z_values = []\n",
    "\n",
    "#         for i in range(len(self.weights)):\n",
    "#             z = cp.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "#             a = self.activate(z, \"linear\" if i == len(self.weights) - 1 else self.activation)\n",
    "#             z_values.append(z)\n",
    "#             activations.append(a)\n",
    "\n",
    "#         return activations, z_values\n",
    "\n",
    "#     def backward(self, X, y, activations, z_values):\n",
    "#         deltas = []\n",
    "#         delta = activations[-1] - y\n",
    "#         deltas.append(delta)\n",
    "\n",
    "#         for i in reversed(range(len(self.weights) - 1)):\n",
    "#             delta = cp.dot(deltas[0], self.weights[i + 1].T) * self.activate_derivative(activations[i + 1], self.activation)\n",
    "#             deltas.insert(0, delta)\n",
    "\n",
    "#         weight_grads = []\n",
    "#         bias_grads = []\n",
    "\n",
    "#         for i in range(len(self.weights)):\n",
    "#             weight_grad = cp.dot(activations[i].T, deltas[i]) / X.shape[0]\n",
    "#             bias_grad = cp.mean(deltas[i], axis=0, keepdims=True)\n",
    "#             weight_grads.append(weight_grad)\n",
    "#             bias_grads.append(bias_grad)\n",
    "\n",
    "#         return weight_grads, bias_grads\n",
    "\n",
    "#     def update_parameters(self, weight_grads, bias_grads, learning_rate):\n",
    "#         for i in range(len(self.weights)):\n",
    "#             self.weights[i] -= learning_rate * weight_grads[i]\n",
    "#             self.biases[i] -= learning_rate * bias_grads[i]\n",
    "\n",
    "#     def fit(self, X, y, epochs=1000, learning_rate=0.01):\n",
    "#         for epoch in range(epochs):\n",
    "#             activations, z_values = self.forward(X)\n",
    "#             weight_grads, bias_grads = self.backward(X, y, activations, z_values)\n",
    "#             self.update_parameters(weight_grads, bias_grads, learning_rate)\n",
    "\n",
    "#             if epoch % 100 == 0:\n",
    "#                 loss = self.calculate_loss(y, activations[-1])\n",
    "#                 print(f\"Epoch {epoch}/{epochs} - Loss: {loss}\")\n",
    "\n",
    "#     def calculate_loss(self, y, y_pred):\n",
    "#         if self.loss == \"mse\":\n",
    "#             return cp.mean((y - y_pred) ** 2)\n",
    "#         else:\n",
    "#             raise ValueError(\"Unsupported loss function\")\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         activations, _ = self.forward(X)\n",
    "#         return activations[-1]\n",
    "\n",
    "# def preprocess_data(X, y):\n",
    "#     X = X.reshape(X.shape[0], -1)  # Flatten time-series data\n",
    "#     mean_X = np.mean(X, axis=0)\n",
    "#     std_X = np.std(X, axis=0)\n",
    "#     X_normalized = (X - mean_X) / std_X  # Normalize inputs\n",
    "\n",
    "#     mean_y = np.mean(y, axis=0)\n",
    "#     std_y = np.std(y, axis=0)\n",
    "#     y_normalized = (y - mean_y) / std_y  # Normalize targets\n",
    "\n",
    "#     return (\n",
    "#         cp.array(X_normalized, dtype=cp.float32),\n",
    "#         cp.array(y_normalized, dtype=cp.float32),\n",
    "#         mean_X, std_X,\n",
    "#         mean_y, std_y\n",
    "#     )\n",
    "\n",
    "# def denormalize(predictions, mean_y, std_y):\n",
    "#     mean_y = cp.asnumpy(mean_y)  # Convert cupy to numpy\n",
    "#     std_y = cp.asnumpy(std_y)    # Convert cupy to numpy\n",
    "#     predictions = np.array(predictions)  # Ensure numpy array\n",
    "#     return predictions * std_y + mean_y\n",
    "\n",
    "\n",
    "\n",
    "# # Dataset preparation\n",
    "# # X_train = np.random.rand(1504, 7, 5)\n",
    "# # y_train = np.random.rand(1504, 2)\n",
    "# # X_test = np.random.rand(377, 7, 5)\n",
    "# # y_test = np.random.rand(377, 2)\n",
    "\n",
    "# # Preprocess the data\n",
    "# X_train, y_train, mean_X, std_X, mean_y, std_y = preprocess_data(X_train, y_train)\n",
    "# X_test, y_test, _, _, _, _ = preprocess_data(X_test, y_test)\n",
    "\n",
    "# # Initialize the neural network\n",
    "# nn = NeuralNetwork(\n",
    "#     layers=[X_train.shape[1], 64, 32, y_train.shape[1]],  # Ensure 2 outputs for temperature and wind speed\n",
    "#     activation=\"relu\",\n",
    "#     loss=\"mse\",\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# nn.fit(X_train, y_train, epochs=15000, learning_rate=0.01)\n",
    "\n",
    "# # Predict normalized values\n",
    "# predictions_normalized = nn.predict(X_test).get()\n",
    "\n",
    "# # Denormalize the predictions\n",
    "# predictions_denormalized = denormalize(predictions_normalized, mean_y, std_y)\n",
    "\n",
    "# # Print results\n",
    "# print(\"Predictions (first 5):\", predictions_denormalized[:5])\n",
    "# print(\"Actual (first 5):\", y_test[:5].get())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7752,
     "sourceId": 11375,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "weather",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
