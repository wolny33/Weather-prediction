{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install numpy\n",
    "# %pip install scikit-learn\n",
    "# %pip install kagglehub\n",
    "# %pip install cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:24.574362Z",
     "iopub.status.busy": "2024-12-29T16:54:24.573960Z",
     "iopub.status.idle": "2024-12-29T16:54:25.867960Z",
     "shell.execute_reply": "2024-12-29T16:54:25.866721Z",
     "shell.execute_reply.started": "2024-12-29T16:54:24.574325Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\patry\\anaconda3\\envs\\weather\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "import cupy as cp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:25.869990Z",
     "iopub.status.busy": "2024-12-29T16:54:25.869419Z",
     "iopub.status.idle": "2024-12-29T16:54:29.158623Z",
     "shell.execute_reply": "2024-12-29T16:54:29.157504Z",
     "shell.execute_reply.started": "2024-12-29T16:54:25.869950Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "historical_hourly_weather_data_path = kagglehub.dataset_download('selfishgene/historical-hourly-weather-data')\n",
    "\n",
    "city = \"Portland\"\n",
    "\n",
    "city_attributes = pd.read_csv(f\"{historical_hourly_weather_data_path}/city_attributes.csv\")\n",
    "humidity = pd.read_csv(f\"{historical_hourly_weather_data_path}/humidity.csv\")\n",
    "pressure = pd.read_csv(f\"{historical_hourly_weather_data_path}/pressure.csv\")\n",
    "temperature = pd.read_csv(f\"{historical_hourly_weather_data_path}/temperature.csv\")\n",
    "weather_description = pd.read_csv(f\"{historical_hourly_weather_data_path}/weather_description.csv\")\n",
    "wind_speed = pd.read_csv(f\"{historical_hourly_weather_data_path}/wind_speed.csv\")\n",
    "wind_direction = pd.read_csv(f\"{historical_hourly_weather_data_path}/wind_direction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patry\\AppData\\Local\\Temp\\ipykernel_2896\\263827200.py:30: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  combined_data = combined_data.ffill().bfill().interpolate()\n"
     ]
    }
   ],
   "source": [
    "historical_hourly_weather_data_path = kagglehub.dataset_download('selfishgene/historical-hourly-weather-data')\n",
    "\n",
    "city_attributes = pd.read_csv(f\"{historical_hourly_weather_data_path}/city_attributes.csv\")\n",
    "humidity = pd.read_csv(f\"{historical_hourly_weather_data_path}/humidity.csv\")\n",
    "pressure = pd.read_csv(f\"{historical_hourly_weather_data_path}/pressure.csv\")\n",
    "temperature = pd.read_csv(f\"{historical_hourly_weather_data_path}/temperature.csv\")\n",
    "weather_description = pd.read_csv(f\"{historical_hourly_weather_data_path}/weather_description.csv\")\n",
    "wind_speed = pd.read_csv(f\"{historical_hourly_weather_data_path}/wind_speed.csv\")\n",
    "wind_direction = pd.read_csv(f\"{historical_hourly_weather_data_path}/wind_direction.csv\")\n",
    "\n",
    "data_frames = []\n",
    "for city in city_attributes['City']:\n",
    "    city_data = pd.DataFrame({\n",
    "        'datetime': pd.to_datetime(humidity['datetime']),\n",
    "        'humidity': humidity[city],\n",
    "        'pressure': pressure[city],\n",
    "        'temperature': temperature[city],\n",
    "        'weather_description': weather_description[city],\n",
    "        'wind_speed': wind_speed[city],\n",
    "        'wind_direction': wind_direction[city],\n",
    "        'latitude': city_attributes.loc[city_attributes['City'] == city, 'Latitude'].values[0],\n",
    "        'longitude': city_attributes.loc[city_attributes['City'] == city, 'Longitude'].values[0],\n",
    "        'city': city\n",
    "    })\n",
    "    city_data.set_index('datetime', inplace=True)\n",
    "    data_frames.append(city_data)\n",
    "\n",
    "combined_data = pd.concat(data_frames)\n",
    "\n",
    "combined_data = combined_data.ffill().bfill().interpolate()\n",
    "\n",
    "aggregated_data = combined_data.groupby(['city']).resample('D').agg({\n",
    "    'temperature': 'mean',\n",
    "    'humidity': 'mean',\n",
    "    'wind_speed': ['max', 'mean'],\n",
    "    'pressure': 'mean',\n",
    "    'weather_description': lambda x: x.mode()[0] if not x.mode().empty else np.nan,\n",
    "    'wind_direction': 'mean',\n",
    "    'latitude': 'mean',\n",
    "    'longitude': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "aggregated_data.columns = [\n",
    "    '_'.join(col).strip('_') if isinstance(col, tuple) else col for col in aggregated_data.columns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_weather_description = pd.get_dummies(\n",
    "    aggregated_data['weather_description_<lambda>'],\n",
    "    prefix='weather_desc',\n",
    "    drop_first=False\n",
    ")\n",
    "\n",
    "aggregated_data = pd.concat([aggregated_data, one_hot_weather_description], axis=1)\n",
    "aggregated_data.drop(columns=['weather_description_<lambda>'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_output = True # True better\n",
    "two_models = False # True = 2.165, Flase = 2.151\n",
    "standarize_2nd_data = False # False better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_weather_data(data, binary_output=True, window_size=3, two_models=False, standarize_2nd_data=False):\n",
    "    X, y, X_2, y_2 = [], [], [], []\n",
    "    for i in range(window_size, len(data) - 1):\n",
    "        X_window = data.iloc[i-window_size:i][[\n",
    "            'temperature_mean', 'humidity_mean', 'wind_speed_max',\n",
    "            'wind_speed_mean', 'pressure_mean', 'wind_direction_mean',\n",
    "            'weather_desc_broken clouds', 'weather_desc_dust',\n",
    "            'weather_desc_few clouds', 'weather_desc_fog',\n",
    "            'weather_desc_freezing rain', 'weather_desc_haze',\n",
    "            'weather_desc_heavy intensity rain', 'weather_desc_light rain',\n",
    "            'weather_desc_mist', 'weather_desc_moderate rain',\n",
    "            'weather_desc_overcast clouds', 'weather_desc_scattered clouds',\n",
    "            'weather_desc_sky is clear', 'weather_desc_smoke', 'weather_desc_snow'\n",
    "        ]].values\n",
    "        if two_models is False:\n",
    "            y_target = data.iloc[i + 1][['temperature_mean', 'wind_speed_max']].values\n",
    "        else:\n",
    "            y_target = data.iloc[i][['temperature_mean', 'wind_speed_max']].values\n",
    "            y_target2 = data.iloc[i + 1][['temperature_mean', 'wind_speed_max']].values\n",
    "\n",
    "        if binary_output:\n",
    "            # encode wind speed > 6 as binary\n",
    "            y_target[1] = 1 if y_target[1] >= 6 else 0\n",
    "            if two_models:\n",
    "                y_target2[1] = 1 if y_target2[1] >= 6 else 0\n",
    "        \n",
    "        X.append(X_window)\n",
    "        y.append(y_target)\n",
    "        if two_models:\n",
    "            X_2.append(y_target)\n",
    "            y_2.append(y_target2)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X_2 = np.array(X_2)\n",
    "    y_2 = np.array(y_2)\n",
    "\n",
    "    # split into train/test (0.7 or 0.8)\n",
    "    train_size = int(0.8 * len(X))\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    continuous_indices = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "    X_train_continuous = X_train[:, :, continuous_indices].astype(float) \n",
    "    X_test_continuous = X_test[:, :, continuous_indices].astype(float)\n",
    "\n",
    "    X_mean = X_train_continuous.mean(axis=(0, 1), keepdims=True)\n",
    "    X_std = X_train_continuous.std(axis=(0, 1), keepdims=True)\n",
    "\n",
    "    X_train[:, :, continuous_indices] = (X_train_continuous - X_mean) / (X_std + 1e-9)\n",
    "    X_test[:, :, continuous_indices] = (X_test_continuous - X_mean) / (X_std + 1e-9)\n",
    "\n",
    "    if two_models:\n",
    "        X_train_2, X_test_2 = X_2[:train_size], X_2[train_size:]\n",
    "        y_train_2, y_test_2 = y_2[:train_size], y_2[train_size:]\n",
    "\n",
    "        X_train_2_continuous = X_train_2[:, 0].astype(float) \n",
    "        X_test_2_continuous = X_test_2[:, 0].astype(float)\n",
    "\n",
    "        if standarize_2nd_data:\n",
    "            X_2_mean = X_train_2_continuous.mean(keepdims=True)\n",
    "            X_2_std = X_train_2_continuous.std(keepdims=True)\n",
    "\n",
    "            X_train_2[:, 0] = (X_train_2_continuous - X_2_mean) / (X_2_std + 1e-9)\n",
    "            X_test_2[:, 0] = (X_test_2_continuous - X_2_mean) / (X_2_std + 1e-9)\n",
    "        else:\n",
    "            X_2_mean = None\n",
    "            X_2_std = None\n",
    "    else:\n",
    "        X_train_2 = None\n",
    "        X_test_2 = None\n",
    "        y_train_2 = None\n",
    "        y_test_2 = None\n",
    "        X_2_mean = None\n",
    "        X_2_std = None\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, X_train_2, X_test_2, y_train_2, y_test_2, X_2_mean, X_2_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, X_train_2, X_test_2, y_train_2, y_test_2, X_2_mean, X_2_std = preprocess_weather_data(\n",
    "    aggregated_data, binary_output=binary_output, window_size=3, two_models=two_models, standarize_2nd_data=standarize_2nd_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54342, 3, 21)\n",
      "(54342, 2)\n",
      "(13586, 3, 21)\n",
      "(13586, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "if two_models:\n",
    "    print(X_train_2.shape)\n",
    "    print(y_train_2.shape)\n",
    "    \n",
    "    print(X_test_2.shape)\n",
    "    print(y_test_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=1000, learning_rate=0.001, rate = [500], \n",
    "                       batch_size=256, binary_output=True, model2=None, X_train2=None, X_test2=None, y_train2=None, y_test2=None,\n",
    "                       X_2_std=None, X_2_mean=None, standarize_2nd_data=False, add_noise=False):\n",
    "    X_train_cp = cp.array(X_train.reshape(X_train.shape[0], -1), dtype=cp.float32)\n",
    "    y_train_cp = cp.array(y_train, dtype=cp.float32)\n",
    "    X_test_cp = cp.array(X_test.reshape(X_test.shape[0], -1), dtype=cp.float32)\n",
    "    y_test_cp = cp.array(y_test, dtype=cp.float32)\n",
    "\n",
    "    model.train(X_train_cp, y_train_cp, X_test_cp, y_test_cp, epochs, learning_rate, rate, batch_size=batch_size)\n",
    "\n",
    "    predictions = model.predict(X_test_cp)\n",
    "    \n",
    "    mae = cp.mean(cp.abs(predictions[:, 0] - y_test_cp[:, 0]))\n",
    "\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    if binary_output:\n",
    "        auc = roc_auc_score(cp.asnumpy(y_test_cp[:, 1]), cp.asnumpy(predictions[:, 1]))\n",
    "    else:\n",
    "        auc = roc_auc_score((cp.asnumpy(y_test_cp[:, 1]) >= 6), cp.asnumpy(predictions[:, 1]))\n",
    "\n",
    "    print(f\"Test Regression MAE: {mae}\")\n",
    "    print(f\"Test Classification AUC: {auc}\")\n",
    "\n",
    "    # print(\"Predictions:\" , predictions[:5, :])\n",
    "    # print(\"True values:\", y_test_cp[:5, :])\n",
    "\n",
    "    if model2 is not None:\n",
    "        if add_noise:\n",
    "            predictions_train = model.predict(X_train_cp)\n",
    "            error_mean = cp.mean(cp.abs(predictions_train - y_train_cp), axis=0)\n",
    "            error_std = cp.std(cp.abs(predictions_train - y_train_cp), axis=0)\n",
    "            noise = cp.random.normal(loc=error_mean, scale=error_std, size=X_train_2.shape)\n",
    "\n",
    "        X_train_cp = cp.array(X_train2, dtype=cp.float32)\n",
    "        if add_noise:\n",
    "            X_train_cp = X_train_cp + noise\n",
    "        X_test_cp = cp.array(X_test2, dtype=cp.float32)\n",
    "        y_train_cp = cp.array(y_train2, dtype=cp.float32)\n",
    "        y_test_cp = cp.array(y_test2, dtype=cp.float32)\n",
    "        \n",
    "        model2.train(X_train_cp, y_train_cp, X_test_cp, y_test_cp, epochs, learning_rate * 10, rate, batch_size=batch_size)\n",
    "\n",
    "        X_2_mean_cp = cp.array(X_2_mean, dtype=cp.float32)\n",
    "        X_2_std_cp = cp.array(X_2_std, dtype=cp.float32)\n",
    "        if standarize_2nd_data:\n",
    "            predictions = (predictions - X_2_mean_cp) / (X_2_std_cp + 1e-9)\n",
    "        predictions = model2.predict(cp.array(predictions, dtype=cp.float32))\n",
    "\n",
    "        mae = cp.mean(cp.abs(predictions[:, 0] - y_test_cp[:, 0]))\n",
    "\n",
    "        if binary_output:\n",
    "            auc = roc_auc_score(cp.asnumpy(y_test_cp[:, 1]), cp.asnumpy(predictions[:, 1]))\n",
    "        else:\n",
    "            auc = roc_auc_score((cp.asnumpy(y_test_cp[:, 1]) >= 6), cp.asnumpy(predictions[:, 1]))\n",
    "    \n",
    "        print(f\"Test Regression MAE: {mae}\")\n",
    "        print(f\"Test Classification AUC: {auc}\")\n",
    "    \n",
    "    return mae, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Regression Loss: 192.01336669921875, Classification AUC: 0.4946325597163322, Learning Rate: 0.0001\n",
      "Test Regression MAE: 191.5661163330078, Test Classification AUC: 0.5564088370869792\n",
      "Epoch 10, Regression Loss: 2.9188144207000732, Classification AUC: 0.7094694369565815, Learning Rate: 0.0001\n",
      "Epoch 20, Regression Loss: 2.908524751663208, Classification AUC: 0.7272396786548674, Learning Rate: 0.0001\n",
      "Epoch 30, Regression Loss: 2.808375597000122, Classification AUC: 0.7303512651912571, Learning Rate: 0.0001\n",
      "Epoch 40, Regression Loss: 2.8991143703460693, Classification AUC: 0.730729329559092, Learning Rate: 0.0001\n",
      "Epoch 50, Regression Loss: 2.9227960109710693, Classification AUC: 0.7328753435403139, Learning Rate: 0.0001\n",
      "Epoch 60, Regression Loss: 2.839613676071167, Classification AUC: 0.7316957810907894, Learning Rate: 0.0001\n",
      "Epoch 70, Regression Loss: 2.8230133056640625, Classification AUC: 0.7334988340991411, Learning Rate: 0.0001\n",
      "Epoch 80, Regression Loss: 2.7896931171417236, Classification AUC: 0.7334379029512929, Learning Rate: 0.0001\n",
      "Epoch 90, Regression Loss: 2.8135879039764404, Classification AUC: 0.7351378318766597, Learning Rate: 0.0001\n",
      "Epoch 100, Regression Loss: 2.776475191116333, Classification AUC: 0.7376037266526653, Learning Rate: 0.0001\n",
      "Test Regression MAE: 2.227506160736084, Test Classification AUC: 0.7333323804854635\n",
      "Epoch 110, Regression Loss: 2.732630491256714, Classification AUC: 0.7388529461022643, Learning Rate: 0.0001\n",
      "Epoch 120, Regression Loss: 2.742535352706909, Classification AUC: 0.7401780777860059, Learning Rate: 0.0001\n",
      "Epoch 130, Regression Loss: 2.8074517250061035, Classification AUC: 0.7415473294183238, Learning Rate: 0.0001\n",
      "Epoch 140, Regression Loss: 2.8144681453704834, Classification AUC: 0.7425656749271382, Learning Rate: 0.0001\n",
      "Epoch 150, Regression Loss: 2.8012747764587402, Classification AUC: 0.7436531297078849, Learning Rate: 0.0001\n",
      "Epoch 160, Regression Loss: 2.7184500694274902, Classification AUC: 0.7439909364008704, Learning Rate: 0.0001\n",
      "Epoch 170, Regression Loss: 2.723245859146118, Classification AUC: 0.7449367096489077, Learning Rate: 0.0001\n",
      "Epoch 180, Regression Loss: 2.7191479206085205, Classification AUC: 0.7449516020815878, Learning Rate: 0.0001\n",
      "Epoch 190, Regression Loss: 2.746612310409546, Classification AUC: 0.7458393869319724, Learning Rate: 0.0001\n",
      "Epoch 200, Regression Loss: 2.716350793838501, Classification AUC: 0.745691109295485, Learning Rate: 0.0001\n",
      "Test Regression MAE: 2.1639673709869385, Test Classification AUC: 0.7365262336925946\n",
      "Epoch 210, Regression Loss: 2.6997721195220947, Classification AUC: 0.7468584760315649, Learning Rate: 0.0001\n",
      "Epoch 220, Regression Loss: 2.7046961784362793, Classification AUC: 0.7465900658757104, Learning Rate: 0.0001\n",
      "Epoch 230, Regression Loss: 2.726274013519287, Classification AUC: 0.7470395555052387, Learning Rate: 0.0001\n",
      "Epoch 240, Regression Loss: 2.7113404273986816, Classification AUC: 0.7474457677391428, Learning Rate: 0.0001\n",
      "Epoch 250, Regression Loss: 2.7103946208953857, Classification AUC: 0.7479751047236965, Learning Rate: 0.0001\n",
      "Epoch 260, Regression Loss: 2.7142581939697266, Classification AUC: 0.7484180051218203, Learning Rate: 0.0001\n",
      "Epoch 270, Regression Loss: 2.784954071044922, Classification AUC: 0.7483662912008806, Learning Rate: 0.0001\n",
      "Epoch 280, Regression Loss: 2.7737984657287598, Classification AUC: 0.7490433842087973, Learning Rate: 0.0001\n",
      "Epoch 290, Regression Loss: 2.7192208766937256, Classification AUC: 0.749002966407653, Learning Rate: 0.0001\n",
      "Epoch 300, Regression Loss: 2.6780054569244385, Classification AUC: 0.748632459588604, Learning Rate: 0.0001\n",
      "Test Regression MAE: 2.1599297523498535, Test Classification AUC: 0.7397090993036384\n",
      "Epoch 310, Regression Loss: 2.7538092136383057, Classification AUC: 0.7491147696107264, Learning Rate: 0.0001\n",
      "Epoch 320, Regression Loss: 2.861997604370117, Classification AUC: 0.7500684144750036, Learning Rate: 0.0001\n",
      "Epoch 330, Regression Loss: 2.719897508621216, Classification AUC: 0.7500524636968535, Learning Rate: 0.0001\n",
      "Epoch 340, Regression Loss: 2.7471320629119873, Classification AUC: 0.7498680332540173, Learning Rate: 0.0001\n",
      "Epoch 350, Regression Loss: 2.754556179046631, Classification AUC: 0.7498526446359934, Learning Rate: 0.0001\n",
      "Epoch 360, Regression Loss: 2.9977991580963135, Classification AUC: 0.7498247434881213, Learning Rate: 0.0001\n",
      "Epoch 370, Regression Loss: 2.668407917022705, Classification AUC: 0.7504827581008539, Learning Rate: 0.0001\n",
      "Epoch 380, Regression Loss: 2.686758279800415, Classification AUC: 0.7510613775607163, Learning Rate: 0.0001\n",
      "Epoch 390, Regression Loss: 2.66752552986145, Classification AUC: 0.7515531975728449, Learning Rate: 0.0001\n",
      "Epoch 400, Regression Loss: 2.7249138355255127, Classification AUC: 0.7516768384387205, Learning Rate: 0.0001\n",
      "Test Regression MAE: 2.2287442684173584, Test Classification AUC: 0.740669574378111\n",
      "Epoch 410, Regression Loss: 2.6637964248657227, Classification AUC: 0.7516974363156146, Learning Rate: 0.0001\n",
      "Epoch 420, Regression Loss: 2.665790319442749, Classification AUC: 0.7520713167826574, Learning Rate: 0.0001\n",
      "Epoch 430, Regression Loss: 2.709540367126465, Classification AUC: 0.7520299526557264, Learning Rate: 0.0001\n",
      "Epoch 440, Regression Loss: 2.7361996173858643, Classification AUC: 0.7526289205253365, Learning Rate: 0.0001\n",
      "Epoch 450, Regression Loss: 2.655188798904419, Classification AUC: 0.7521460916390389, Learning Rate: 0.0001\n",
      "Epoch 460, Regression Loss: 2.7437071800231934, Classification AUC: 0.7528340643009364, Learning Rate: 0.0001\n",
      "Epoch 470, Regression Loss: 2.6619441509246826, Classification AUC: 0.753250222233306, Learning Rate: 0.0001\n",
      "Epoch 480, Regression Loss: 2.654611110687256, Classification AUC: 0.7534154711953605, Learning Rate: 0.0001\n",
      "Epoch 490, Regression Loss: 2.6463510990142822, Classification AUC: 0.7528233633286582, Learning Rate: 0.0001\n",
      "Epoch 500, Regression Loss: 2.6312613487243652, Classification AUC: 0.753438823519425, Learning Rate: 1e-05\n",
      "Test Regression MAE: 2.1452012062072754, Test Classification AUC: 0.7376496988116418\n",
      "Epoch 510, Regression Loss: 2.6276562213897705, Classification AUC: 0.7538895302462435, Learning Rate: 1e-05\n",
      "Epoch 520, Regression Loss: 2.6291697025299072, Classification AUC: 0.7539670506157102, Learning Rate: 1e-05\n",
      "Epoch 530, Regression Loss: 2.6427340507507324, Classification AUC: 0.7540175598096315, Learning Rate: 1e-05\n",
      "Epoch 540, Regression Loss: 2.6335201263427734, Classification AUC: 0.7540348397047545, Learning Rate: 1e-05\n",
      "Epoch 550, Regression Loss: 2.642714738845825, Classification AUC: 0.7540142617577426, Learning Rate: 1e-05\n",
      "Epoch 560, Regression Loss: 2.6284701824188232, Classification AUC: 0.7540691039829537, Learning Rate: 1e-05\n",
      "Epoch 570, Regression Loss: 2.6369221210479736, Classification AUC: 0.7541487321092747, Learning Rate: 1e-05\n",
      "Epoch 580, Regression Loss: 2.6372389793395996, Classification AUC: 0.7542053845175813, Learning Rate: 1e-05\n",
      "Epoch 590, Regression Loss: 2.6271674633026123, Classification AUC: 0.7542172208059087, Learning Rate: 1e-05\n",
      "Epoch 600, Regression Loss: 2.6426382064819336, Classification AUC: 0.7542886646230098, Learning Rate: 1e-05\n",
      "Test Regression MAE: 2.1682968139648438, Test Classification AUC: 0.7382256743465764\n",
      "Epoch 610, Regression Loss: 2.626546859741211, Classification AUC: 0.7542844793477433, Learning Rate: 1e-05\n",
      "Epoch 620, Regression Loss: 2.627117395401001, Classification AUC: 0.7543611894644122, Learning Rate: 1e-05\n",
      "Epoch 630, Regression Loss: 2.6338019371032715, Classification AUC: 0.7544296424247057, Learning Rate: 1e-05\n",
      "Epoch 640, Regression Loss: 2.6381239891052246, Classification AUC: 0.7543642895919536, Learning Rate: 1e-05\n",
      "Epoch 650, Regression Loss: 2.6257476806640625, Classification AUC: 0.7544254489025913, Learning Rate: 1e-05\n",
      "Epoch 660, Regression Loss: 2.6238372325897217, Classification AUC: 0.7544616264494839, Learning Rate: 1e-05\n",
      "Epoch 670, Regression Loss: 2.6389567852020264, Classification AUC: 0.7544431803126322, Learning Rate: 1e-05\n",
      "Epoch 680, Regression Loss: 2.6222684383392334, Classification AUC: 0.7545288568145886, Learning Rate: 1e-05\n",
      "Epoch 690, Regression Loss: 2.6260151863098145, Classification AUC: 0.7545632234911481, Learning Rate: 1e-05\n",
      "Epoch 700, Regression Loss: 2.6289961338043213, Classification AUC: 0.754581136331841, Learning Rate: 1e-05\n",
      "Test Regression MAE: 2.158881425857544, Test Classification AUC: 0.7378209350802417\n",
      "Epoch 710, Regression Loss: 2.6248626708984375, Classification AUC: 0.7545271552149893, Learning Rate: 1e-05\n",
      "Epoch 720, Regression Loss: 2.6289541721343994, Classification AUC: 0.7546537154649915, Learning Rate: 1e-05\n",
      "Epoch 730, Regression Loss: 2.628150701522827, Classification AUC: 0.7546523107519136, Learning Rate: 1e-05\n",
      "Epoch 740, Regression Loss: 2.6293721199035645, Classification AUC: 0.7546153415076309, Learning Rate: 1e-05\n",
      "Epoch 750, Regression Loss: 2.626985788345337, Classification AUC: 0.7546649799718703, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 760, Regression Loss: 2.624656915664673, Classification AUC: 0.7546793274253576, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 770, Regression Loss: 2.628051519393921, Classification AUC: 0.7546871124496954, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 780, Regression Loss: 2.6230721473693848, Classification AUC: 0.7546897445686234, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 790, Regression Loss: 2.6223840713500977, Classification AUC: 0.754689264189738, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 800, Regression Loss: 2.626335859298706, Classification AUC: 0.7546928556919617, Learning Rate: 1.0000000000000002e-06\n",
      "Test Regression MAE: 2.1540703773498535, Test Classification AUC: 0.737473897274245\n",
      "Epoch 810, Regression Loss: 2.623603343963623, Classification AUC: 0.7546935408675677, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 820, Regression Loss: 2.6241931915283203, Classification AUC: 0.7547052589510764, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 830, Regression Loss: 2.6248390674591064, Classification AUC: 0.7547078134121873, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 840, Regression Loss: 2.621565103530884, Classification AUC: 0.7547198998548983, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 850, Regression Loss: 2.6225674152374268, Classification AUC: 0.7547235504595313, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 860, Regression Loss: 2.6227641105651855, Classification AUC: 0.7547236006278556, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 870, Regression Loss: 2.6233420372009277, Classification AUC: 0.7547225938251848, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 880, Regression Loss: 2.622283458709717, Classification AUC: 0.754732184221957, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 890, Regression Loss: 2.621417284011841, Classification AUC: 0.7547415134685485, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 900, Regression Loss: 2.6224753856658936, Classification AUC: 0.7547424295558933, Learning Rate: 1.0000000000000002e-07\n",
      "Test Regression MAE: 2.1511826515197754, Test Classification AUC: 0.7374433995100549\n",
      "Epoch 910, Regression Loss: 2.62255597114563, Classification AUC: 0.754744138715103, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 920, Regression Loss: 2.6232311725616455, Classification AUC: 0.7547464457707794, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 930, Regression Loss: 2.622586488723755, Classification AUC: 0.7547454540873297, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 940, Regression Loss: 2.622877359390259, Classification AUC: 0.7547461530076821, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 950, Regression Loss: 2.6223561763763428, Classification AUC: 0.7547473274962585, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 960, Regression Loss: 2.622868299484253, Classification AUC: 0.7547471158271646, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 970, Regression Loss: 2.622810125350952, Classification AUC: 0.7547484779315288, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 980, Regression Loss: 2.623537302017212, Classification AUC: 0.7547479123352161, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 990, Regression Loss: 2.6228926181793213, Classification AUC: 0.7547472986322912, Learning Rate: 1.0000000000000002e-07\n",
      "Test Regression MAE: 2.1514065265655518\n",
      "Test Classification AUC: 0.7373832440795649\n"
     ]
    }
   ],
   "source": [
    "from weather_prediction import WeatherPredictionNetwork\n",
    "\n",
    "layers = [X_train.shape[1] * X_train.shape[2], 512, 512, 2]\n",
    "activations = [\"sigmoid\", \"relu\"]\n",
    "model = WeatherPredictionNetwork(layers, activations, binary_output=binary_output, seed=42)\n",
    "\n",
    "if two_models:\n",
    "    model2 = WeatherPredictionNetwork([X_train_2.shape[1], 64, 32, 2], [\"linear\", \"linear\"], binary_output=binary_output, seed=42)\n",
    "    train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=1000, learning_rate=0.0001, rate = [500, 750, 900], \n",
    "                    batch_size=245, binary_output=binary_output, \n",
    "                    model2=model2, X_train2=X_train_2, X_test2=X_test_2, y_train2=y_train_2, y_test2=y_test_2, \n",
    "                    X_2_std=X_2_std, X_2_mean=X_2_mean, standarize_2nd_data=standarize_2nd_data, add_noise=False)\n",
    "else:\n",
    "    train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=1000, learning_rate=0.0001, rate = [500, 750, 900], \n",
    "                    batch_size=245, binary_output=binary_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority class percentage in training data: 56.02%\n",
      "Class distribution in training data: {0: np.int64(30444), 1: np.int64(23898)}\n",
      "Majority class percentage in test data: 57.03%\n",
      "Class distribution in test data: {0: np.int64(7748), 1: np.int64(5838)}\n"
     ]
    }
   ],
   "source": [
    "def compute_majority_class_percentage(y, binary_output=True):\n",
    "    if binary_output:\n",
    "        binary_labels = y[:, 1]\n",
    "    else:\n",
    "        binary_labels = y[:, 1] >=6\n",
    "    \n",
    "    unique, counts = np.unique(binary_labels, return_counts=True)\n",
    "    class_counts = dict(zip(unique, counts))\n",
    "    \n",
    "    majority_class_count = max(class_counts.values())\n",
    "    total_samples = len(binary_labels)\n",
    "    majority_class_percentage = (majority_class_count / total_samples) * 100\n",
    "    \n",
    "    return majority_class_percentage, class_counts\n",
    "\n",
    "majority_percentage_train, train_class_counts = compute_majority_class_percentage(y_train, binary_output=binary_output)\n",
    "majority_percentage_test, test_class_counts = compute_majority_class_percentage(y_test, binary_output=binary_output)\n",
    "\n",
    "print(f\"Majority class percentage in training data: {majority_percentage_train:.2f}%\")\n",
    "print(f\"Class distribution in training data: {train_class_counts}\")\n",
    "print(f\"Majority class percentage in test data: {majority_percentage_test:.2f}%\")\n",
    "print(f\"Class distribution in test data: {test_class_counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7752,
     "sourceId": 11375,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "weather",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
