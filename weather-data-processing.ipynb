{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install numpy\n",
    "# %pip install scikit-learn\n",
    "# %pip install kagglehub\n",
    "# %pip install cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:24.574362Z",
     "iopub.status.busy": "2024-12-29T16:54:24.573960Z",
     "iopub.status.idle": "2024-12-29T16:54:25.867960Z",
     "shell.execute_reply": "2024-12-29T16:54:25.866721Z",
     "shell.execute_reply.started": "2024-12-29T16:54:24.574325Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import kagglehub\n",
    "import cupy as cp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:25.869990Z",
     "iopub.status.busy": "2024-12-29T16:54:25.869419Z",
     "iopub.status.idle": "2024-12-29T16:54:29.158623Z",
     "shell.execute_reply": "2024-12-29T16:54:29.157504Z",
     "shell.execute_reply.started": "2024-12-29T16:54:25.869950Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "historical_hourly_weather_data_path = kagglehub.dataset_download('selfishgene/historical-hourly-weather-data')\n",
    "\n",
    "city = \"Portland\"\n",
    "\n",
    "city_attributes = pd.read_csv(f\"{historical_hourly_weather_data_path}/city_attributes.csv\")\n",
    "humidity = pd.read_csv(f\"{historical_hourly_weather_data_path}/humidity.csv\")\n",
    "pressure = pd.read_csv(f\"{historical_hourly_weather_data_path}/pressure.csv\")\n",
    "temperature = pd.read_csv(f\"{historical_hourly_weather_data_path}/temperature.csv\")\n",
    "weather_description = pd.read_csv(f\"{historical_hourly_weather_data_path}/weather_description.csv\")\n",
    "wind_speed = pd.read_csv(f\"{historical_hourly_weather_data_path}/wind_speed.csv\")\n",
    "wind_direction = pd.read_csv(f\"{historical_hourly_weather_data_path}/wind_direction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:29.161394Z",
     "iopub.status.busy": "2024-12-29T16:54:29.160941Z",
     "iopub.status.idle": "2024-12-29T16:54:29.178700Z",
     "shell.execute_reply": "2024-12-29T16:54:29.177526Z",
     "shell.execute_reply.started": "2024-12-29T16:54:29.161353Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if city not in city_attributes['City'].values:\n",
    "    raise ValueError(f\"City '{city}' does not exist in the data. Available cities are: {city_attributes['City'].unique()}\")\n",
    "\n",
    "selected_city = city_attributes[city_attributes['City'] == city].index[0]\n",
    "data_frames = [humidity, pressure, temperature, weather_description, wind_speed, wind_direction]\n",
    "\n",
    "for i, df in enumerate(data_frames):\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    data_frames[i] = df.iloc[:, selected_city]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:29.180658Z",
     "iopub.status.busy": "2024-12-29T16:54:29.180324Z",
     "iopub.status.idle": "2024-12-29T16:54:29.674041Z",
     "shell.execute_reply": "2024-12-29T16:54:29.673014Z",
     "shell.execute_reply.started": "2024-12-29T16:54:29.180632Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "combined_data = pd.concat(data_frames, axis=1)\n",
    "combined_data.columns = [\n",
    "    'humidity', 'pressure', 'temperature', 'weather_description', \n",
    "    'wind_speed', 'wind_direction'\n",
    "]\n",
    "combined_data.index = pd.to_datetime(combined_data.index)\n",
    "# aggregate daily\n",
    "aggregated_data = combined_data.resample('D').agg({\n",
    "    'temperature': 'mean',\n",
    "    'humidity': 'mean',\n",
    "    'wind_speed': ['max', 'mean'],\n",
    "    'pressure': 'mean',\n",
    "    'weather_description': lambda x: x.mode()[0] if not x.mode().empty else np.nan,\n",
    "    'wind_direction': 'mean'\n",
    "})\n",
    "aggregated_data.columns = ['_'.join(col).strip('_') for col in aggregated_data.columns.values]\n",
    "aggregated_data = aggregated_data.rename(columns={\n",
    "    'temperature_mean': 'mean_temperature',\n",
    "    'wind_speed_max': 'max_wind_speed'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:29.675192Z",
     "iopub.status.busy": "2024-12-29T16:54:29.674948Z",
     "iopub.status.idle": "2024-12-29T16:54:29.680544Z",
     "shell.execute_reply": "2024-12-29T16:54:29.679146Z",
     "shell.execute_reply.started": "2024-12-29T16:54:29.675171Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def get_wind_direction_for_max_speed(group):\n",
    "#     max_wind_speed_idx = group['wind_speed'].idxmax()\n",
    "#     return group.loc[max_wind_speed_idx, 'wind_direction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:29.681920Z",
     "iopub.status.busy": "2024-12-29T16:54:29.681630Z",
     "iopub.status.idle": "2024-12-29T16:54:30.109542Z",
     "shell.execute_reply": "2024-12-29T16:54:30.108502Z",
     "shell.execute_reply.started": "2024-12-29T16:54:29.681896Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# aggregated_data.rename(columns={'wind_speed': 'wind_speed_max'}, inplace=True)\n",
    "\n",
    "# aggregated_data['wind_direction'] = combined_data.groupby(combined_data.index.date).apply(get_wind_direction_for_max_speed)\n",
    "\n",
    "# aggregated_data['wind_speed_mean'] = combined_data['wind_speed'].resample('D').mean()\n",
    "\n",
    "# encoder = LabelEncoder()\n",
    "# aggregated_data['weather_description'] = encoder.fit_transform(aggregated_data['weather_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:30.110815Z",
     "iopub.status.busy": "2024-12-29T16:54:30.110545Z",
     "iopub.status.idle": "2024-12-29T16:54:30.116792Z",
     "shell.execute_reply": "2024-12-29T16:54:30.115861Z",
     "shell.execute_reply.started": "2024-12-29T16:54:30.110788Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# weather_mapping = dict(enumerate(encoder.classes_))\n",
    "# print(\"Mapping for weather_description:\", weather_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:30.119864Z",
     "iopub.status.busy": "2024-12-29T16:54:30.119559Z",
     "iopub.status.idle": "2024-12-29T16:54:30.142206Z",
     "shell.execute_reply": "2024-12-29T16:54:30.141099Z",
     "shell.execute_reply.started": "2024-12-29T16:54:30.119840Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# aggregated_data['mean_temperature_next_day'] = aggregated_data['temperature'].shift(-1)\n",
    "# aggregated_data['max_wind_speed_next_day'] = aggregated_data['wind_speed_max'].shift(-1)\n",
    "\n",
    "# aggregated_data = aggregated_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:30.144158Z",
     "iopub.status.busy": "2024-12-29T16:54:30.143864Z",
     "iopub.status.idle": "2024-12-29T16:54:30.153837Z",
     "shell.execute_reply": "2024-12-29T16:54:30.152886Z",
     "shell.execute_reply.started": "2024-12-29T16:54:30.144133Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# X = aggregated_data.drop(columns=['mean_temperature_next_day', 'max_wind_speed_next_day'])\n",
    "# y = aggregated_data[['mean_temperature_next_day', 'max_wind_speed_next_day']]\n",
    "\n",
    "# train_size = int(0.8 * len(aggregated_data))\n",
    "# X_train, X_test = X[:train_size], X[train_size:]\n",
    "# y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:30.155495Z",
     "iopub.status.busy": "2024-12-29T16:54:30.155061Z",
     "iopub.status.idle": "2024-12-29T16:54:30.172300Z",
     "shell.execute_reply": "2024-12-29T16:54:30.171295Z",
     "shell.execute_reply.started": "2024-12-29T16:54:30.155450Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(\"Train data:\", X_train.shape, y_train.shape)\n",
    "# print(\"Test data:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:30.173978Z",
     "iopub.status.busy": "2024-12-29T16:54:30.173528Z",
     "iopub.status.idle": "2024-12-29T16:54:30.195625Z",
     "shell.execute_reply": "2024-12-29T16:54:30.194359Z",
     "shell.execute_reply.started": "2024-12-29T16:54:30.173940Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:30.197273Z",
     "iopub.status.busy": "2024-12-29T16:54:30.196868Z",
     "iopub.status.idle": "2024-12-29T16:54:30.207111Z",
     "shell.execute_reply": "2024-12-29T16:54:30.205597Z",
     "shell.execute_reply.started": "2024-12-29T16:54:30.197244Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:30.208969Z",
     "iopub.status.busy": "2024-12-29T16:54:30.208583Z",
     "iopub.status.idle": "2024-12-29T16:54:30.224534Z",
     "shell.execute_reply": "2024-12-29T16:54:30.223181Z",
     "shell.execute_reply.started": "2024-12-29T16:54:30.208941Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def create_time_windows(data, window_size=5):\n",
    "#     X, y = [], []\n",
    "#     for i in range(window_size, len(data)):\n",
    "#         X.append(data.iloc[i-window_size:i][['temperature', 'humidity', 'pressure', 'wind_speed_max', 'wind_speed_mean', 'wind_direction', 'weather_description']].values.T)\n",
    "#         y.append(data.iloc[i][['mean_temperature_next_day', 'max_wind_speed_next_day']].values)\n",
    "#     return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:30.226245Z",
     "iopub.status.busy": "2024-12-29T16:54:30.225856Z",
     "iopub.status.idle": "2024-12-29T16:54:32.214422Z",
     "shell.execute_reply": "2024-12-29T16:54:32.213087Z",
     "shell.execute_reply.started": "2024-12-29T16:54:30.226208Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# X, y = create_time_windows(aggregated_data)\n",
    "\n",
    "# train_size = int(0.8 * len(X))\n",
    "# X_train, X_test = X[:train_size], X[train_size:]\n",
    "# y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:52.861230Z",
     "iopub.status.busy": "2024-12-29T16:54:52.860846Z",
     "iopub.status.idle": "2024-12-29T16:54:52.868867Z",
     "shell.execute_reply": "2024-12-29T16:54:52.867778Z",
     "shell.execute_reply.started": "2024-12-29T16:54:52.861203Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(\"Training data windowed:\", X_train.shape, y_train.shape)\n",
    "# print(\"Test data windowed:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_weather_data(data, window_size=3, dayoffset=0):\n",
    "    X, y = [], []\n",
    "    for i in range(window_size, len(data) - 1):\n",
    "        X_window = data.iloc[i-window_size:i][[\n",
    "            'mean_temperature', 'humidity_mean', 'pressure_mean', 'max_wind_speed', 'wind_speed_mean', 'wind_direction_mean'\n",
    "        ]].values\n",
    "        y_target = data.iloc[i + dayoffset][['mean_temperature', 'max_wind_speed']].values\n",
    "\n",
    "        # encode wind speed > 6 as binary\n",
    "        y_target[1] = 1 if y_target[1] >= 6 else 0\n",
    "        X.append(X_window)\n",
    "        y.append(y_target)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # X_mean = X.mean(axis=(0, 1), keepdims=True)\n",
    "    # X_std = X.std(axis=(0, 1), keepdims=True)\n",
    "    # X_normalized = (X - X_mean) / (X_std + 1e-9)\n",
    "\n",
    "    train_size = int(0.7 * len(X))\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    X_mean = X_train.mean(axis=(0, 1), keepdims=True)\n",
    "    X_std = X_train.std(axis=(0, 1), keepdims=True)\n",
    "    X_train = (X_train - X_mean) / (X_std + 1e-9)\n",
    "    X_test = (X_test - X_mean) / (X_std + 1e-9)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = preprocess_weather_data(aggregated_data, window_size=3)\n",
    "\n",
    "# # Split into train/test\n",
    "# train_size = int(0.8 * len(X))\n",
    "# X_train, X_test = X[:train_size], X[train_size:]\n",
    "# y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_weather_data(aggregated_data, window_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1318, 3, 6)\n",
      "(1318, 2)\n",
      "(565, 3, 6)\n",
      "(565, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=1000, learning_rate=0.001,rate = [500]):\n",
    "    X_train_cp = cp.array(X_train.reshape(X_train.shape[0], -1), dtype=cp.float32)\n",
    "    y_train_cp = cp.array(y_train, dtype=cp.float32)\n",
    "    X_test_cp = cp.array(X_test.reshape(X_test.shape[0], -1), dtype=cp.float32)\n",
    "    y_test_cp = cp.array(y_test, dtype=cp.float32)\n",
    "\n",
    "    model.train(X_train_cp, y_train_cp, epochs, learning_rate,lower_rate = rate)\n",
    "\n",
    "    predictions = model.predict(X_test_cp)\n",
    "    \n",
    "    mae = cp.mean(cp.abs(predictions[:, 0] - y_test_cp[:, 0]))\n",
    "\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    auc = roc_auc_score(cp.asnumpy(y_test_cp[:, 1]), cp.asnumpy(predictions[:, 1]))\n",
    "\n",
    "    print(f\"Test Regression MAE: {mae}\")\n",
    "    print(f\"Test Classification AUC: {auc}\")\n",
    "\n",
    "    return mae, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Epoch 0, Regression Loss: 283.9161071777344, Classification AUC: 0.7825649501275805, Learning Rate: 0.0001\n",
      "Epoch 100, Regression Loss: 252.24388122558594, Classification AUC: 0.7655271398747389, Learning Rate: 0.0001\n",
      "Epoch 200, Regression Loss: 212.68060302734375, Classification AUC: 0.7789491997216422, Learning Rate: 0.0001\n",
      "Epoch 300, Regression Loss: 162.89266967773438, Classification AUC: 0.7900052192066807, Learning Rate: 0.0001\n",
      "Epoch 400, Regression Loss: 102.13925170898438, Classification AUC: 0.7952824170726049, Learning Rate: 0.0001\n",
      "Epoch 500, Regression Loss: 30.539581298828125, Classification AUC: 0.7946111691022966, Learning Rate: 1e-05\n",
      "Epoch 600, Regression Loss: 22.780210494995117, Classification AUC: 0.7939196242171189, Learning Rate: 1e-05\n",
      "Epoch 700, Regression Loss: 14.894908905029297, Classification AUC: 0.7945082347483182, Learning Rate: 1e-05\n",
      "Epoch 800, Regression Loss: 6.892397880554199, Classification AUC: 0.7951287404314542, Learning Rate: 1e-05\n",
      "Epoch 900, Regression Loss: 1.684070348739624, Classification AUC: 0.7954331941544887, Learning Rate: 1e-05\n",
      "Epoch 1000, Regression Loss: 1.6636888980865479, Classification AUC: 0.7958246346555324, Learning Rate: 1e-05\n",
      "Epoch 1100, Regression Loss: 1.6500482559204102, Classification AUC: 0.7953085131060079, Learning Rate: 1e-05\n",
      "Epoch 1200, Regression Loss: 1.6381895542144775, Classification AUC: 0.794922871723498, Learning Rate: 1e-05\n",
      "Epoch 1300, Regression Loss: 1.627539038658142, Classification AUC: 0.7952432730225004, Learning Rate: 1e-05\n",
      "Epoch 1400, Regression Loss: 1.617584228515625, Classification AUC: 0.7950504523312456, Learning Rate: 1e-05\n",
      "Epoch 1500, Regression Loss: 1.6081831455230713, Classification AUC: 0.7954505915100905, Learning Rate: 1e-05\n",
      "Epoch 1600, Regression Loss: 1.5995585918426514, Classification AUC: 0.7960855949895616, Learning Rate: 1e-05\n",
      "Epoch 1700, Regression Loss: 1.5911407470703125, Classification AUC: 0.7966568081651588, Learning Rate: 1e-05\n",
      "Epoch 1800, Regression Loss: 1.583457589149475, Classification AUC: 0.7970714451403387, Learning Rate: 1e-05\n",
      "Epoch 1900, Regression Loss: 1.5763330459594727, Classification AUC: 0.7974483878450476, Learning Rate: 1e-05\n",
      "Epoch 2000, Regression Loss: 1.5699166059494019, Classification AUC: 0.7980167014613779, Learning Rate: 1e-05\n",
      "Epoch 2100, Regression Loss: 1.56421959400177, Classification AUC: 0.7985299234516354, Learning Rate: 1e-05\n",
      "Epoch 2200, Regression Loss: 1.5589593648910522, Classification AUC: 0.7998289260032476, Learning Rate: 1e-05\n",
      "Epoch 2300, Regression Loss: 1.5543103218078613, Classification AUC: 0.8010380422175829, Learning Rate: 1e-05\n",
      "Epoch 2400, Regression Loss: 1.5507086515426636, Classification AUC: 0.8017919276270007, Learning Rate: 1e-05\n",
      "Epoch 2500, Regression Loss: 1.5475027561187744, Classification AUC: 0.802377638598933, Learning Rate: 1e-05\n",
      "Epoch 2600, Regression Loss: 1.5446984767913818, Classification AUC: 0.8029691486893993, Learning Rate: 1e-05\n",
      "Epoch 2700, Regression Loss: 1.5424017906188965, Classification AUC: 0.8036679424727441, Learning Rate: 1e-05\n",
      "Epoch 2800, Regression Loss: 1.5406850576400757, Classification AUC: 0.8043174437485502, Learning Rate: 1e-05\n",
      "Epoch 2900, Regression Loss: 1.5394500494003296, Classification AUC: 0.8047407794015309, Learning Rate: 1e-05\n",
      "Test Regression MAE: 1.6609647274017334\n",
      "Test Classification AUC: 0.6643835616438356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(1.6609647, dtype=float32), 0.6643835616438356)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predykcja na dzisiaj\n",
    "# X_train, X_test, y_train, y_test = preprocess_weather_data(aggregated_data, window_size=3)\n",
    "X_train, X_test, y_train, y_test = preprocess_weather_data(aggregated_data, window_size=3)\n",
    "from weather_prediction import WeatherPredictionNetwork\n",
    "print(X_train.shape[1])\n",
    "layers = [X_train.shape[1] * X_train.shape[2],512, 512, 2]\n",
    "activations = [\"sigmoid\", \"relu\"]\n",
    "model_today = WeatherPredictionNetwork(layers, activations, seed=42)\n",
    "\n",
    "train_and_evaluate(model_today, X_train, y_train, X_test, y_test, epochs=3000, learning_rate=0.0001, rate = [500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_weather_data_with_today_prediction(model = model_today):\n",
    "    X_train, X_test, y_train, y_test = preprocess_weather_data(aggregated_data, window_size=3, dayoffset=1)\n",
    "    X_train_cp = cp.array(X_train.reshape(X_train.shape[0], -1), dtype=cp.float32)\n",
    "    y_train_cp = cp.array(y_train, dtype=cp.float32)\n",
    "    X_test_cp = cp.array(X_test.reshape(X_test.shape[0], -1), dtype=cp.float32)\n",
    "    y_test_cp = cp.array(y_test, dtype=cp.float32)\n",
    "    expanded_X_train = []\n",
    "    for sample in X_train_cp:\n",
    "        # Predict for each individual sample\n",
    "        prediction = model.predict(sample.reshape(1, -1))  # Reshape to 2D array (1, features)\n",
    "        expanded_sample = cp.hstack([sample, prediction.flatten()])  # Add prediction to sample\n",
    "        expanded_X_train.append(expanded_sample)\n",
    "\n",
    "    expanded_X_test = []\n",
    "    for sample in X_test_cp:\n",
    "        # Predict for each individual sample\n",
    "        prediction = model.predict(sample.reshape(1, -1))  # Reshape to 2D array (1, features)\n",
    "        expanded_sample = cp.hstack([sample, prediction.flatten()])  # Add prediction to sample\n",
    "        expanded_X_test.append(expanded_sample)\n",
    "\n",
    "    # Convert the expanded lists back into numpy arrays\n",
    "    expanded_X_train = cp.array(expanded_X_train)\n",
    "    expanded_X_test = cp.array(expanded_X_test)\n",
    "    return expanded_X_train, expanded_X_test, y_train_cp, y_test_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1318, 20)\n",
      "(1318, 2)\n",
      "(565, 20)\n",
      "(565, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_weather_data_with_today_prediction()\n",
    "# X_train = X_train[:, 6:]\n",
    "# X_test = X_test[:, 6:]\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Regression Loss: 283.6138610839844, Classification AUC: 0.7489841147202432, Learning Rate: 1e-05\n",
      "Epoch 100, Regression Loss: 276.1835021972656, Classification AUC: 0.7537941390898713, Learning Rate: 1e-05\n",
      "Epoch 200, Regression Loss: 268.6015625, Classification AUC: 0.7537941390898713, Learning Rate: 1e-05\n",
      "Epoch 300, Regression Loss: 260.7461242675781, Classification AUC: 0.7495650355378312, Learning Rate: 1e-05\n",
      "Epoch 400, Regression Loss: 252.53907775878906, Classification AUC: 0.7434101794754866, Learning Rate: 1e-05\n",
      "Epoch 500, Regression Loss: 243.9389190673828, Classification AUC: 0.7358306151080076, Learning Rate: 1e-05\n",
      "Epoch 600, Regression Loss: 234.92282104492188, Classification AUC: 0.7282103862832977, Learning Rate: 1e-05\n",
      "Epoch 700, Regression Loss: 225.48638916015625, Classification AUC: 0.7209808267084156, Learning Rate: 1e-05\n",
      "Epoch 800, Regression Loss: 215.62672424316406, Classification AUC: 0.7147446417316087, Learning Rate: 1e-05\n",
      "Epoch 900, Regression Loss: 205.34153747558594, Classification AUC: 0.7091678018827644, Learning Rate: 1e-05\n",
      "Epoch 1000, Regression Loss: 194.6279754638672, Classification AUC: 0.7048355848856022, Learning Rate: 1e-05\n",
      "Epoch 1100, Regression Loss: 183.4808349609375, Classification AUC: 0.7002274305000856, Learning Rate: 1e-05\n",
      "Epoch 1200, Regression Loss: 171.8925018310547, Classification AUC: 0.6955161626694473, Learning Rate: 1e-05\n",
      "Epoch 1300, Regression Loss: 159.8536376953125, Classification AUC: 0.6901571681271983, Learning Rate: 1e-05\n",
      "Epoch 1400, Regression Loss: 147.35450744628906, Classification AUC: 0.6856361518643201, Learning Rate: 1e-05\n",
      "Epoch 1500, Regression Loss: 134.38597106933594, Classification AUC: 0.6844612395107486, Learning Rate: 1e-05\n",
      "Epoch 1600, Regression Loss: 120.9407730102539, Classification AUC: 0.6881123268492887, Learning Rate: 1e-05\n",
      "Epoch 1700, Regression Loss: 107.01361083984375, Classification AUC: 0.6992369605060983, Learning Rate: 1e-05\n",
      "Epoch 1800, Regression Loss: 92.60154724121094, Classification AUC: 0.7200339257757471, Learning Rate: 1e-05\n",
      "Epoch 1900, Regression Loss: 77.70331573486328, Classification AUC: 0.7409093153557704, Learning Rate: 1e-05\n",
      "Epoch 2000, Regression Loss: 62.31892395019531, Classification AUC: 0.7416253002634475, Learning Rate: 1e-05\n",
      "Epoch 2100, Regression Loss: 46.44825744628906, Classification AUC: 0.7293664187102976, Learning Rate: 1e-05\n",
      "Epoch 2200, Regression Loss: 30.091142654418945, Classification AUC: 0.7148768012176099, Learning Rate: 1e-05\n",
      "Epoch 2300, Regression Loss: 13.325623512268066, Classification AUC: 0.7046685701505455, Learning Rate: 1e-05\n",
      "Epoch 2400, Regression Loss: 5.555579662322998, Classification AUC: 0.7029258076977818, Learning Rate: 1e-05\n",
      "Epoch 2500, Regression Loss: 5.341028690338135, Classification AUC: 0.7129263595725583, Learning Rate: 1e-05\n",
      "Epoch 2600, Regression Loss: 5.137378692626953, Classification AUC: 0.7244968499568666, Learning Rate: 1e-05\n",
      "Epoch 2700, Regression Loss: 4.946642875671387, Classification AUC: 0.7308114592440478, Learning Rate: 1e-05\n",
      "Epoch 2800, Regression Loss: 4.7631306648254395, Classification AUC: 0.7223953688992422, Learning Rate: 1e-05\n",
      "Epoch 2900, Regression Loss: 4.584290981292725, Classification AUC: 0.7007618776522666, Learning Rate: 1e-05\n",
      "Epoch 3000, Regression Loss: 4.409628868103027, Classification AUC: 0.6825500100208841, Learning Rate: 1e-05\n",
      "Epoch 3100, Regression Loss: 4.240563869476318, Classification AUC: 0.6746364161832922, Learning Rate: 1e-05\n",
      "Epoch 3200, Regression Loss: 4.071286678314209, Classification AUC: 0.6713150014087329, Learning Rate: 1e-05\n",
      "Epoch 3300, Regression Loss: 3.8996543884277344, Classification AUC: 0.6684292772473648, Learning Rate: 1e-05\n",
      "Epoch 3400, Regression Loss: 3.7244722843170166, Classification AUC: 0.6655203162532931, Learning Rate: 1e-05\n",
      "Epoch 3500, Regression Loss: 3.547250270843506, Classification AUC: 0.6634188351956687, Learning Rate: 1e-05\n",
      "Epoch 3600, Regression Loss: 3.3779375553131104, Classification AUC: 0.6620144591191498, Learning Rate: 1e-05\n",
      "Epoch 3700, Regression Loss: 3.2240335941314697, Classification AUC: 0.6605795846997075, Learning Rate: 1e-05\n",
      "Epoch 3800, Regression Loss: 3.090094566345215, Classification AUC: 0.6604111176626071, Learning Rate: 1e-05\n",
      "Epoch 3900, Regression Loss: 2.9946932792663574, Classification AUC: 0.661430633697474, Learning Rate: 1e-05\n",
      "Epoch 4000, Regression Loss: 2.93428635597229, Classification AUC: 0.6624414359200769, Learning Rate: 1e-05\n",
      "Epoch 4100, Regression Loss: 2.8919849395751953, Classification AUC: 0.6627609423697504, Learning Rate: 1e-05\n",
      "Epoch 4200, Regression Loss: 2.858822822570801, Classification AUC: 0.6637543169678256, Learning Rate: 1e-05\n",
      "Epoch 4300, Regression Loss: 2.832752227783203, Classification AUC: 0.665192095991356, Learning Rate: 1e-05\n",
      "Epoch 4400, Regression Loss: 2.8112146854400635, Classification AUC: 0.6658412750050106, Learning Rate: 1e-05\n",
      "Epoch 4500, Regression Loss: 2.791935920715332, Classification AUC: 0.6673356938082555, Learning Rate: 1e-05\n",
      "Epoch 4600, Regression Loss: 2.773608922958374, Classification AUC: 0.6685280337863547, Learning Rate: 1e-05\n",
      "Epoch 4700, Regression Loss: 2.756103754043579, Classification AUC: 0.6695083376660345, Learning Rate: 1e-05\n",
      "Epoch 4800, Regression Loss: 2.738335132598877, Classification AUC: 0.6705568997417807, Learning Rate: 1e-05\n",
      "Epoch 4900, Regression Loss: 2.7209293842315674, Classification AUC: 0.6715212282989766, Learning Rate: 1e-05\n",
      "Epoch 5000, Regression Loss: 2.7045066356658936, Classification AUC: 0.672375181900831, Learning Rate: 1e-05\n",
      "Epoch 5100, Regression Loss: 2.688462495803833, Classification AUC: 0.6733307966457631, Learning Rate: 1e-05\n",
      "Epoch 5200, Regression Loss: 2.6743922233581543, Classification AUC: 0.6747453388365899, Learning Rate: 1e-05\n",
      "Epoch 5300, Regression Loss: 2.6619136333465576, Classification AUC: 0.6758926574513261, Learning Rate: 1e-05\n",
      "Epoch 5400, Regression Loss: 2.650489091873169, Classification AUC: 0.6767001373877732, Learning Rate: 1e-05\n",
      "Epoch 5500, Regression Loss: 2.6395416259765625, Classification AUC: 0.6775047127201328, Learning Rate: 1e-05\n",
      "Epoch 5600, Regression Loss: 2.629049301147461, Classification AUC: 0.6781437256194794, Learning Rate: 1e-05\n",
      "Epoch 5700, Regression Loss: 2.6191768646240234, Classification AUC: 0.6787304556452434, Learning Rate: 1e-05\n",
      "Epoch 5800, Regression Loss: 2.60986590385437, Classification AUC: 0.6792416659647207, Learning Rate: 1e-05\n",
      "Epoch 5900, Regression Loss: 2.601006031036377, Classification AUC: 0.679659928953384, Learning Rate: 1e-05\n",
      "Epoch 6000, Regression Loss: 2.5928351879119873, Classification AUC: 0.6800200998602886, Learning Rate: 1e-05\n",
      "Epoch 6100, Regression Loss: 2.585207939147949, Classification AUC: 0.6803715569549293, Learning Rate: 1e-05\n",
      "Epoch 6200, Regression Loss: 2.5779178142547607, Classification AUC: 0.6806445897391957, Learning Rate: 1e-05\n",
      "Epoch 6300, Regression Loss: 2.571030855178833, Classification AUC: 0.6809524777725172, Learning Rate: 1e-05\n",
      "Epoch 6400, Regression Loss: 2.5644993782043457, Classification AUC: 0.6814724019042584, Learning Rate: 1e-05\n",
      "Epoch 6500, Regression Loss: 2.558345317840576, Classification AUC: 0.681974898411472, Learning Rate: 1e-05\n",
      "Epoch 6600, Regression Loss: 2.5524888038635254, Classification AUC: 0.6824715857105098, Learning Rate: 1e-05\n",
      "Epoch 6700, Regression Loss: 2.5469236373901367, Classification AUC: 0.6828695164705574, Learning Rate: 1e-05\n",
      "Epoch 6800, Regression Loss: 2.541684627532959, Classification AUC: 0.6833342531246278, Learning Rate: 1e-05\n",
      "Epoch 6900, Regression Loss: 2.5365052223205566, Classification AUC: 0.6838047989868741, Learning Rate: 1e-05\n",
      "Epoch 7000, Regression Loss: 2.531508207321167, Classification AUC: 0.6842695356409444, Learning Rate: 1e-05\n",
      "Epoch 7100, Regression Loss: 2.5268096923828125, Classification AUC: 0.6848272196258289, Learning Rate: 1e-05\n",
      "Epoch 7200, Regression Loss: 2.522451639175415, Classification AUC: 0.6853616667780098, Learning Rate: 1e-05\n",
      "Epoch 7300, Regression Loss: 2.518259048461914, Classification AUC: 0.6859193507628943, Learning Rate: 1e-05\n",
      "Epoch 7400, Regression Loss: 2.514315128326416, Classification AUC: 0.6864784870498226, Learning Rate: 1e-05\n",
      "Epoch 7500, Regression Loss: 2.5105159282684326, Classification AUC: 0.6869388667977611, Learning Rate: 1e-05\n",
      "Epoch 7600, Regression Loss: 2.506805658340454, Classification AUC: 0.6875081691989974, Learning Rate: 1e-05\n",
      "Epoch 7700, Regression Loss: 2.5030288696289062, Classification AUC: 0.6879903334775954, Learning Rate: 1e-05\n",
      "Epoch 7800, Regression Loss: 2.4993021488189697, Classification AUC: 0.6884085964662586, Learning Rate: 1e-05\n",
      "Epoch 7900, Regression Loss: 2.495680093765259, Classification AUC: 0.6888936653489446, Learning Rate: 1e-05\n",
      "Epoch 8000, Regression Loss: 2.4921412467956543, Classification AUC: 0.6892654546722008, Learning Rate: 1e-05\n",
      "Epoch 8100, Regression Loss: 2.4886741638183594, Classification AUC: 0.6896285301831934, Learning Rate: 1e-05\n",
      "Epoch 8200, Regression Loss: 2.48529052734375, Classification AUC: 0.6900380793595928, Learning Rate: 1e-05\n",
      "Epoch 8300, Regression Loss: 2.4819438457489014, Classification AUC: 0.6903633950174421, Learning Rate: 1e-05\n",
      "Epoch 8400, Regression Loss: 2.4786622524261475, Classification AUC: 0.6906858060712036, Learning Rate: 1e-05\n",
      "Epoch 8500, Regression Loss: 2.475395441055298, Classification AUC: 0.6909530296472939, Learning Rate: 1e-05\n",
      "Epoch 8600, Regression Loss: 2.472165107727051, Classification AUC: 0.6912667268887913, Learning Rate: 1e-05\n",
      "Epoch 8700, Regression Loss: 2.4689903259277344, Classification AUC: 0.6916109224732122, Learning Rate: 1e-05\n",
      "Epoch 8800, Regression Loss: 2.4658663272857666, Classification AUC: 0.6919376904331056, Learning Rate: 1e-05\n",
      "Epoch 8900, Regression Loss: 2.4627697467803955, Classification AUC: 0.6922252462378116, Learning Rate: 1e-05\n",
      "Epoch 9000, Regression Loss: 2.4597463607788086, Classification AUC: 0.6925767033324522, Learning Rate: 1e-05\n",
      "Epoch 9100, Regression Loss: 2.456716775894165, Classification AUC: 0.6929339696352689, Learning Rate: 1e-05\n",
      "Epoch 9200, Regression Loss: 2.4536609649658203, Classification AUC: 0.6932418576685906, Learning Rate: 1e-05\n",
      "Epoch 9300, Regression Loss: 2.450564384460449, Classification AUC: 0.6935570072121319, Learning Rate: 1e-05\n",
      "Epoch 9400, Regression Loss: 2.4474539756774902, Classification AUC: 0.6939796271069272, Learning Rate: 1e-05\n",
      "Epoch 9500, Regression Loss: 2.4444069862365723, Classification AUC: 0.694376105564931, Learning Rate: 1e-05\n",
      "Epoch 9600, Regression Loss: 2.4413397312164307, Classification AUC: 0.694804534667902, Learning Rate: 1e-05\n",
      "Epoch 9700, Regression Loss: 2.4383018016815186, Classification AUC: 0.6951821331993342, Learning Rate: 1e-05\n",
      "Epoch 9800, Regression Loss: 2.4352827072143555, Classification AUC: 0.6956584882697565, Learning Rate: 1e-05\n",
      "Epoch 9900, Regression Loss: 2.432283401489258, Classification AUC: 0.6960767512584197, Learning Rate: 1e-05\n",
      "Epoch 10000, Regression Loss: 2.4292709827423096, Classification AUC: 0.6965502017247539, Learning Rate: 1e-05\n",
      "Epoch 10100, Regression Loss: 2.4262707233428955, Classification AUC: 0.6969597509011535, Learning Rate: 1e-05\n",
      "Epoch 10200, Regression Loss: 2.42325758934021, Classification AUC: 0.6972763527467389, Learning Rate: 1e-05\n",
      "Epoch 10300, Regression Loss: 2.4202942848205566, Classification AUC: 0.6976423328618193, Learning Rate: 1e-05\n",
      "Epoch 10400, Regression Loss: 2.417351007461548, Classification AUC: 0.6981186879322413, Learning Rate: 1e-05\n",
      "Epoch 10500, Regression Loss: 2.414416551589966, Classification AUC: 0.6985369509209048, Learning Rate: 1e-05\n",
      "Epoch 10600, Regression Loss: 2.4114675521850586, Classification AUC: 0.6990713980730856, Learning Rate: 1e-05\n",
      "Epoch 10700, Regression Loss: 2.4085030555725098, Classification AUC: 0.6996755557233771, Learning Rate: 1e-05\n",
      "Epoch 10800, Regression Loss: 2.4055263996124268, Classification AUC: 0.7002129074796459, Learning Rate: 1e-05\n",
      "Epoch 10900, Regression Loss: 2.4025676250457764, Classification AUC: 0.7006834533418923, Learning Rate: 1e-05\n",
      "Epoch 11000, Regression Loss: 2.39963698387146, Classification AUC: 0.7011569038082265, Learning Rate: 1e-05\n",
      "Epoch 11100, Regression Loss: 2.396730661392212, Classification AUC: 0.7016942555644953, Learning Rate: 1e-05\n",
      "Epoch 11200, Regression Loss: 2.3938381671905518, Classification AUC: 0.7022446780391599, Learning Rate: 1e-05\n",
      "Epoch 11300, Regression Loss: 2.3909926414489746, Classification AUC: 0.7027108669952742, Learning Rate: 1e-05\n",
      "Epoch 11400, Regression Loss: 2.3881821632385254, Classification AUC: 0.7032278865229276, Learning Rate: 1e-05\n",
      "Epoch 11500, Regression Loss: 2.3853952884674072, Classification AUC: 0.7038189734548231, Learning Rate: 1e-05\n",
      "Epoch 11600, Regression Loss: 2.38264536857605, Classification AUC: 0.7043723005335757, Learning Rate: 1e-05\n",
      "Epoch 11700, Regression Loss: 2.379940986633301, Classification AUC: 0.7049241753102844, Learning Rate: 1e-05\n",
      "Epoch 11800, Regression Loss: 2.3772313594818115, Classification AUC: 0.705473145482905, Learning Rate: 1e-05\n",
      "Epoch 11900, Regression Loss: 2.374540090560913, Classification AUC: 0.706101992267944, Learning Rate: 1e-05\n",
      "Epoch 12000, Regression Loss: 2.3718442916870117, Classification AUC: 0.7065885134526739, Learning Rate: 1e-05\n",
      "Epoch 12100, Regression Loss: 2.3691513538360596, Classification AUC: 0.707207194123405, Learning Rate: 1e-05\n",
      "Epoch 12200, Regression Loss: 2.3664824962615967, Classification AUC: 0.7078897760840709, Learning Rate: 1e-05\n",
      "Epoch 12300, Regression Loss: 2.363891363143921, Classification AUC: 0.7084387462566915, Learning Rate: 1e-05\n",
      "Epoch 12400, Regression Loss: 2.361313581466675, Classification AUC: 0.7089993348456638, Learning Rate: 1e-05\n",
      "Epoch 12500, Regression Loss: 2.3587398529052734, Classification AUC: 0.7095788033612079, Learning Rate: 1e-05\n",
      "Epoch 12600, Regression Loss: 2.3561654090881348, Classification AUC: 0.7101844133135433, Learning Rate: 1e-05\n",
      "Epoch 12700, Regression Loss: 2.353595495223999, Classification AUC: 0.7107508111106916, Learning Rate: 1e-05\n",
      "Epoch 12800, Regression Loss: 2.3510215282440186, Classification AUC: 0.7113288273241918, Learning Rate: 1e-05\n",
      "Epoch 12900, Regression Loss: 2.3484554290771484, Classification AUC: 0.7118923205172518, Learning Rate: 1e-05\n",
      "Epoch 13000, Regression Loss: 2.3459036350250244, Classification AUC: 0.7123890078162896, Learning Rate: 1e-05\n",
      "Epoch 13100, Regression Loss: 2.343353271484375, Classification AUC: 0.7130367345279002, Learning Rate: 1e-05\n",
      "Epoch 13200, Regression Loss: 2.3407909870147705, Classification AUC: 0.7136147507414001, Learning Rate: 1e-05\n",
      "Epoch 13300, Regression Loss: 2.338216543197632, Classification AUC: 0.7141404840813174, Learning Rate: 1e-05\n",
      "Epoch 13400, Regression Loss: 2.3356363773345947, Classification AUC: 0.714683645045762, Learning Rate: 1e-05\n",
      "Epoch 13500, Regression Loss: 2.333047389984131, Classification AUC: 0.715212282989767, Learning Rate: 1e-05\n",
      "Epoch 13600, Regression Loss: 2.330482006072998, Classification AUC: 0.715700256476541, Learning Rate: 1e-05\n",
      "Epoch 13700, Regression Loss: 2.327937126159668, Classification AUC: 0.7161737069428753, Learning Rate: 1e-05\n",
      "Epoch 13800, Regression Loss: 2.3254144191741943, Classification AUC: 0.7166123021601541, Learning Rate: 1e-05\n",
      "Epoch 13900, Regression Loss: 2.3229124546051025, Classification AUC: 0.7171641769368626, Learning Rate: 1e-05\n",
      "Epoch 14000, Regression Loss: 2.3204286098480225, Classification AUC: 0.7176492458195487, Learning Rate: 1e-05\n",
      "Epoch 14100, Regression Loss: 2.317988157272339, Classification AUC: 0.7181851452737734, Learning Rate: 1e-05\n",
      "Epoch 14200, Regression Loss: 2.3155524730682373, Classification AUC: 0.7187036171034706, Learning Rate: 1e-05\n",
      "Epoch 14300, Regression Loss: 2.313127279281616, Classification AUC: 0.7192700149006189, Learning Rate: 1e-05\n",
      "Epoch 14400, Regression Loss: 2.3107168674468994, Classification AUC: 0.7197318469506013, Learning Rate: 1e-05\n",
      "Epoch 14500, Regression Loss: 2.3083159923553467, Classification AUC: 0.7202052974169356, Learning Rate: 1e-05\n",
      "Epoch 14600, Regression Loss: 2.305931806564331, Classification AUC: 0.7206787478832698, Learning Rate: 1e-05\n",
      "Epoch 14700, Regression Loss: 2.3035526275634766, Classification AUC: 0.7212567640967699, Learning Rate: 1e-05\n",
      "Epoch 14800, Regression Loss: 2.301187753677368, Classification AUC: 0.7217970204571267, Learning Rate: 1e-05\n",
      "Epoch 14900, Regression Loss: 2.2988343238830566, Classification AUC: 0.7223169445888677, Learning Rate: 1e-05\n",
      "Epoch 15000, Regression Loss: 2.2964751720428467, Classification AUC: 0.7227932996592901, Learning Rate: 1e-05\n",
      "Epoch 15100, Regression Loss: 2.294111728668213, Classification AUC: 0.7233480790400864, Learning Rate: 1e-05\n",
      "Epoch 15200, Regression Loss: 2.291745185852051, Classification AUC: 0.7238360525268603, Learning Rate: 1e-05\n",
      "Epoch 15300, Regression Loss: 2.2893922328948975, Classification AUC: 0.724281909254359, Learning Rate: 1e-05\n",
      "Epoch 15400, Regression Loss: 2.2870395183563232, Classification AUC: 0.7248584731658152, Learning Rate: 1e-05\n",
      "Epoch 15500, Regression Loss: 2.284712314605713, Classification AUC: 0.725343542048501, Learning Rate: 1e-05\n",
      "Epoch 15600, Regression Loss: 2.28239369392395, Classification AUC: 0.7258925122211217, Learning Rate: 1e-05\n",
      "Epoch 15700, Regression Loss: 2.280076265335083, Classification AUC: 0.7262860860750376, Learning Rate: 1e-05\n",
      "Epoch 15800, Regression Loss: 2.277761936187744, Classification AUC: 0.726699992157569, Learning Rate: 1e-05\n",
      "Epoch 15900, Regression Loss: 2.2754569053649902, Classification AUC: 0.7273215774323882, Learning Rate: 1e-05\n",
      "Epoch 16000, Regression Loss: 2.273160457611084, Classification AUC: 0.7278066463150742, Learning Rate: 1e-05\n",
      "Epoch 16100, Regression Loss: 2.2708683013916016, Classification AUC: 0.7282830013854961, Learning Rate: 1e-05\n",
      "Epoch 16200, Regression Loss: 2.2686033248901367, Classification AUC: 0.7287651656640941, Learning Rate: 1e-05\n",
      "Epoch 16300, Regression Loss: 2.2663393020629883, Classification AUC: 0.7292821851917475, Learning Rate: 1e-05\n",
      "Epoch 16400, Regression Loss: 2.2640793323516846, Classification AUC: 0.7297498264499057, Learning Rate: 1e-05\n",
      "Epoch 16500, Regression Loss: 2.2618367671966553, Classification AUC: 0.7302319907285039, Learning Rate: 1e-05\n",
      "Epoch 16600, Regression Loss: 2.2596356868743896, Classification AUC: 0.7306589675294309, Learning Rate: 1e-05\n",
      "Epoch 16700, Regression Loss: 2.257455825805664, Classification AUC: 0.7310627074976546, Learning Rate: 1e-05\n",
      "Epoch 16800, Regression Loss: 2.255279541015625, Classification AUC: 0.7314809704863179, Learning Rate: 1e-05\n",
      "Epoch 16900, Regression Loss: 2.2531075477600098, Classification AUC: 0.7320270360548505, Learning Rate: 1e-05\n",
      "Epoch 17000, Regression Loss: 2.250979423522949, Classification AUC: 0.7324191576067224, Learning Rate: 1e-05\n",
      "Epoch 17100, Regression Loss: 2.248899221420288, Classification AUC: 0.7328635620321773, Learning Rate: 1e-05\n",
      "Epoch 17200, Regression Loss: 2.2468607425689697, Classification AUC: 0.7333137756658079, Learning Rate: 1e-05\n",
      "Epoch 17300, Regression Loss: 2.2448368072509766, Classification AUC: 0.7337610846953506, Learning Rate: 1e-05\n",
      "Epoch 17400, Regression Loss: 2.2428138256073, Classification AUC: 0.7342171075371571, Learning Rate: 1e-05\n",
      "Epoch 17500, Regression Loss: 2.2407937049865723, Classification AUC: 0.7346586073585241, Learning Rate: 1e-05\n",
      "Epoch 17600, Regression Loss: 2.238795280456543, Classification AUC: 0.7350187782654285, Learning Rate: 1e-05\n",
      "Epoch 17700, Regression Loss: 2.236829996109009, Classification AUC: 0.7354341366500039, Learning Rate: 1e-05\n",
      "Epoch 17800, Regression Loss: 2.234894037246704, Classification AUC: 0.7357884983487326, Learning Rate: 1e-05\n",
      "Epoch 17900, Regression Loss: 2.232990264892578, Classification AUC: 0.7361515738597251, Learning Rate: 1e-05\n",
      "Epoch 18000, Regression Loss: 2.2310986518859863, Classification AUC: 0.7365233631829813, Learning Rate: 1e-05\n",
      "Epoch 18100, Regression Loss: 2.22921085357666, Classification AUC: 0.7369706722125241, Learning Rate: 1e-05\n",
      "Epoch 18200, Regression Loss: 2.2273294925689697, Classification AUC: 0.7372901786621975, Learning Rate: 1e-05\n",
      "Epoch 18300, Regression Loss: 2.2254576683044434, Classification AUC: 0.7376561587772779, Learning Rate: 1e-05\n",
      "Epoch 18400, Regression Loss: 2.2236199378967285, Classification AUC: 0.7379814744351271, Learning Rate: 1e-05\n",
      "Epoch 18500, Regression Loss: 2.221789598464966, Classification AUC: 0.7382370795948658, Learning Rate: 1e-05\n",
      "Epoch 18600, Regression Loss: 2.2199673652648926, Classification AUC: 0.738562395252715, Learning Rate: 1e-05\n",
      "Epoch 18700, Regression Loss: 2.218151807785034, Classification AUC: 0.7387570037266071, Learning Rate: 1e-05\n",
      "Epoch 18800, Regression Loss: 2.2163593769073486, Classification AUC: 0.7390358457190493, Learning Rate: 1e-05\n",
      "Epoch 18900, Regression Loss: 2.214592456817627, Classification AUC: 0.7393030692951397, Learning Rate: 1e-05\n",
      "Epoch 19000, Regression Loss: 2.2128424644470215, Classification AUC: 0.7395702928712302, Learning Rate: 1e-05\n",
      "Epoch 19100, Regression Loss: 2.2111289501190186, Classification AUC: 0.7398404210514087, Learning Rate: 1e-05\n",
      "Epoch 19200, Regression Loss: 2.209423303604126, Classification AUC: 0.7400611709620919, Learning Rate: 1e-05\n",
      "Epoch 19300, Regression Loss: 2.2077443599700928, Classification AUC: 0.7403952004322052, Learning Rate: 1e-05\n",
      "Epoch 19400, Regression Loss: 2.2061097621917725, Classification AUC: 0.7406043319265367, Learning Rate: 1e-05\n",
      "Epoch 19500, Regression Loss: 2.204500913619995, Classification AUC: 0.7408715555026272, Learning Rate: 1e-05\n",
      "Epoch 19600, Regression Loss: 2.2028989791870117, Classification AUC: 0.7411329698705418, Learning Rate: 1e-05\n",
      "Epoch 19700, Regression Loss: 2.2012953758239746, Classification AUC: 0.7413595289894012, Learning Rate: 1e-05\n",
      "Epoch 19800, Regression Loss: 2.1997127532958984, Classification AUC: 0.7415948019205243, Learning Rate: 1e-05\n",
      "Epoch 19900, Regression Loss: 2.1981332302093506, Classification AUC: 0.7417763396760205, Learning Rate: 1e-05\n",
      "Test Regression MAE: 2.262542486190796\n",
      "Test Classification AUC: 0.6365616438356164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(2.2625425, dtype=float32), 0.6365616438356164)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predykcja na jutro z predykcjÄ… na dzisiaj\n",
    "X_train, X_test, y_train, y_test = preprocess_weather_data_with_today_prediction(model_today)\n",
    "layers = [X_train.shape[1], 512,512, 2]\n",
    "activations = [\"sigmoid\",\"linear\",]\n",
    "model2 = WeatherPredictionNetwork(layers, activations, seed=42)\n",
    "\n",
    "train_and_evaluate(model2, X_train, y_train, X_test, y_test, epochs=20000, learning_rate=0.00001, rate = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Epoch 0, Regression Loss: 283.91766357421875, Classification AUC: 0.7413798612180167, Learning Rate: 0.0001\n",
      "Epoch 100, Regression Loss: 252.24551391601562, Classification AUC: 0.7378013889816748, Learning Rate: 0.0001\n",
      "Epoch 200, Regression Loss: 212.68592834472656, Classification AUC: 0.7509360086673386, Learning Rate: 0.0001\n",
      "Epoch 300, Regression Loss: 162.90313720703125, Classification AUC: 0.7596527255352459, Learning Rate: 0.0001\n",
      "Epoch 400, Regression Loss: 102.12982177734375, Classification AUC: 0.7624542161780639, Learning Rate: 0.0001\n",
      "Epoch 500, Regression Loss: 30.50147247314453, Classification AUC: 0.7607187152355197, Learning Rate: 1e-05\n",
      "Epoch 600, Regression Loss: 22.716453552246094, Classification AUC: 0.7617033760213314, Learning Rate: 1e-05\n",
      "Epoch 700, Regression Loss: 14.805656433105469, Classification AUC: 0.7622639646103039, Learning Rate: 1e-05\n",
      "Epoch 800, Regression Loss: 6.835212707519531, Classification AUC: 0.7622901060470952, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 900, Regression Loss: 6.068936824798584, Classification AUC: 0.762444050063756, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 1000, Regression Loss: 5.320836544036865, Classification AUC: 0.7625326404884382, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 1100, Regression Loss: 4.6047868728637695, Classification AUC: 0.7625718526436254, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 1200, Regression Loss: 3.9436347484588623, Classification AUC: 0.7626299447253841, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 1300, Regression Loss: 3.3656933307647705, Classification AUC: 0.7626822275989671, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 1400, Regression Loss: 2.9003398418426514, Classification AUC: 0.7627984117624846, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 1500, Regression Loss: 2.5797338485717773, Classification AUC: 0.7628448854278918, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 1600, Regression Loss: 2.4044342041015625, Classification AUC: 0.7629204051341781, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 1700, Regression Loss: 2.395094871520996, Classification AUC: 0.7629320235505299, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 1800, Regression Loss: 2.3883790969848633, Classification AUC: 0.7629494511750576, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 1900, Regression Loss: 2.383098602294922, Classification AUC: 0.7629552603832335, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 2000, Regression Loss: 2.380373954772949, Classification AUC: 0.7629726880077611, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 2100, Regression Loss: 2.3801379203796387, Classification AUC: 0.7629843064241129, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 2200, Regression Loss: 2.3799095153808594, Classification AUC: 0.7630743491508389, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 2300, Regression Loss: 2.3796842098236084, Classification AUC: 0.7631179182121581, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 2400, Regression Loss: 2.3794589042663574, Classification AUC: 0.7632253885634117, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 2500, Regression Loss: 2.3792362213134766, Classification AUC: 0.7632892898533465, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 2600, Regression Loss: 2.3790204524993896, Classification AUC: 0.7633183358942258, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 2700, Regression Loss: 2.37880539894104, Classification AUC: 0.7633735233718969, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 2800, Regression Loss: 2.3785922527313232, Classification AUC: 0.7634461384740953, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 2900, Regression Loss: 2.3783822059631348, Classification AUC: 0.7635419904089973, Learning Rate: 1.0000000000000002e-07\n",
      "Test Regression MAE: 2.4123239517211914\n",
      "Test Classification AUC: 0.5876164383561644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(2.412324, dtype=float32), 0.5876164383561644)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prota predykcja sigmoid + relu\n",
    "X_train, X_test, y_train, y_test = preprocess_weather_data(aggregated_data, window_size=3,dayoffset = 1)\n",
    "from weather_prediction import WeatherPredictionNetwork\n",
    "print(X_train.shape[1])\n",
    "layers = [X_train.shape[1] * X_train.shape[2],512, 512, 2]\n",
    "activations = [\"sigmoid\", \"relu\"]\n",
    "model = WeatherPredictionNetwork(layers, activations, seed=42)\n",
    "\n",
    "train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=3000, learning_rate=0.0001, rate = [500,800,1600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Epoch 0, Regression Loss: 284.4524841308594, Classification AUC: 0.6888326686630979, Learning Rate: 0.0001\n",
      "Epoch 100, Regression Loss: 279.5395812988281, Classification AUC: 0.7185061040254908, Learning Rate: 0.0001\n",
      "Epoch 200, Regression Loss: 271.79608154296875, Classification AUC: 0.7201210638983853, Learning Rate: 0.0001\n",
      "Epoch 300, Regression Loss: 259.9175720214844, Classification AUC: 0.7589527159500523, Learning Rate: 0.0001\n",
      "Epoch 400, Regression Loss: 242.89019775390625, Classification AUC: 0.7677042880670151, Learning Rate: 0.0001\n",
      "Epoch 500, Regression Loss: 219.74588012695312, Classification AUC: 0.769249537441799, Learning Rate: 0.0001\n",
      "Epoch 600, Regression Loss: 189.7799835205078, Classification AUC: 0.7691130210496658, Learning Rate: 0.0001\n",
      "Epoch 700, Regression Loss: 153.2547149658203, Classification AUC: 0.7686802350405628, Learning Rate: 0.0001\n",
      "Epoch 800, Regression Loss: 113.67735290527344, Classification AUC: 0.768000557683985, Learning Rate: 0.0001\n",
      "Epoch 900, Regression Loss: 83.55284118652344, Classification AUC: 0.7681777385333491, Learning Rate: 0.0001\n",
      "Epoch 1000, Regression Loss: 71.1581802368164, Classification AUC: 0.7700105437128392, Learning Rate: 0.0001\n",
      "Epoch 1100, Regression Loss: 65.0780029296875, Classification AUC: 0.769539997850593, Learning Rate: 0.0001\n",
      "Epoch 1200, Regression Loss: 60.0074348449707, Classification AUC: 0.768285208884603, Learning Rate: 0.0001\n",
      "Epoch 1300, Regression Loss: 55.17441177368164, Classification AUC: 0.7665104957868718, Learning Rate: 0.0001\n",
      "Epoch 1400, Regression Loss: 50.52680587768555, Classification AUC: 0.7651569502818918, Learning Rate: 0.0001\n",
      "Epoch 1500, Regression Loss: 46.0557975769043, Classification AUC: 0.7640212500835074, Learning Rate: 0.0001\n",
      "Epoch 1600, Regression Loss: 41.74445343017578, Classification AUC: 0.7624614776882837, Learning Rate: 0.0001\n",
      "Epoch 1700, Regression Loss: 37.60490798950195, Classification AUC: 0.7603178798713841, Learning Rate: 0.0001\n",
      "Epoch 1800, Regression Loss: 33.5522346496582, Classification AUC: 0.756948539129374, Learning Rate: 0.0001\n",
      "Epoch 1900, Regression Loss: 29.48980712890625, Classification AUC: 0.7537273331958487, Learning Rate: 0.0001\n",
      "Epoch 2000, Regression Loss: 25.33294105529785, Classification AUC: 0.7484641905885019, Learning Rate: 0.0001\n",
      "Epoch 2100, Regression Loss: 21.1951847076416, Classification AUC: 0.7412418925238395, Learning Rate: 0.0001\n",
      "Epoch 2200, Regression Loss: 17.174560546875, Classification AUC: 0.7374295996584185, Learning Rate: 0.0001\n",
      "Epoch 2300, Regression Loss: 13.341479301452637, Classification AUC: 0.7355677484380492, Learning Rate: 0.0001\n",
      "Epoch 2400, Regression Loss: 9.979340553283691, Classification AUC: 0.7363548961458809, Learning Rate: 0.0001\n",
      "Epoch 2500, Regression Loss: 7.529717922210693, Classification AUC: 0.7441305212892957, Learning Rate: 1e-05\n",
      "Epoch 2600, Regression Loss: 7.205221652984619, Classification AUC: 0.7471803555816324, Learning Rate: 1e-05\n",
      "Epoch 2700, Regression Loss: 6.911458969116211, Classification AUC: 0.7499774893183184, Learning Rate: 1e-05\n",
      "Epoch 2800, Regression Loss: 6.626694679260254, Classification AUC: 0.7526787711201025, Learning Rate: 1e-05\n",
      "Epoch 2900, Regression Loss: 6.351320743560791, Classification AUC: 0.7554178127750297, Learning Rate: 1e-05\n",
      "Epoch 3000, Regression Loss: 6.08271598815918, Classification AUC: 0.7576717855472709, Learning Rate: 1e-05\n",
      "Epoch 3100, Regression Loss: 5.823539733886719, Classification AUC: 0.7599054260908966, Learning Rate: 1e-05\n",
      "Epoch 3200, Regression Loss: 5.575069427490234, Classification AUC: 0.7620577377200601, Learning Rate: 1e-05\n",
      "Epoch 3300, Regression Loss: 5.336573123931885, Classification AUC: 0.7638934475036381, Learning Rate: 1e-05\n",
      "Epoch 3400, Regression Loss: 5.107306957244873, Classification AUC: 0.7653341311312561, Learning Rate: 1e-05\n",
      "Epoch 3500, Regression Loss: 4.891280651092529, Classification AUC: 0.7666644398035325, Learning Rate: 1e-05\n",
      "Epoch 3600, Regression Loss: 4.682522296905518, Classification AUC: 0.7680325083289522, Learning Rate: 1e-05\n",
      "Epoch 3700, Regression Loss: 4.481106281280518, Classification AUC: 0.7693831492298442, Learning Rate: 1e-05\n",
      "Epoch 3800, Regression Loss: 4.290937900543213, Classification AUC: 0.7705711323018117, Learning Rate: 1e-05\n",
      "Epoch 3900, Regression Loss: 4.110410213470459, Classification AUC: 0.7715354608590077, Learning Rate: 1e-05\n",
      "Epoch 4000, Regression Loss: 3.9399144649505615, Classification AUC: 0.7726450196206005, Learning Rate: 1e-05\n",
      "Epoch 4100, Regression Loss: 3.7775683403015137, Classification AUC: 0.7733740752466735, Learning Rate: 1e-05\n",
      "Epoch 4200, Regression Loss: 3.6226894855499268, Classification AUC: 0.7742164104321759, Learning Rate: 1e-05\n",
      "Epoch 4300, Regression Loss: 3.4787371158599854, Classification AUC: 0.7750122719522715, Learning Rate: 1e-05\n",
      "Epoch 4400, Regression Loss: 3.344650983810425, Classification AUC: 0.7757790874314876, Learning Rate: 1e-05\n",
      "Epoch 4500, Regression Loss: 3.219762086868286, Classification AUC: 0.7763919588940431, Learning Rate: 1e-05\n",
      "Epoch 4600, Regression Loss: 3.105555534362793, Classification AUC: 0.7769438336707515, Learning Rate: 1e-05\n",
      "Epoch 4700, Regression Loss: 3.0015649795532227, Classification AUC: 0.7774608531984049, Learning Rate: 1e-05\n",
      "Epoch 4800, Regression Loss: 2.90655255317688, Classification AUC: 0.7780272509955531, Learning Rate: 1e-05\n",
      "Epoch 4900, Regression Loss: 2.8204586505889893, Classification AUC: 0.7784687508169198, Learning Rate: 1e-05\n",
      "Epoch 5000, Regression Loss: 2.740804433822632, Classification AUC: 0.7788724907851434, Learning Rate: 1e-05\n",
      "Epoch 5100, Regression Loss: 2.6687979698181152, Classification AUC: 0.7792442801083999, Learning Rate: 1e-05\n",
      "Epoch 5200, Regression Loss: 2.6044111251831055, Classification AUC: 0.779674161513415, Learning Rate: 1e-05\n",
      "Epoch 5300, Regression Loss: 2.54426646232605, Classification AUC: 0.7800198093998798, Learning Rate: 1e-05\n",
      "Epoch 5400, Regression Loss: 2.490370988845825, Classification AUC: 0.7803828849108722, Learning Rate: 1e-05\n",
      "Epoch 5500, Regression Loss: 2.4428277015686035, Classification AUC: 0.7806413946746988, Learning Rate: 1e-05\n",
      "Epoch 5600, Regression Loss: 2.4022915363311768, Classification AUC: 0.7808679537935581, Learning Rate: 1e-05\n",
      "Epoch 5700, Regression Loss: 2.3675529956817627, Classification AUC: 0.7811497003900882, Learning Rate: 1e-05\n",
      "Epoch 5800, Regression Loss: 2.3351476192474365, Classification AUC: 0.781309453614925, Learning Rate: 1e-05\n",
      "Epoch 5900, Regression Loss: 2.305635929107666, Classification AUC: 0.7814866344642892, Learning Rate: 1e-05\n",
      "Epoch 6000, Regression Loss: 2.2780401706695557, Classification AUC: 0.7816202462523346, Learning Rate: 1e-05\n",
      "Epoch 6100, Regression Loss: 2.252882480621338, Classification AUC: 0.7818787560161612, Learning Rate: 1e-05\n",
      "Epoch 6200, Regression Loss: 2.2300515174865723, Classification AUC: 0.7820646506777894, Learning Rate: 1e-05\n",
      "Epoch 6300, Regression Loss: 2.209204912185669, Classification AUC: 0.7821256473636361, Learning Rate: 1e-05\n",
      "Epoch 6400, Regression Loss: 2.1898176670074463, Classification AUC: 0.7822447361312416, Learning Rate: 1e-05\n",
      "Epoch 6500, Regression Loss: 2.171726703643799, Classification AUC: 0.7823347788579678, Learning Rate: 1e-05\n",
      "Epoch 6600, Regression Loss: 2.15470027923584, Classification AUC: 0.7825903840177064, Learning Rate: 1e-05\n",
      "Epoch 6700, Regression Loss: 2.1383094787597656, Classification AUC: 0.782761755658895, Learning Rate: 1e-05\n",
      "Epoch 6800, Regression Loss: 2.1231470108032227, Classification AUC: 0.7828488937815331, Learning Rate: 1e-05\n",
      "Epoch 6900, Regression Loss: 2.1090052127838135, Classification AUC: 0.7830405976513372, Learning Rate: 1e-05\n",
      "Epoch 7000, Regression Loss: 2.0954883098602295, Classification AUC: 0.783188732459822, Learning Rate: 1e-05\n",
      "Epoch 7100, Regression Loss: 2.0827438831329346, Classification AUC: 0.7832642521661086, Learning Rate: 1e-05\n",
      "Epoch 7200, Regression Loss: 2.07049560546875, Classification AUC: 0.7833804363296261, Learning Rate: 1e-05\n",
      "Epoch 7300, Regression Loss: 2.0586512088775635, Classification AUC: 0.7834501468277365, Learning Rate: 1e-05\n",
      "Epoch 7400, Regression Loss: 2.046942949295044, Classification AUC: 0.7834820974727039, Learning Rate: 1e-05\n",
      "Epoch 7500, Regression Loss: 2.035914421081543, Classification AUC: 0.7834966204931436, Learning Rate: 1e-05\n",
      "Epoch 7600, Regression Loss: 2.0250372886657715, Classification AUC: 0.7835924724280457, Learning Rate: 1e-05\n",
      "Epoch 7700, Regression Loss: 2.0146496295928955, Classification AUC: 0.7836331368852769, Learning Rate: 1e-05\n",
      "Epoch 7800, Regression Loss: 2.004856586456299, Classification AUC: 0.7836389460934526, Learning Rate: 1e-05\n",
      "Epoch 7900, Regression Loss: 1.9954150915145874, Classification AUC: 0.7837406072365306, Learning Rate: 1e-05\n",
      "Epoch 8000, Regression Loss: 1.9866409301757812, Classification AUC: 0.7838277453591689, Learning Rate: 1e-05\n",
      "Epoch 8100, Regression Loss: 1.9782919883728027, Classification AUC: 0.7839758801676536, Learning Rate: 1e-05\n",
      "Epoch 8200, Regression Loss: 1.9701848030090332, Classification AUC: 0.7841065873516112, Learning Rate: 1e-05\n",
      "Epoch 8300, Regression Loss: 1.9625545740127563, Classification AUC: 0.7842895774091511, Learning Rate: 1e-05\n",
      "Epoch 8400, Regression Loss: 1.9548877477645874, Classification AUC: 0.7844406168217241, Learning Rate: 1e-05\n",
      "Epoch 8500, Regression Loss: 1.947369933128357, Classification AUC: 0.78454518256889, Learning Rate: 1e-05\n",
      "Epoch 8600, Regression Loss: 1.9402782917022705, Classification AUC: 0.7847020311896387, Learning Rate: 1e-05\n",
      "Epoch 8700, Regression Loss: 1.9331589937210083, Classification AUC: 0.7848356429776838, Learning Rate: 1e-05\n",
      "Epoch 8800, Regression Loss: 1.9262287616729736, Classification AUC: 0.7849605409534653, Learning Rate: 1e-05\n",
      "Epoch 8900, Regression Loss: 1.9195799827575684, Classification AUC: 0.7851595063334892, Learning Rate: 1e-05\n",
      "Epoch 9000, Regression Loss: 1.913319706916809, Classification AUC: 0.7852480967581714, Learning Rate: 1e-05\n",
      "Epoch 9100, Regression Loss: 1.9071050882339478, Classification AUC: 0.7854281822116236, Learning Rate: 1e-05\n",
      "Epoch 9200, Regression Loss: 1.9010380506515503, Classification AUC: 0.7855298433547016, Learning Rate: 1e-05\n",
      "Epoch 9300, Regression Loss: 1.8953142166137695, Classification AUC: 0.7856663597468347, Learning Rate: 1e-05\n",
      "Epoch 9400, Regression Loss: 1.8899247646331787, Classification AUC: 0.7857273564326814, Learning Rate: 1e-05\n",
      "Epoch 9500, Regression Loss: 1.8846126794815063, Classification AUC: 0.7857796393062643, Learning Rate: 1e-05\n",
      "Epoch 9600, Regression Loss: 1.8795464038848877, Classification AUC: 0.7858958234697819, Learning Rate: 1e-05\n",
      "Epoch 9700, Regression Loss: 1.8746347427368164, Classification AUC: 0.7859219649065734, Learning Rate: 1e-05\n",
      "Epoch 9800, Regression Loss: 1.8698914051055908, Classification AUC: 0.7859771523842444, Learning Rate: 1e-05\n",
      "Epoch 9900, Regression Loss: 1.8652838468551636, Classification AUC: 0.7860468628823549, Learning Rate: 1e-05\n",
      "Test Regression MAE: 4.420950889587402\n",
      "Test Classification AUC: 0.6535890410958903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(4.420951, dtype=float32), 0.6535890410958903)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prota predykcja relu + relu\n",
    "X_train, X_test, y_train, y_test = preprocess_weather_data(aggregated_data, window_size=3,dayoffset = 1)\n",
    "from weather_prediction import WeatherPredictionNetwork\n",
    "print(X_train.shape[1])\n",
    "layers = [X_train.shape[1] * X_train.shape[2],256, 256, 2]\n",
    "activations = [\"relu\", \"relu\"]\n",
    "model = WeatherPredictionNetwork(layers, activations, seed=42)\n",
    "\n",
    "train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=10000, learning_rate=0.0001, rate = [2500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_weather_data_all_y(data, window_size=3, dayoffset=0):\n",
    "    X, y = [], []\n",
    "    for i in range(window_size, len(data) - 1):\n",
    "        X_window = data.iloc[i-window_size:i][[\n",
    "            'mean_temperature', 'humidity_mean', 'pressure_mean', 'max_wind_speed', 'wind_speed_mean', 'wind_direction_mean'\n",
    "        ]].values\n",
    "        y_target = data.iloc[i + dayoffset][['mean_temperature', 'humidity_mean', 'pressure_mean', 'max_wind_speed', 'wind_speed_mean', 'wind_direction_mean']].values\n",
    "\n",
    "        X.append(X_window)\n",
    "        y.append(y_target)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # X_mean = X.mean(axis=(0, 1), keepdims=True)\n",
    "    # X_std = X.std(axis=(0, 1), keepdims=True)\n",
    "    # X_normalized = (X - X_mean) / (X_std + 1e-9)\n",
    "\n",
    "    train_size = int(0.7 * len(X))\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    X_mean = X_train.mean(axis=(0, 1), keepdims=True)\n",
    "    X_std = X_train.std(axis=(0, 1), keepdims=True)\n",
    "    X_train = (X_train - X_mean) / (X_std + 1e-9)\n",
    "    X_test = (X_test - X_mean) / (X_std + 1e-9)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Network.train() got an unexpected keyword argument 'lower_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m activations \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m Network(layers, activations, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(model, X_train, y_train, X_test, y_test, epochs, learning_rate, rate)\u001b[0m\n\u001b[0;32m      4\u001b[0m X_test_cp \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39marray(X_test\u001b[38;5;241m.\u001b[39mreshape(X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mcp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      5\u001b[0m y_test_cp \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39marray(y_test, dtype\u001b[38;5;241m=\u001b[39mcp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_cp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_cp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlower_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_cp)\n\u001b[0;32m     11\u001b[0m mae \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mmean(cp\u001b[38;5;241m.\u001b[39mabs(predictions[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m y_test_cp[:, \u001b[38;5;241m0\u001b[39m]))\n",
      "\u001b[1;31mTypeError\u001b[0m: Network.train() got an unexpected keyword argument 'lower_rate'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_weather_data_all_y(aggregated_data, window_size=3,dayoffset = 0)\n",
    "print(X_train.shape[1])\n",
    "from neural_net import Network\n",
    "layers = [X_train.shape[1] * X_train.shape[2],256, 256, 6]\n",
    "activations = [\"relu\", \"relu\"]\n",
    "model = Network(layers, activations, seed=42)\n",
    "\n",
    "train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=10000, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_data(X, y):\n",
    "#     X = X.reshape(X.shape[0], -1)  # Flattening the time-series data\n",
    "#     X = cp.array(X, dtype=cp.float32)\n",
    "#     y = cp.array(y, dtype=cp.float32)\n",
    "#     return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from network_test import Network\n",
    "# # Dataset preparation\n",
    "# X_train = np.random.rand(1504, 7, 5)  # Training data (1504 samples, 7 days, 5 features per day)\n",
    "# y_train = np.random.rand(1504, 2)     # Training targets (1504 samples, 2 output features)\n",
    "# X_test = np.random.rand(377, 7, 5)    # Test data\n",
    "# y_test = np.random.rand(377, 2)       # Test targets\n",
    "\n",
    "# # Preprocess the data\n",
    "# X_train, y_train = preprocess_data(X_train, y_train)\n",
    "# X_test, y_test = preprocess_data(X_test, y_test)\n",
    "\n",
    "# # Initialize and train the network\n",
    "# activations = [\"relu\", \"relu\", \"relu\"] \n",
    "# nn = Network([X_train.shape[1], 64, 32, y_train.shape[1]], activations, loss_function=\"mse\", seed=123)\n",
    "# nn.train(X_train, y_train, epochs=500, learning_rate=0.001)\n",
    "\n",
    "# # Predict and evaluate\n",
    "# predictions = nn.predict(X_test)\n",
    "# print(\"Predictions (first 5):\", predictions[:5].get())\n",
    "# print(\"Actual (first 5):\", y_test[:5].get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cupy as cp\n",
    "# from network_test import Network\n",
    "\n",
    "# # Dataset preparation\n",
    "# X_train = np.random.rand(1504, 7, 5)  # Training data (1504 samples, 7 days, 5 features per day)\n",
    "# y_train = np.random.rand(1504, 2)     # Training targets (1504 samples, 2 output features)\n",
    "# X_test = np.random.rand(377, 7, 5)    # Test data\n",
    "# y_test = np.random.rand(377, 2)       # Test targets\n",
    "\n",
    "# # Preprocess the data (no flattening for time-series)\n",
    "# def preprocess_data(X, y):\n",
    "#     # Flattening the time-series data to (samples, days*features)\n",
    "#     X = X.reshape(X.shape[0], -1)  # Now shape is (samples, days*features)\n",
    "#     X = cp.array(X, dtype=cp.float32)\n",
    "#     y = cp.array(y, dtype=cp.float32)\n",
    "#     return X, y\n",
    "\n",
    "# # Preprocess training and testing data\n",
    "# X_train, y_train = preprocess_data(X_train, y_train)\n",
    "# X_test, y_test = preprocess_data(X_test, y_test)\n",
    "\n",
    "# # Initialize and train the network\n",
    "# activations = [\"relu\", \"relu\", \"relu\"]  # Relu for hidden layers\n",
    "# nn = Network([X_train.shape[1], 64, 32, y_train.shape[1]], activations, loss_function=\"mse\", seed=123)\n",
    "\n",
    "# # Train the network\n",
    "# nn.train(X_train, y_train, epochs=500, learning_rate=0.001)\n",
    "\n",
    "# # Predict and evaluate\n",
    "# predictions = nn.predict(X_test)\n",
    "# print(\"Predictions (first 5):\", predictions[:5].get())\n",
    "# print(\"Actual (first 5):\", y_test[:5].get())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import cupy as cp\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# class NeuralNetwork:\n",
    "#     def __init__(self, layers, activation=\"relu\", loss=\"mse\"):\n",
    "#         self.layers = layers\n",
    "#         self.activation = activation\n",
    "#         self.loss = loss\n",
    "#         self.weights = []\n",
    "#         self.biases = []\n",
    "#         self.initialize_weights()\n",
    "\n",
    "#     def initialize_weights(self):\n",
    "#         for i in range(len(self.layers) - 1):\n",
    "#             weight = cp.random.randn(self.layers[i], self.layers[i + 1]) * cp.sqrt(2. / self.layers[i])\n",
    "#             bias = cp.zeros((1, self.layers[i + 1]))\n",
    "#             self.weights.append(weight)\n",
    "#             self.biases.append(bias)\n",
    "\n",
    "#     def activate(self, z, activation):\n",
    "#         if activation == \"sigmoid\":\n",
    "#             return 1 / (1 + cp.exp(-z))\n",
    "#         elif activation == \"relu\":\n",
    "#             return cp.maximum(0, z)\n",
    "#         elif activation == \"linear\":\n",
    "#             return z\n",
    "#         else:\n",
    "#             raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "#     def activate_derivative(self, a, activation):\n",
    "#         if activation == \"sigmoid\":\n",
    "#             return a * (1 - a)\n",
    "#         elif activation == \"relu\":\n",
    "#             return (a > 0).astype(cp.float32)\n",
    "#         elif activation == \"linear\":\n",
    "#             return cp.ones_like(a)\n",
    "#         else:\n",
    "#             raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         activations = [X]\n",
    "#         z_values = []\n",
    "\n",
    "#         for i in range(len(self.weights)):\n",
    "#             z = cp.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "#             a = self.activate(z, \"linear\" if i == len(self.weights) - 1 else self.activation)\n",
    "#             z_values.append(z)\n",
    "#             activations.append(a)\n",
    "\n",
    "#         return activations, z_values\n",
    "\n",
    "#     def backward(self, X, y, activations, z_values):\n",
    "#         deltas = []\n",
    "#         delta = activations[-1] - y\n",
    "#         deltas.append(delta)\n",
    "\n",
    "#         for i in reversed(range(len(self.weights) - 1)):\n",
    "#             delta = cp.dot(deltas[0], self.weights[i + 1].T) * self.activate_derivative(activations[i + 1], self.activation)\n",
    "#             deltas.insert(0, delta)\n",
    "\n",
    "#         weight_grads = []\n",
    "#         bias_grads = []\n",
    "\n",
    "#         for i in range(len(self.weights)):\n",
    "#             weight_grad = cp.dot(activations[i].T, deltas[i]) / X.shape[0]\n",
    "#             bias_grad = cp.mean(deltas[i], axis=0, keepdims=True)\n",
    "#             weight_grads.append(weight_grad)\n",
    "#             bias_grads.append(bias_grad)\n",
    "\n",
    "#         return weight_grads, bias_grads\n",
    "\n",
    "#     def update_parameters(self, weight_grads, bias_grads, learning_rate):\n",
    "#         for i in range(len(self.weights)):\n",
    "#             self.weights[i] -= learning_rate * weight_grads[i]\n",
    "#             self.biases[i] -= learning_rate * bias_grads[i]\n",
    "\n",
    "#     def fit(self, X, y, epochs=1000, learning_rate=0.01):\n",
    "#         for epoch in range(epochs):\n",
    "#             activations, z_values = self.forward(X)\n",
    "#             weight_grads, bias_grads = self.backward(X, y, activations, z_values)\n",
    "#             self.update_parameters(weight_grads, bias_grads, learning_rate)\n",
    "\n",
    "#             if epoch % 100 == 0:\n",
    "#                 loss = self.calculate_loss(y, activations[-1])\n",
    "#                 print(f\"Epoch {epoch}/{epochs} - Loss: {loss}\")\n",
    "\n",
    "#     def calculate_loss(self, y, y_pred):\n",
    "#         if self.loss == \"mse\":\n",
    "#             return cp.mean((y - y_pred) ** 2)\n",
    "#         else:\n",
    "#             raise ValueError(\"Unsupported loss function\")\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         activations, _ = self.forward(X)\n",
    "#         return activations[-1]\n",
    "\n",
    "# def preprocess_data(X, y):\n",
    "#     X = X.reshape(X.shape[0], -1)  # Flatten time-series data\n",
    "#     mean_X = np.mean(X, axis=0)\n",
    "#     std_X = np.std(X, axis=0)\n",
    "#     X_normalized = (X - mean_X) / std_X  # Normalize inputs\n",
    "\n",
    "#     mean_y = np.mean(y, axis=0)\n",
    "#     std_y = np.std(y, axis=0)\n",
    "#     y_normalized = (y - mean_y) / std_y  # Normalize targets\n",
    "\n",
    "#     return (\n",
    "#         cp.array(X_normalized, dtype=cp.float32),\n",
    "#         cp.array(y_normalized, dtype=cp.float32),\n",
    "#         mean_X, std_X,\n",
    "#         mean_y, std_y\n",
    "#     )\n",
    "\n",
    "# def denormalize(predictions, mean_y, std_y):\n",
    "#     mean_y = cp.asnumpy(mean_y)  # Convert cupy to numpy\n",
    "#     std_y = cp.asnumpy(std_y)    # Convert cupy to numpy\n",
    "#     predictions = np.array(predictions)  # Ensure numpy array\n",
    "#     return predictions * std_y + mean_y\n",
    "\n",
    "\n",
    "\n",
    "# # Dataset preparation\n",
    "# # X_train = np.random.rand(1504, 7, 5)\n",
    "# # y_train = np.random.rand(1504, 2)\n",
    "# # X_test = np.random.rand(377, 7, 5)\n",
    "# # y_test = np.random.rand(377, 2)\n",
    "\n",
    "# # Preprocess the data\n",
    "# X_train, y_train, mean_X, std_X, mean_y, std_y = preprocess_data(X_train, y_train)\n",
    "# X_test, y_test, _, _, _, _ = preprocess_data(X_test, y_test)\n",
    "\n",
    "# # Initialize the neural network\n",
    "# nn = NeuralNetwork(\n",
    "#     layers=[X_train.shape[1], 64, 32, y_train.shape[1]],  # Ensure 2 outputs for temperature and wind speed\n",
    "#     activation=\"relu\",\n",
    "#     loss=\"mse\",\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# nn.fit(X_train, y_train, epochs=15000, learning_rate=0.01)\n",
    "\n",
    "# # Predict normalized values\n",
    "# predictions_normalized = nn.predict(X_test).get()\n",
    "\n",
    "# # Denormalize the predictions\n",
    "# predictions_denormalized = denormalize(predictions_normalized, mean_y, std_y)\n",
    "\n",
    "# # Print results\n",
    "# print(\"Predictions (first 5):\", predictions_denormalized[:5])\n",
    "# print(\"Actual (first 5):\", y_test[:5].get())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7752,
     "sourceId": 11375,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
