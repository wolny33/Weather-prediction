{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 11375,
          "sourceType": "datasetVersion",
          "datasetId": 7752
        }
      ],
      "dockerImageVersionId": 30822,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport kagglehub\nimport cupy as cp",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T12:11:31.177573Z",
          "iopub.execute_input": "2025-01-28T12:11:31.177847Z",
          "iopub.status.idle": "2025-01-28T12:11:31.631300Z",
          "shell.execute_reply.started": "2025-01-28T12:11:31.177827Z",
          "shell.execute_reply": "2025-01-28T12:11:31.630553Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "from sklearn.metrics import roc_auc_score\n\nimport numpy as np\nimport cupy as cp\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\n\nclass WeatherPredictionNetwork:\n    def __init__(self, layers, activations, binary_output, seed=None, l2_lambda=0.01):\n        if seed is not None:\n            cp.random.seed(seed)\n\n        self.num_layers = len(layers)\n        self.weights = []\n        self.biases = []\n        self.activations = activations\n        self.l2_lambda = l2_lambda\n        self.binary_output = binary_output\n\n        self.weight_error_history = []  # Initialize for weight updates\n        self.bias_error_history = []    # Initialize for bias updates\n        self.training_mae = []  # Track MAE for training\n        self.training_auc = []  # Track AUC for training\n        self.testing_mae = []  # Track MAE for testing\n        self.testing_auc = []  # Track AUC for testing\n\n        for i in range(self.num_layers - 1):\n            limit = cp.sqrt(6 / (layers[i] + layers[i + 1]))\n            self.weights.append(cp.random.uniform(-limit, limit, (layers[i], layers[i + 1])).astype(cp.float32))\n            self.biases.append(cp.zeros((1, layers[i + 1]), dtype=cp.float32))\n\n    def apply_activation(self, z, activation):\n        if activation == \"sigmoid\":\n            return 1 / (1 + cp.exp(-cp.clip(z, -10, 10)))\n        elif activation == \"relu\":\n            return cp.maximum(0, z)\n        elif activation == \"tanh\":\n            return cp.tanh(z)\n        elif activation == \"linear\":\n            return z\n        elif activation == \"softmax\":\n            z_stable = z - cp.max(z, axis=1, keepdims=True)\n            exps = cp.exp(z_stable)\n            return exps / cp.sum(exps, axis=1, keepdims=True)\n\n    def apply_activation_derivative(self, a, activation):\n        if activation == \"sigmoid\":\n            return a * (1 - a)\n        elif activation == \"relu\":\n            return cp.where(a > 0, 1, 0)\n        elif activation == \"tanh\":\n            return 1 - a**2\n        elif activation == \"linear\":\n            return 1\n\n    def forward(self, X):\n        self.activations_values = [X]\n        self.z_values = []\n\n        for i in range(self.num_layers - 2):\n            z = cp.dot(self.activations_values[-1], self.weights[i]) + self.biases[i]\n            self.z_values.append(z)\n            a = self.apply_activation(z, self.activations[i])\n            self.activations_values.append(a)\n\n        z_last = cp.dot(self.activations_values[-1], self.weights[-1]) + self.biases[-1]\n        self.z_values.append(z_last)\n        \n        z_reg, z_cls = z_last[:, 0:1], z_last[:, 1:]\n        a_reg = self.apply_activation(z_reg, \"linear\")\n        if self.binary_output:\n            a_cls = self.apply_activation(z_cls, \"sigmoid\")\n        else:\n            a_cls = self.apply_activation(z_cls, \"linear\")\n        \n        self.activations_values.append(cp.hstack([a_reg, a_cls]))\n\n        return self.activations_values[-1]\n\n    def backward(self, X, y, output, learning_rate):\n        deltas = []\n\n        y_reg, y_cls = y[:, 0:1], y[:, 1:]\n        if self.binary_output:\n            output_reg, output_cls = output[:, 0:1], cp.clip(output[:, 1:], 1e-9, 1 - 1e-9)\n        else:\n            output_reg, output_cls = output[:, 0:1], output[:, 1:]\n\n        reg_error = y_reg - output_reg\n        reg_delta = reg_error * self.apply_activation_derivative(output_reg, \"linear\")\n\n        if self.binary_output:\n            cls_error = y_cls - output_cls\n            cls_delta = cls_error * self.apply_activation_derivative(output_cls, \"sigmoid\")\n        else:\n            cls_error = y_cls - output_cls\n            cls_delta = cls_error * self.apply_activation_derivative(output_cls, \"linear\")\n\n        output_delta = cp.hstack([reg_delta, cls_delta])\n        deltas.append(output_delta)\n\n        for i in range(self.num_layers - 2, 0, -1):\n            z = self.z_values[i - 1]\n            delta = cp.dot(deltas[-1], self.weights[i].T) * self.apply_activation_derivative(self.activations_values[i], self.activations[i - 1])\n            deltas.append(delta)\n\n        deltas.reverse()\n\n        for i in range(self.num_layers - 1):\n            grad_w = cp.dot(self.activations_values[i].T, deltas[i]) + self.l2_lambda * self.weights[i]\n            grad_b = cp.sum(deltas[i], axis=0, keepdims=True)\n\n            grad_w = cp.clip(grad_w, -1.0, 1.0)\n            grad_b = cp.clip(grad_b, -1.0, 1.0)\n\n            # Append the norms of gradients to history\n            self.weight_error_history.append(cp.linalg.norm(grad_w))\n            self.bias_error_history.append(cp.linalg.norm(grad_b))\n\n            self.weights[i] += learning_rate * grad_w\n            self.biases[i] += learning_rate * grad_b\n\n    def clip_weights(self, clip_value=1.0):\n        for i in range(len(self.weights)):\n            self.weights[i] = cp.clip(self.weights[i], -clip_value, clip_value)\n\n    def plot_error_history(self):\n        weight_error_history = cp.array(self.weight_error_history).get()  # Convert to NumPy\n        bias_error_history = cp.array(self.bias_error_history).get()  # Convert to NumPy\n\n        def running_mean(data, window_size):\n            return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n\n        num_layers = self.num_layers - 1\n\n        for layer in range(num_layers):\n            fig, ax = plt.subplots(figsize=(10, 5))\n            layer_weight_errors = weight_error_history[layer::num_layers]\n            smoothed_weight_errors = running_mean(layer_weight_errors, window_size=200)\n            ax.plot(smoothed_weight_errors, label=f'Weight Update Norms (Layer {layer+1} -> {layer+2})')\n            ax.set_title(f'Weight Error Over Epochs (Layer {layer+1} -> {layer+2})')\n            ax.set_xlabel('Epochs')\n            ax.set_ylabel('Error (Norm)')\n            ax.legend()\n            plt.tight_layout()\n            plt.show()\n\n        for layer in range(num_layers):\n            fig, ax = plt.subplots(figsize=(10, 5))\n            layer_bias_errors = bias_error_history[layer::num_layers]\n            smoothed_bias_errors = running_mean(layer_bias_errors, window_size=200)\n            ax.plot(smoothed_bias_errors, label=f'Bias Update Norms (Layer {layer+1} -> {layer+2})')\n            ax.set_title(f'Bias Error Over Epochs (Layer {layer+1} -> {layer+2})')\n            ax.set_xlabel('Epochs')\n            ax.set_ylabel('Error (Norm)')\n            ax.legend()\n            plt.tight_layout()\n            plt.show()\n\n    def train(self, X, y, X_test, y_test, epochs, learning_rate, lower_rate=[2500], batch_size=32):\n        num_samples = X.shape[0]\n\n        for epoch in range(epochs):\n            if epoch in lower_rate:\n                learning_rate = learning_rate / 10\n\n            permutation = cp.random.permutation(num_samples)\n            X_shuffled = X[permutation]\n            y_shuffled = y[permutation]\n\n            for i in range(0, num_samples, batch_size): \n                X_batch = X_shuffled[i:i + batch_size]\n                y_batch = y_shuffled[i:i + batch_size]\n                output = self.forward(X_batch)\n                self.backward(X_batch, y_batch, output, learning_rate)\n\n            if epoch % 100 == 0:\n                # Training metrics\n                output = self.forward(X)\n                train_mae = cp.mean(cp.abs(y[:, 0] - output[:, 0])).get()\n                if self.binary_output:\n                    train_auc = roc_auc_score(cp.asnumpy(y[:, 1]), cp.asnumpy(output[:, 1]))\n                else:\n                    cls_binary_output = (cp.asnumpy(output[:, 1]) >= 6).astype(cp.float32)\n                    y_bin = (cp.asnumpy(y[:, 1]) >= 6).astype(cp.float32)\n                    train_auc = roc_auc_score(y_bin, cls_binary_output)\n\n                self.training_mae.append(train_mae)\n                self.training_auc.append(train_auc)\n\n                # Testing metrics\n                predictions = self.predict(X_test)\n                test_mae = cp.mean(cp.abs(predictions[:, 0] - y_test[:, 0])).get()\n                if self.binary_output:\n                    test_auc = roc_auc_score(cp.asnumpy(y_test[:, 1]), cp.asnumpy(predictions[:, 1]))\n                else:\n                    cls_binary_output = (cp.asnumpy(predictions[:, 1]) >= 6).astype(cp.float32)\n                    y_bin = (cp.asnumpy(y_test[:, 1]) >= 6).astype(cp.float32)\n                    test_auc = roc_auc_score(y_bin, cls_binary_output)\n\n                self.testing_mae.append(test_mae)\n                self.testing_auc.append(test_auc)\n\n                print(f\"Epoch {epoch}: Train MAE = {train_mae}, Train AUC = {train_auc}, Test MAE = {test_mae}, Test AUC = {test_auc}\")\n\n        self.plot_training_testing_metrics()\n\n    def plot_training_testing_metrics(self):\n        epochs = range(0, len(self.training_mae) * 100, 100)\n        print(\"Training MAE:\", self.training_mae)  # Debug print\n        print(\"Testing MAE:\", self.testing_mae)    # Debug print\n        print(\"Training AUC:\", self.training_auc)  # Debug print\n        print(\"Testing AUC:\", self.testing_auc)    # Debug print\n\n        plt.figure(figsize=(12, 6))\n\n        # Plot MAE\n        plt.subplot(1, 2, 1)\n        plt.plot(epochs, self.training_mae, label=\"Training MAE\")\n        plt.plot(epochs, self.testing_mae, label=\"Testing MAE\")\n        plt.ylim([0, 5])\n        plt.title(\"MAE Over Epochs\")\n        plt.xlabel(\"Epochs\")\n        plt.ylabel(\"MAE\")\n        plt.legend()\n\n        # Plot AUC\n        plt.subplot(1, 2, 2)\n        plt.plot(epochs, self.training_auc, label=\"Training AUC\")\n        plt.plot(epochs, self.testing_auc, label=\"Testing AUC\")\n        plt.title(\"AUC Over Epochs\")\n        plt.xlabel(\"Epochs\")\n        plt.ylabel(\"AUC\")\n        plt.legend()\n\n        plt.tight_layout()\n        plt.show()\n\n    def predict(self, X):\n        output = self.forward(X)\n        reg_output = output[:, 0]\n        if self.binary_output:\n            # cls_output = (output[:, 1] >= 0.5).astype(cp.float32)\n            cls_output = (output[:, 1]).astype(cp.float32)\n        else:\n            cls_output = (output[:, 1] >= 6).astype(cp.float32)\n        return cp.hstack([reg_output[:, None], cls_output[:, None]])",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T12:11:33.626604Z",
          "iopub.execute_input": "2025-01-28T12:11:33.627083Z",
          "iopub.status.idle": "2025-01-28T12:11:33.654727Z",
          "shell.execute_reply.started": "2025-01-28T12:11:33.627043Z",
          "shell.execute_reply": "2025-01-28T12:11:33.653880Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "historical_hourly_weather_data_path = kagglehub.dataset_download('selfishgene/historical-hourly-weather-data')\n\ncity_attributes = pd.read_csv(f\"{historical_hourly_weather_data_path}/city_attributes.csv\")\nhumidity = pd.read_csv(f\"{historical_hourly_weather_data_path}/humidity.csv\")\npressure = pd.read_csv(f\"{historical_hourly_weather_data_path}/pressure.csv\")\ntemperature = pd.read_csv(f\"{historical_hourly_weather_data_path}/temperature.csv\")\nweather_description = pd.read_csv(f\"{historical_hourly_weather_data_path}/weather_description.csv\")\nwind_speed = pd.read_csv(f\"{historical_hourly_weather_data_path}/wind_speed.csv\")\nwind_direction = pd.read_csv(f\"{historical_hourly_weather_data_path}/wind_direction.csv\")\n\ndata_frames = []\nfor city in city_attributes['City']:\n    city_data = pd.DataFrame({\n        'datetime': pd.to_datetime(humidity['datetime']),\n        'humidity': humidity[city],\n        'pressure': pressure[city],\n        'temperature': temperature[city],\n        'weather_description': weather_description[city],\n        'wind_speed': wind_speed[city],\n        'wind_direction': wind_direction[city],\n        'latitude': city_attributes.loc[city_attributes['City'] == city, 'Latitude'].values[0],\n        'longitude': city_attributes.loc[city_attributes['City'] == city, 'Longitude'].values[0],\n        'city': city\n    })\n    city_data.set_index('datetime', inplace=True)\n    data_frames.append(city_data)\n\ncombined_data = pd.concat(data_frames)\n\ncombined_data = combined_data.ffill().bfill().interpolate()\n\naggregated_data = combined_data.groupby(['city']).resample('D').agg({\n    'temperature': 'mean',\n    'humidity': 'mean',\n    'wind_speed': ['max', 'mean'],\n    'pressure': 'mean',\n    'weather_description': lambda x: x.mode()[0] if not x.mode().empty else np.nan,\n    'wind_direction': 'mean',\n    'latitude': 'mean',\n    'longitude': 'mean'\n}).reset_index()\n\naggregated_data.columns = [\n    '_'.join(col).strip('_') if isinstance(col, tuple) else col for col in aggregated_data.columns\n]\n",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T12:12:31.086559Z",
          "iopub.execute_input": "2025-01-28T12:12:31.086839Z",
          "iopub.status.idle": "2025-01-28T12:12:47.121419Z",
          "shell.execute_reply.started": "2025-01-28T12:12:31.086818Z",
          "shell.execute_reply": "2025-01-28T12:12:47.120524Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "one_hot_weather_description = pd.get_dummies(\n    aggregated_data['weather_description_<lambda>'],\n    prefix='weather_desc',\n    drop_first=False\n)\n\naggregated_data = pd.concat([aggregated_data, one_hot_weather_description], axis=1)\naggregated_data.drop(columns=['weather_description_<lambda>'], inplace=True)",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T12:12:51.326784Z",
          "iopub.execute_input": "2025-01-28T12:12:51.327210Z",
          "iopub.status.idle": "2025-01-28T12:12:51.354604Z",
          "shell.execute_reply.started": "2025-01-28T12:12:51.327173Z",
          "shell.execute_reply": "2025-01-28T12:12:51.353483Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "binary_output = True # True better\ntwo_models = True # True = 2.165, Flase = 2.151\nstandarize_2nd_data = False # False better",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T12:12:53.849388Z",
          "iopub.execute_input": "2025-01-28T12:12:53.849667Z",
          "iopub.status.idle": "2025-01-28T12:12:53.853373Z",
          "shell.execute_reply.started": "2025-01-28T12:12:53.849647Z",
          "shell.execute_reply": "2025-01-28T12:12:53.852460Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def preprocess_weather_data(data, binary_output=True, window_size=3, two_models=False, standarize_2nd_data=False, dayoffset=1):\n    X, y, X_2, y_2 = [], [], [], []\n    for i in range(window_size, len(data) - 1):\n        X_window = data.iloc[i-window_size:i][[\n            'temperature_mean', 'humidity_mean', 'wind_speed_max',\n            'wind_speed_mean', 'pressure_mean', 'wind_direction_mean',\n            'weather_desc_broken clouds', 'weather_desc_dust',\n            'weather_desc_few clouds', 'weather_desc_fog',\n            'weather_desc_freezing rain', 'weather_desc_haze',\n            'weather_desc_heavy intensity rain', 'weather_desc_light rain',\n            'weather_desc_mist', 'weather_desc_moderate rain',\n            'weather_desc_overcast clouds', 'weather_desc_scattered clouds',\n            'weather_desc_sky is clear', 'weather_desc_smoke', 'weather_desc_snow'\n        ]].values\n        y_target = data.iloc[i + dayoffset][['temperature_mean', 'wind_speed_max']].values\n\n\n        if binary_output:\n            # encode wind speed > 6 as binary\n            y_target[1] = 1 if y_target[1] >= 6 else 0\n            if two_models:\n                y_target2[1] = 1 if y_target2[1] >= 6 else 0\n        \n        X.append(X_window)\n        y.append(y_target)\n        if two_models:\n            X_2.append(y_target)\n            y_2.append(y_target2)\n\n    X = np.array(X)\n    y = np.array(y)\n    X_2 = np.array(X_2)\n    y_2 = np.array(y_2)\n\n    # split into train/test (0.7 or 0.8)\n    train_size = int(0.8 * len(X))\n    X_train, X_test = X[:train_size], X[train_size:]\n    y_train, y_test = y[:train_size], y[train_size:]\n\n    continuous_indices = [0, 1, 2, 3, 4, 5]\n\n    X_train_continuous = X_train[:, :, continuous_indices].astype(float) \n    X_test_continuous = X_test[:, :, continuous_indices].astype(float)\n\n    X_mean = X_train_continuous.mean(axis=(0, 1), keepdims=True)\n    X_std = X_train_continuous.std(axis=(0, 1), keepdims=True)\n\n    X_train[:, :, continuous_indices] = (X_train_continuous - X_mean) / (X_std + 1e-9)\n    X_test[:, :, continuous_indices] = (X_test_continuous - X_mean) / (X_std + 1e-9)\n\n\n    return X_train, X_test, y_train, y_test\n",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T12:12:55.802622Z",
          "iopub.execute_input": "2025-01-28T12:12:55.802932Z",
          "iopub.status.idle": "2025-01-28T12:12:55.810639Z",
          "shell.execute_reply.started": "2025-01-28T12:12:55.802904Z",
          "shell.execute_reply": "2025-01-28T12:12:55.809841Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "X_train, X_test, y_train, y_test = preprocess_weather_data(aggregated_data, window_size=3, dayoffset=0)",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T12:16:25.947912Z",
          "iopub.execute_input": "2025-01-28T12:16:25.948263Z",
          "iopub.status.idle": "2025-01-28T12:17:25.770562Z",
          "shell.execute_reply.started": "2025-01-28T12:16:25.948232Z",
          "shell.execute_reply": "2025-01-28T12:17:25.769841Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "print(X_train.shape)\nprint(y_train.shape)\n\nprint(X_test.shape)\nprint(y_test.shape)",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T12:14:05.726449Z",
          "iopub.execute_input": "2025-01-28T12:14:05.726750Z",
          "iopub.status.idle": "2025-01-28T12:14:05.732205Z",
          "shell.execute_reply.started": "2025-01-28T12:14:05.726729Z",
          "shell.execute_reply": "2025-01-28T12:14:05.731295Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=1000, learning_rate=0.001,rate = [500], batch_size=256):\n    X_train_cp = cp.array(X_train.reshape(X_train.shape[0], -1), dtype=cp.float32)\n    y_train_cp = cp.array(y_train, dtype=cp.float32)\n    X_test_cp = cp.array(X_test.reshape(X_test.shape[0], -1), dtype=cp.float32)\n    y_test_cp = cp.array(y_test, dtype=cp.float32)\n\n    model.train(X=X_train_cp, y=y_train_cp, X_test=X_test_cp, y_test=y_test_cp, epochs=epochs, learning_rate=learning_rate ,lower_rate = rate,batch_size=batch_size )\n\n    predictions = model.predict(X_test_cp)\n    \n    mae = cp.mean(cp.abs(predictions[:, 0] - y_test_cp[:, 0]))\n\n    from sklearn.metrics import roc_auc_score\n    auc = roc_auc_score(cp.asnumpy(y_test_cp[:, 1]), cp.asnumpy(predictions[:, 1]))\n\n    print(f\"Test Regression MAE: {mae}\")\n    print(f\"Test Classification AUC: {auc}\")\n\n    return mae, auc",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T12:14:08.574083Z",
          "iopub.execute_input": "2025-01-28T12:14:08.574418Z",
          "iopub.status.idle": "2025-01-28T12:14:08.580331Z",
          "shell.execute_reply.started": "2025-01-28T12:14:08.574391Z",
          "shell.execute_reply": "2025-01-28T12:14:08.579337Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#predykcja na dzisiaj\n# X_train, X_test, y_train, y_test = preprocess_weather_data(aggregated_data, window_size=3)\n\nprint(X_train.shape[1])\nlayers = [X_train.shape[1] * X_train.shape[2],64, 32, 2]\nactivations = [\"sigmoid\", \"relu\"]\nmodel_today = WeatherPredictionNetwork(layers, activations, seed=42, binary_output=True)\n\ntrain_and_evaluate(model_today, X_train, y_train, X_test, y_test, epochs=200, learning_rate=0.0001, rate = [500, 750], batch_size=128)",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T12:18:17.841330Z",
          "iopub.execute_input": "2025-01-28T12:18:17.841625Z",
          "iopub.status.idle": "2025-01-28T12:21:33.690990Z",
          "shell.execute_reply.started": "2025-01-28T12:18:17.841604Z",
          "shell.execute_reply": "2025-01-28T12:21:33.690321Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def preprocess_weather_data_with_today_prediction(model = model_today):\n    X_train2, X_test2, y_train2, y_test2 = preprocess_weather_data(aggregated_data, window_size=3, dayoffset=1)\n    X_train_cp = cp.array(X_train.reshape(X_train.shape[0], -1), dtype=cp.float32)\n    y_train_cp = cp.array(y_train, dtype=cp.float32)\n    X_test_cp = cp.array(X_test.reshape(X_test.shape[0], -1), dtype=cp.float32)\n    y_test_cp = cp.array(y_test, dtype=cp.float32)\n    expanded_X_train = []\n    for sample in X_train_cp:\n        # Predict for each individual sample\n        prediction = model.predict(sample.reshape(1, -1))  # Reshape to 2D array (1, features)\n        expanded_sample = cp.hstack([sample, prediction.flatten()])  # Add prediction to sample\n        expanded_X_train.append(expanded_sample)\n\n    expanded_X_test = []\n    for sample in X_test_cp:\n        # Predict for each individual sample\n        prediction = model.predict(sample.reshape(1, -1))  # Reshape to 2D array (1, features)\n        expanded_sample = cp.hstack([sample, prediction.flatten()])  # Add prediction to sample\n        expanded_X_test.append(expanded_sample)\n\n    # Convert the expanded lists back into numpy arrays\n    expanded_X_train = cp.array(expanded_X_train)\n    expanded_X_test = cp.array(expanded_X_test)\n    return expanded_X_train, expanded_X_test, y_train_cp, y_test_cp",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T12:24:18.534476Z",
          "iopub.execute_input": "2025-01-28T12:24:18.534908Z",
          "iopub.status.idle": "2025-01-28T12:24:18.541435Z",
          "shell.execute_reply.started": "2025-01-28T12:24:18.534873Z",
          "shell.execute_reply": "2025-01-28T12:24:18.540479Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "X_train2, X_test2, y_train2, y_test2 = preprocess_weather_data_with_today_prediction()\n# X_train = X_train[:, 6:]\n# X_test = X_test[:, 6:]\nprint(X_train.shape)\nprint(y_train.shape)\n\nprint(X_test.shape)\nprint(y_test.shape)",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T12:24:24.715166Z",
          "iopub.execute_input": "2025-01-28T12:24:24.715440Z",
          "iopub.status.idle": "2025-01-28T12:26:10.429665Z",
          "shell.execute_reply.started": "2025-01-28T12:24:24.715420Z",
          "shell.execute_reply": "2025-01-28T12:26:10.428903Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#predykcja na jutro z predykcją na dzisiaj\n\nlayers = [X_train.shape[1], 64,32, 2]\nactivations = [\"sigmoid\",\"linear\",]\nmodel2 = WeatherPredictionNetwork(layers, activations, seed=42, binary_output=True)\n\ntrain_and_evaluate(model2, X_train2, y_train2, X_test2, y_test2, epochs=200, learning_rate=0.00001, rate = [],batch_size=128)",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T12:29:21.701897Z",
          "iopub.execute_input": "2025-01-28T12:29:21.702297Z",
          "iopub.status.idle": "2025-01-28T12:32:09.927234Z",
          "shell.execute_reply.started": "2025-01-28T12:29:21.702267Z",
          "shell.execute_reply": "2025-01-28T12:32:09.926504Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#predykcja na jutro z predykcją na dzisiaj\n\nlayers = [X_train.shape[1], 64,32, 2]\nactivations = [\"sigmoid\",\"linear\",]\nmodel2 = WeatherPredictionNetwork(layers, activations, seed=42, binary_output=True)\n\ntrain_and_evaluate(model2, X_train2, y_train2, X_test2, y_test2, epochs=500, learning_rate=0.0001, rate = [],batch_size=256)",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T12:35:08.238013Z",
          "iopub.execute_input": "2025-01-28T12:35:08.238364Z",
          "iopub.status.idle": "2025-01-28T12:38:39.669772Z",
          "shell.execute_reply.started": "2025-01-28T12:35:08.238338Z",
          "shell.execute_reply": "2025-01-28T12:38:39.669063Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#predykcja na jutro z predykcją na dzisiaj\n\nlayers = [X_train.shape[1], 64,32, 2]\nactivations = [\"sigmoid\",\"relu\",]\nmodel2 = WeatherPredictionNetwork(layers, activations, seed=42, binary_output=True)\n\ntrain_and_evaluate(model2, X_train2, y_train2, X_test2, y_test2, epochs=500, learning_rate=0.0001, rate = [],batch_size=256)",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T12:38:39.670728Z",
          "iopub.execute_input": "2025-01-28T12:38:39.670973Z",
          "iopub.status.idle": "2025-01-28T12:42:34.149055Z",
          "shell.execute_reply.started": "2025-01-28T12:38:39.670953Z",
          "shell.execute_reply": "2025-01-28T12:42:34.148409Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#predykcja na jutro z predykcją na dzisiaj\n\nlayers = [X_train.shape[1], 128,64, 2]\nactivations = [\"sigmoid\",\"linear\",]\nmodel2 = WeatherPredictionNetwork(layers, activations, seed=42, binary_output=True)\n\ntrain_and_evaluate(model2, X_train2, y_train2, X_test2, y_test2, epochs=500, learning_rate=0.0001, rate = [],batch_size=256)",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T12:45:13.416565Z",
          "iopub.execute_input": "2025-01-28T12:45:13.416901Z",
          "iopub.status.idle": "2025-01-28T12:48:45.974867Z",
          "shell.execute_reply.started": "2025-01-28T12:45:13.416874Z",
          "shell.execute_reply": "2025-01-28T12:48:45.974098Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#predykcja na jutro z predykcją na dzisiaj\n\nlayers = [X_train.shape[1], 256,128, 2]\nactivations = [\"sigmoid\",\"linear\",]\nmodel2 = WeatherPredictionNetwork(layers, activations, seed=42, binary_output=True)\n\ntrain_and_evaluate(model2, X_train2, y_train2, X_test2, y_test2, epochs=400, learning_rate=0.0001, rate = [],batch_size=256)",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T12:51:48.460523Z",
          "iopub.execute_input": "2025-01-28T12:51:48.460817Z",
          "iopub.status.idle": "2025-01-28T12:54:37.839005Z",
          "shell.execute_reply.started": "2025-01-28T12:51:48.460794Z",
          "shell.execute_reply": "2025-01-28T12:54:37.838108Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#predykcja na jutro z predykcją na dzisiaj\n\nlayers = [X_train.shape[1], 512,512, 2]\nactivations = [\"sigmoid\",\"linear\",]\nmodel2 = WeatherPredictionNetwork(layers, activations, seed=42, binary_output=True)\n\ntrain_and_evaluate(model2, X_train2, y_train2, X_test2, y_test2, epochs=400, learning_rate=0.00001, rate = [],batch_size=256)",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T12:56:25.193931Z",
          "iopub.execute_input": "2025-01-28T12:56:25.194258Z",
          "iopub.status.idle": "2025-01-28T12:59:15.775572Z",
          "shell.execute_reply.started": "2025-01-28T12:56:25.194234Z",
          "shell.execute_reply": "2025-01-28T12:59:15.774699Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#predykcja na jutro z predykcją na dzisiaj\n\nlayers = [X_train.shape[1], 512,512, 2]\nactivations = [\"sigmoid\",\"linear\",]\nmodel2 = WeatherPredictionNetwork(layers, activations, seed=42, binary_output=True)\n\ntrain_and_evaluate(model2, X_train2, y_train2, X_test2, y_test2, epochs=1000, learning_rate=0.00001, rate = [500],batch_size=256)",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T13:00:58.424119Z",
          "iopub.execute_input": "2025-01-28T13:00:58.424474Z",
          "iopub.status.idle": "2025-01-28T13:08:04.585965Z",
          "shell.execute_reply.started": "2025-01-28T13:00:58.424447Z",
          "shell.execute_reply": "2025-01-28T13:08:04.585299Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# from itertools import product\n# import cupy as cp\n# from sklearn.metrics import roc_auc_score\n\n# def hyperparameter_optimization(train_and_evaluate, X_train, y_train, X_test, y_test, binary_output):\n#     # Zakres hiperparametrów do optymalizacji\n#     layer_options = [\n#         [X_train.shape[1] * X_train.shape[2], 256, 128, 2],\n#         [X_train.shape[1] * X_train.shape[2], 512, 512, 2],\n#         [X_train.shape[1] * X_train.shape[2], 1024, 512, 2]\n#     ]\n#     activation_options = [\n#         [\"sigmoid\", \"relu\"],\n#         [\"relu\", \"relu\"],\n#         [\"tanh\", \"relu\"]\n#     ]\n#     learning_rate_options = [0.005, 0.0001, 0.00001]\n#     batch_size_options = [64, 128, 256]\n#     epochs_options = [500, 1000]\n\n#     # Lista wszystkich kombinacji hiperparametrów\n#     search_space = list(product(layer_options, activation_options, learning_rate_options, batch_size_options, epochs_options))\n\n#     best_mae = float(\"inf\")\n#     best_auc = 0\n#     best_params = None\n\n#     # Iteracja po wszystkich kombinacjach hiperparametrów\n#     for layers, activations, learning_rate, batch_size, epochs in search_space:\n#         print(f\"Testing layers={layers}, activations={activations}, learning_rate={learning_rate}, batch_size={batch_size}, epochs={epochs}\")\n        \n#         # Tworzenie modelu\n#         model = WeatherPredictionNetwork(layers, activations, binary_output=binary_output, seed=42)\n\n#         # Trening i ewaluacja\n#         mae, auc = train_and_evaluate(\n#             model, X_train, y_train, X_test, y_test,\n#             epochs=epochs, learning_rate=learning_rate, rate=[500, 750, 900],\n#             batch_size=batch_size, binary_output=binary_output\n#         )\n\n#         # Aktualizacja najlepszych hiperparametrów\n#         if mae < best_mae or (mae == best_mae and auc > best_auc):\n#             best_mae = mae\n#             best_auc = auc\n#             best_params = (layers, activations, learning_rate, batch_size, epochs)\n\n#         print(f\"Finished testing with MAE: {mae}, AUC: {auc}\\n\")\n\n#     print(f\"Best params: layers={best_params[0]}, activations={best_params[1]}, learning_rate={best_params[2]}, batch_size={best_params[3]}, epochs={best_params[4]}\")\n#     print(f\"Best MAE: {best_mae}, Best AUC: {best_auc}\")\n\n#     return best_params, best_mae, best_auc\n",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T00:38:36.425730Z",
          "iopub.execute_input": "2025-01-28T00:38:36.426108Z",
          "iopub.status.idle": "2025-01-28T00:38:36.433496Z",
          "shell.execute_reply.started": "2025-01-28T00:38:36.426081Z",
          "shell.execute_reply": "2025-01-28T00:38:36.432736Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# best_params, best_mae, best_auc = hyperparameter_optimization(\n#     train_and_evaluate,\n#     X_train, y_train,\n#     X_test, y_test,\n#     binary_output=binary_output\n# )",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-26T12:02:52.341939Z",
          "iopub.execute_input": "2025-01-26T12:02:52.342164Z",
          "iopub.status.idle": "2025-01-26T12:02:52.359644Z",
          "shell.execute_reply.started": "2025-01-26T12:02:52.342146Z",
          "shell.execute_reply": "2025-01-26T12:02:52.358933Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# # %pip install scikit-optimize\n# from skopt import gp_minimize\n# from skopt.space import Integer, Real, Categorical\n# from skopt.utils import use_named_args\n# import numpy as np\n\n# def bayesian_optimization(train_and_evaluate, X_train, y_train, X_test, y_test, binary_output):\n#     # Przestrzeń wyszukiwania dla hiperparametrów\n#     space = [\n#         Categorical([\"config_0\", \"config_1\", \"config_2\", \"config_3\"], name=\"layers\"),\n#         Categorical([\"sigmoid_relu\", \"sigmoid_linear\"], name=\"activations\"),  # Zakodowane krotki jako stringi\n#         Real(1e-4, 1e-3, prior=\"log-uniform\", name=\"learning_rate\"),\n#         Categorical([64, 128, 256], name=\"batch_size\"),\n#         Categorical([50, 200, 500], name=\"epochs\")\n#     ]\n\n#     # Mapowanie z ciągów znaków na rzeczywiste konfiguracje warstw\n#     layers_mapping = {\n#         \"config_0\": (X_train.shape[1] * X_train.shape[2], 64, 32, 2),\n#         \"config_1\": (X_train.shape[1] * X_train.shape[2], 128, 64, 2),\n#         \"config_2\": (X_train.shape[1] * X_train.shape[2], 256, 128, 2),\n#         \"config_3\": (X_train.shape[1] * X_train.shape[2], 512, 256, 2)\n#     }\n\n#     # Mapowanie string `activations` na rzeczywistą krotkę\n#     activations_mapping = {\n#         \"sigmoid_relu\": (\"sigmoid\", \"relu\"),\n#         \"sigmoid_linear\": (\"sigmoid\", \"linear\")\n#     }\n\n#     @use_named_args(space)\n#     def objective(layers, activations, learning_rate, batch_size, epochs):\n#         # Mapowanie string `layers` na rzeczywistą konfigurację\n#         mapped_layers = layers_mapping[layers]\n#         # Mapowanie string `activations` na rzeczywistą krotkę\n#         mapped_activations = activations_mapping[activations]\n#         print(f\"Testing: layers={mapped_layers}, activations={mapped_activations}, learning_rate={learning_rate}, batch_size={batch_size}, epochs={epochs}\")\n\n#         # Tworzenie modelu\n#         model = WeatherPredictionNetwork(list(mapped_layers), list(mapped_activations), binary_output=binary_output, seed=42)\n\n#         # Trening i ewaluacja\n#         mae, auc = train_and_evaluate(\n#             model, X_train, y_train, X_test, y_test,\n#             epochs=epochs, learning_rate=learning_rate, rate=[500, 750, 900],\n#             batch_size=batch_size, binary_output=binary_output\n#         )\n\n#         print(f\"MAE: {mae}, AUC: {auc}\")\n\n#         # Zwróć MAE jako wartość skalarną\n#         return float(mae)\n\n#     # Optymalizacja przy użyciu Bayesian Optimization\n#     result = gp_minimize(\n#         func=objective,\n#         dimensions=space,\n#         n_calls=10,  # Liczba iteracji optymalizacji (tutaj 20)\n#         n_initial_points=2,\n#         random_state=30,\n#         verbose=True\n#     )\n\n#     # Najlepsze hiperparametry\n#     best_params = result.x\n#     print(f\"Best parameters: layers={best_params[0]}, activations={best_params[1]}, learning_rate={best_params[2]}, batch_size={best_params[3]}, epochs={best_params[4]}\")\n#     print(f\"Best MAE: {result.fun}\")\n\n#     return best_params, result.fun\n",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T00:42:39.045511Z",
          "iopub.execute_input": "2025-01-28T00:42:39.045932Z",
          "iopub.status.idle": "2025-01-28T00:42:39.451711Z",
          "shell.execute_reply.started": "2025-01-28T00:42:39.045899Z",
          "shell.execute_reply": "2025-01-28T00:42:39.450819Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# best_params, best_mae = bayesian_optimization(\n#     train_and_evaluate,\n#     X_train, y_train,\n#     X_test, y_test,\n#     binary_output=binary_output\n# )",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T00:42:45.254643Z",
          "iopub.execute_input": "2025-01-28T00:42:45.254961Z",
          "iopub.status.idle": "2025-01-28T00:44:23.898591Z",
          "shell.execute_reply.started": "2025-01-28T00:42:45.254936Z",
          "shell.execute_reply": "2025-01-28T00:44:23.897003Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# def compute_majority_class_percentage(y, binary_output=True):\n#     if binary_output:\n#         binary_labels = y[:, 1]\n#     else:\n#         binary_labels = y[:, 1] >=6\n    \n#     unique, counts = np.unique(binary_labels, return_counts=True)\n#     class_counts = dict(zip(unique, counts))\n    \n#     majority_class_count = max(class_counts.values())\n#     total_samples = len(binary_labels)\n#     majority_class_percentage = (majority_class_count / total_samples) * 100\n    \n#     return majority_class_percentage, class_counts\n\n# majority_percentage_train, train_class_counts = compute_majority_class_percentage(y_train, binary_output=binary_output)\n# majority_percentage_test, test_class_counts = compute_majority_class_percentage(y_test, binary_output=binary_output)\n\n# print(f\"Majority class percentage in training data: {majority_percentage_train:.2f}%\")\n# print(f\"Class distribution in training data: {train_class_counts}\")\n# print(f\"Majority class percentage in test data: {majority_percentage_test:.2f}%\")\n# print(f\"Class distribution in test data: {test_class_counts}\")\n",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-26T14:29:48.046000Z",
          "iopub.status.idle": "2025-01-26T14:29:48.046262Z",
          "shell.execute_reply": "2025-01-26T14:29:48.046156Z"
        }
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}