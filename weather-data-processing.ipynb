{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install numpy\n",
    "# %pip install scikit-learn\n",
    "# %pip install kagglehub\n",
    "# %pip install cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:24.574362Z",
     "iopub.status.busy": "2024-12-29T16:54:24.573960Z",
     "iopub.status.idle": "2024-12-29T16:54:25.867960Z",
     "shell.execute_reply": "2024-12-29T16:54:25.866721Z",
     "shell.execute_reply.started": "2024-12-29T16:54:24.574325Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\patry\\anaconda3\\envs\\weather\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import kagglehub\n",
    "import cupy as cp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:25.869990Z",
     "iopub.status.busy": "2024-12-29T16:54:25.869419Z",
     "iopub.status.idle": "2024-12-29T16:54:29.158623Z",
     "shell.execute_reply": "2024-12-29T16:54:29.157504Z",
     "shell.execute_reply.started": "2024-12-29T16:54:25.869950Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "historical_hourly_weather_data_path = kagglehub.dataset_download('selfishgene/historical-hourly-weather-data')\n",
    "\n",
    "city = \"Portland\"\n",
    "\n",
    "city_attributes = pd.read_csv(f\"{historical_hourly_weather_data_path}/city_attributes.csv\")\n",
    "humidity = pd.read_csv(f\"{historical_hourly_weather_data_path}/humidity.csv\")\n",
    "pressure = pd.read_csv(f\"{historical_hourly_weather_data_path}/pressure.csv\")\n",
    "temperature = pd.read_csv(f\"{historical_hourly_weather_data_path}/temperature.csv\")\n",
    "weather_description = pd.read_csv(f\"{historical_hourly_weather_data_path}/weather_description.csv\")\n",
    "wind_speed = pd.read_csv(f\"{historical_hourly_weather_data_path}/wind_speed.csv\")\n",
    "wind_direction = pd.read_csv(f\"{historical_hourly_weather_data_path}/wind_direction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:29.161394Z",
     "iopub.status.busy": "2024-12-29T16:54:29.160941Z",
     "iopub.status.idle": "2024-12-29T16:54:29.178700Z",
     "shell.execute_reply": "2024-12-29T16:54:29.177526Z",
     "shell.execute_reply.started": "2024-12-29T16:54:29.161353Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if city not in city_attributes['City'].values:\n",
    "    raise ValueError(f\"City '{city}' does not exist in the data. Available cities are: {city_attributes['City'].unique()}\")\n",
    "\n",
    "selected_city = city_attributes[city_attributes['City'] == city].index[0]\n",
    "data_frames = [humidity, pressure, temperature, weather_description, wind_speed, wind_direction]\n",
    "\n",
    "for i, df in enumerate(data_frames):\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    data_frames[i] = df.iloc[:, selected_city]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:29.180658Z",
     "iopub.status.busy": "2024-12-29T16:54:29.180324Z",
     "iopub.status.idle": "2024-12-29T16:54:29.674041Z",
     "shell.execute_reply": "2024-12-29T16:54:29.673014Z",
     "shell.execute_reply.started": "2024-12-29T16:54:29.180632Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "combined_data = pd.concat(data_frames, axis=1)\n",
    "combined_data.columns = [\n",
    "    'humidity', 'pressure', 'temperature', 'weather_description', \n",
    "    'wind_speed', 'wind_direction'\n",
    "]\n",
    "combined_data.index = pd.to_datetime(combined_data.index)\n",
    "# aggregate daily\n",
    "aggregated_data = combined_data.resample('D').agg({\n",
    "    'temperature': 'mean',\n",
    "    'humidity': 'mean',\n",
    "    'wind_speed': ['max', 'mean'],\n",
    "    'pressure': 'mean',\n",
    "    'weather_description': lambda x: x.mode()[0] if not x.mode().empty else np.nan,\n",
    "    'wind_direction': 'mean'\n",
    "})\n",
    "aggregated_data.columns = ['_'.join(col).strip('_') for col in aggregated_data.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:30.119864Z",
     "iopub.status.busy": "2024-12-29T16:54:30.119559Z",
     "iopub.status.idle": "2024-12-29T16:54:30.142206Z",
     "shell.execute_reply": "2024-12-29T16:54:30.141099Z",
     "shell.execute_reply.started": "2024-12-29T16:54:30.119840Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['temperature_mean', 'humidity_mean', 'wind_speed_max',\n",
      "       'wind_speed_mean', 'pressure_mean', 'wind_direction_mean',\n",
      "       'weather_desc_broken clouds', 'weather_desc_dust',\n",
      "       'weather_desc_few clouds', 'weather_desc_fog',\n",
      "       'weather_desc_freezing rain', 'weather_desc_haze',\n",
      "       'weather_desc_heavy intensity rain', 'weather_desc_light rain',\n",
      "       'weather_desc_mist', 'weather_desc_moderate rain',\n",
      "       'weather_desc_overcast clouds', 'weather_desc_scattered clouds',\n",
      "       'weather_desc_sky is clear', 'weather_desc_smoke', 'weather_desc_snow'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "one_hot_weather_description = pd.get_dummies(\n",
    "    aggregated_data['weather_description_<lambda>'],\n",
    "    prefix='weather_desc',\n",
    "    drop_first=False\n",
    ")\n",
    "\n",
    "aggregated_data = pd.concat([aggregated_data, one_hot_weather_description], axis=1)\n",
    "aggregated_data.drop(columns=['weather_description_<lambda>'], inplace=True)\n",
    "print(aggregated_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_output = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_weather_data(data, binary_output=True, window_size=3):\n",
    "    X, y = [], []\n",
    "    for i in range(window_size, len(data) - 1):\n",
    "        X_window = data.iloc[i-window_size:i][[\n",
    "            'temperature_mean', 'humidity_mean', 'wind_speed_max',\n",
    "            'wind_speed_mean', 'pressure_mean', 'wind_direction_mean',\n",
    "            'weather_desc_broken clouds', 'weather_desc_dust',\n",
    "            'weather_desc_few clouds', 'weather_desc_fog',\n",
    "            'weather_desc_freezing rain', 'weather_desc_haze',\n",
    "            'weather_desc_heavy intensity rain', 'weather_desc_light rain',\n",
    "            'weather_desc_mist', 'weather_desc_moderate rain',\n",
    "            'weather_desc_overcast clouds', 'weather_desc_scattered clouds',\n",
    "            'weather_desc_sky is clear', 'weather_desc_smoke', 'weather_desc_snow'\n",
    "        ]].values\n",
    "        y_target = data.iloc[i + 1][['temperature_mean', 'wind_speed_max']].values\n",
    "        # y_target = data.iloc[i][['temperature_mean', 'wind_speed_max']].values\n",
    "\n",
    "        if binary_output:\n",
    "            # encode wind speed > 6 as binary\n",
    "            y_target[1] = 1 if y_target[1] >= 6 else 0\n",
    "        X.append(X_window)\n",
    "        y.append(y_target)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    train_size = int(0.7 * len(X)) # 0.7 or 0.8\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    continuous_indices = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "    X_train_continuous = X_train[:, :, continuous_indices]\n",
    "    X_test_continuous = X_test[:, :, continuous_indices]\n",
    "    X_train_continuous = X_train[:, :, continuous_indices].astype(float) \n",
    "    X_test_continuous = X_test[:, :, continuous_indices].astype(float)\n",
    "\n",
    "    X_mean = X_train_continuous.mean(axis=(0, 1), keepdims=True)\n",
    "    X_std = X_train_continuous.std(axis=(0, 1), keepdims=True)\n",
    "\n",
    "    X_train[:, :, continuous_indices] = (X_train_continuous - X_mean) / (X_std + 1e-9)\n",
    "    X_test[:, :, continuous_indices] = (X_test_continuous - X_mean) / (X_std + 1e-9)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_weather_data(aggregated_data, binary_output=binary_output, window_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1318, 3, 21)\n",
      "(1318, 2)\n",
      "(565, 3, 21)\n",
      "(565, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=1000, learning_rate=0.001, rate = [500], binary_output=True):\n",
    "    X_train_cp = cp.array(X_train.reshape(X_train.shape[0], -1), dtype=cp.float32)\n",
    "    y_train_cp = cp.array(y_train, dtype=cp.float32)\n",
    "    X_test_cp = cp.array(X_test.reshape(X_test.shape[0], -1), dtype=cp.float32)\n",
    "    y_test_cp = cp.array(y_test, dtype=cp.float32)\n",
    "\n",
    "    model.train(X_train_cp, y_train_cp, epochs, learning_rate, rate)\n",
    "\n",
    "    predictions = model.predict(X_test_cp)\n",
    "    \n",
    "    mae = cp.mean(cp.abs(predictions[:, 0] - y_test_cp[:, 0]))\n",
    "\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    if binary_output:\n",
    "        auc = roc_auc_score(cp.asnumpy(y_test_cp[:, 1]), cp.asnumpy(predictions[:, 1]))\n",
    "    else:\n",
    "        auc = roc_auc_score((cp.asnumpy(y_test_cp[:, 1]) >= 6), cp.asnumpy(predictions[:, 1]))\n",
    "\n",
    "    print(f\"Test Regression MAE: {mae}\")\n",
    "    print(f\"Test Classification AUC: {auc}\")\n",
    "\n",
    "    return mae, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Regression Loss: 284.46051025390625, Classification AUC: 0.27883763553608826, Learning Rate: 0.0001\n",
      "Epoch 100, Regression Loss: 250.3007354736328, Classification AUC: 0.6834272004554419, Learning Rate: 0.0001\n",
      "Epoch 200, Regression Loss: 207.3856201171875, Classification AUC: 0.7015461207560103, Learning Rate: 0.0001\n",
      "Epoch 300, Regression Loss: 152.6181182861328, Classification AUC: 0.7058507440143371, Learning Rate: 0.0001\n",
      "Epoch 400, Regression Loss: 85.43536376953125, Classification AUC: 0.7156116660518588, Learning Rate: 0.0001\n",
      "Epoch 500, Regression Loss: 5.396749019622803, Classification AUC: 0.7102047455421588, Learning Rate: 0.0001\n",
      "Epoch 600, Regression Loss: 2.341679334640503, Classification AUC: 0.727228630101574, Learning Rate: 0.0001\n",
      "Epoch 700, Regression Loss: 2.3343052864074707, Classification AUC: 0.732965223175255, Learning Rate: 0.0001\n",
      "Epoch 800, Regression Loss: 2.3307628631591797, Classification AUC: 0.7361457646515492, Learning Rate: 0.0001\n",
      "Epoch 900, Regression Loss: 2.3283944129943848, Classification AUC: 0.7381906059294588, Learning Rate: 0.0001\n",
      "Epoch 1000, Regression Loss: 2.3267455101013184, Classification AUC: 0.7395354376221749, Learning Rate: 0.0001\n",
      "Epoch 1100, Regression Loss: 2.3254241943359375, Classification AUC: 0.7409122199598583, Learning Rate: 0.0001\n",
      "Epoch 1200, Regression Loss: 2.324315309524536, Classification AUC: 0.7420072557010117, Learning Rate: 0.0001\n",
      "Epoch 1300, Regression Loss: 2.3233516216278076, Classification AUC: 0.7428524954906022, Learning Rate: 0.0001\n",
      "Epoch 1400, Regression Loss: 2.322488307952881, Classification AUC: 0.7437848734028308, Learning Rate: 0.0001\n",
      "Epoch 1500, Regression Loss: 2.3216917514801025, Classification AUC: 0.7449045982787317, Learning Rate: 0.0001\n",
      "Epoch 1600, Regression Loss: 2.32094144821167, Classification AUC: 0.7463583526247455, Learning Rate: 0.0001\n",
      "Epoch 1700, Regression Loss: 2.3202240467071533, Classification AUC: 0.7477191596399453, Learning Rate: 0.0001\n",
      "Epoch 1800, Regression Loss: 2.3195528984069824, Classification AUC: 0.7489144042221325, Learning Rate: 0.0001\n",
      "Epoch 1900, Regression Loss: 2.3188846111297607, Classification AUC: 0.7501154580124957, Learning Rate: 0.0001\n",
      "Epoch 2000, Regression Loss: 2.31825590133667, Classification AUC: 0.750959245500042, Learning Rate: 0.0001\n",
      "Epoch 2100, Regression Loss: 2.31764817237854, Classification AUC: 0.7516737781056753, Learning Rate: 0.0001\n",
      "Epoch 2200, Regression Loss: 2.317054271697998, Classification AUC: 0.7521806315190208, Learning Rate: 0.0001\n",
      "Epoch 2300, Regression Loss: 2.3164491653442383, Classification AUC: 0.7523316709315937, Learning Rate: 0.0001\n",
      "Epoch 2400, Regression Loss: 2.315850257873535, Classification AUC: 0.7529082348430497, Learning Rate: 0.0001\n",
      "Epoch 2500, Regression Loss: 2.315243721008301, Classification AUC: 0.7534847987545058, Learning Rate: 1e-05\n",
      "Epoch 2600, Regression Loss: 2.2701377868652344, Classification AUC: 0.7554352403995573, Learning Rate: 1e-05\n",
      "Epoch 2700, Regression Loss: 2.256626605987549, Classification AUC: 0.7590049988236353, Learning Rate: 1e-05\n",
      "Epoch 2800, Regression Loss: 2.249542713165283, Classification AUC: 0.7619735042015098, Learning Rate: 1e-05\n",
      "Epoch 2900, Regression Loss: 2.245185136795044, Classification AUC: 0.7645760294643038, Learning Rate: 1e-05\n",
      "Epoch 3000, Regression Loss: 2.2421653270721436, Classification AUC: 0.766591824701334, Learning Rate: 1e-05\n",
      "Epoch 3100, Regression Loss: 2.239701986312866, Classification AUC: 0.7682213075946682, Learning Rate: 1e-05\n",
      "Epoch 3200, Regression Loss: 2.2376222610473633, Classification AUC: 0.7692553466499747, Learning Rate: 1e-05\n",
      "Epoch 3300, Regression Loss: 2.235931634902954, Classification AUC: 0.7702283890194347, Learning Rate: 1e-05\n",
      "Epoch 3400, Regression Loss: 2.234544277191162, Classification AUC: 0.770652461216274, Learning Rate: 1e-05\n",
      "Epoch 3500, Regression Loss: 2.233168840408325, Classification AUC: 0.7711026748499046, Learning Rate: 1e-05\n",
      "Epoch 3600, Regression Loss: 2.231900453567505, Classification AUC: 0.7716865002715806, Learning Rate: 1e-05\n",
      "Epoch 3700, Regression Loss: 2.2306463718414307, Classification AUC: 0.7718113982473619, Learning Rate: 1e-05\n",
      "Epoch 3800, Regression Loss: 2.2293922901153564, Classification AUC: 0.7722035197992337, Learning Rate: 1e-05\n",
      "Epoch 3900, Regression Loss: 2.228191375732422, Classification AUC: 0.7725985459551936, Learning Rate: 1e-05\n",
      "Epoch 4000, Regression Loss: 2.226957321166992, Classification AUC: 0.772720539326887, Learning Rate: 1e-05\n",
      "Epoch 4100, Regression Loss: 2.2257237434387207, Classification AUC: 0.7727960590331734, Learning Rate: 1e-05\n",
      "Epoch 4200, Regression Loss: 2.2244694232940674, Classification AUC: 0.7728512465108442, Learning Rate: 1e-05\n",
      "Epoch 4300, Regression Loss: 2.223193883895874, Classification AUC: 0.7728338188863166, Learning Rate: 1e-05\n",
      "Epoch 4400, Regression Loss: 2.2218711376190186, Classification AUC: 0.7728977201762514, Learning Rate: 1e-05\n",
      "Epoch 4500, Regression Loss: 2.22055721282959, Classification AUC: 0.773010999735681, Learning Rate: 1e-05\n",
      "Epoch 4600, Regression Loss: 2.219241142272949, Classification AUC: 0.77287157873946, Learning Rate: 1e-05\n",
      "Epoch 4700, Regression Loss: 2.2178993225097656, Classification AUC: 0.77277282220047, Learning Rate: 1e-05\n",
      "Epoch 4800, Regression Loss: 2.2165145874023438, Classification AUC: 0.7727060163064473, Learning Rate: 1e-05\n",
      "Epoch 4900, Regression Loss: 2.2151315212249756, Classification AUC: 0.7726624472451282, Learning Rate: 1e-05\n",
      "Epoch 5000, Regression Loss: 2.2137205600738525, Classification AUC: 0.7725520722897865, Learning Rate: 1e-05\n",
      "Epoch 5100, Regression Loss: 2.2122411727905273, Classification AUC: 0.7723109901504874, Learning Rate: 1e-05\n",
      "Epoch 5200, Regression Loss: 2.2107653617858887, Classification AUC: 0.7722238520278493, Learning Rate: 1e-05\n",
      "Epoch 5300, Regression Loss: 2.2092337608337402, Classification AUC: 0.7721686645501785, Learning Rate: 1e-05\n",
      "Epoch 5400, Regression Loss: 2.2076940536499023, Classification AUC: 0.7720728126152766, Learning Rate: 1e-05\n",
      "Epoch 5500, Regression Loss: 2.2061398029327393, Classification AUC: 0.7720437665743971, Learning Rate: 1e-05\n",
      "Epoch 5600, Regression Loss: 2.2045559883117676, Classification AUC: 0.7719450100354072, Learning Rate: 1e-05\n",
      "Epoch 5700, Regression Loss: 2.2029383182525635, Classification AUC: 0.7719914837008142, Learning Rate: 1e-05\n",
      "Epoch 5800, Regression Loss: 2.20131778717041, Classification AUC: 0.7720147205335176, Learning Rate: 1e-05\n",
      "Epoch 5900, Regression Loss: 2.1996443271636963, Classification AUC: 0.7720582895948368, Learning Rate: 1e-05\n",
      "Epoch 6000, Regression Loss: 2.1979522705078125, Classification AUC: 0.7719682468681106, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 6100, Regression Loss: 2.195760488510132, Classification AUC: 0.7719188685986157, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 6200, Regression Loss: 2.1954073905944824, Classification AUC: 0.7719798652844625, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 6300, Regression Loss: 2.195113182067871, Classification AUC: 0.7719885790967262, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 6400, Regression Loss: 2.1948442459106445, Classification AUC: 0.7719769606803745, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 6500, Regression Loss: 2.194587230682373, Classification AUC: 0.7719421054313191, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 6600, Regression Loss: 2.1943411827087402, Classification AUC: 0.7718665857250327, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 6700, Regression Loss: 2.1941001415252686, Classification AUC: 0.7718607765168569, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 6800, Regression Loss: 2.1938636302948, Classification AUC: 0.771901440974088, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 6900, Regression Loss: 2.193626642227173, Classification AUC: 0.7718985363699999, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 7000, Regression Loss: 2.193391799926758, Classification AUC: 0.7718607765168568, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 7100, Regression Loss: 2.193157196044922, Classification AUC: 0.7718578719127689, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 7200, Regression Loss: 2.1929197311401367, Classification AUC: 0.7718723949332086, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 7300, Regression Loss: 2.192680597305298, Classification AUC: 0.7718491581005051, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 7400, Regression Loss: 2.1924426555633545, Classification AUC: 0.7717968752269221, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 7500, Regression Loss: 2.1922104358673096, Classification AUC: 0.7717736383942188, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 7600, Regression Loss: 2.1922361850738525, Classification AUC: 0.7717765429983067, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 7700, Regression Loss: 2.1922130584716797, Classification AUC: 0.771759115373779, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 7800, Regression Loss: 2.1921885013580322, Classification AUC: 0.7717591153737791, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 7900, Regression Loss: 2.1921639442443848, Classification AUC: 0.771753306165603, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 8000, Regression Loss: 2.192141532897949, Classification AUC: 0.7717460446553832, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 8100, Regression Loss: 2.1921164989471436, Classification AUC: 0.7717344262390315, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 8200, Regression Loss: 2.192094326019287, Classification AUC: 0.7717300693328996, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 8300, Regression Loss: 2.192070960998535, Classification AUC: 0.7717358785410754, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 8400, Regression Loss: 2.192046880722046, Classification AUC: 0.7717329739369875, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 8500, Regression Loss: 2.1920204162597656, Classification AUC: 0.7717184509165478, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 8600, Regression Loss: 2.1919946670532227, Classification AUC: 0.7717242601247237, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 8700, Regression Loss: 2.191969156265259, Classification AUC: 0.7717271647288115, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 8800, Regression Loss: 2.1919448375701904, Classification AUC: 0.771712641708372, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 8900, Regression Loss: 2.191922187805176, Classification AUC: 0.7717097371042839, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 9000, Regression Loss: 2.1918983459472656, Classification AUC: 0.771706832500196, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 9100, Regression Loss: 2.191873550415039, Classification AUC: 0.771706832500196, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 9200, Regression Loss: 2.191850423812866, Classification AUC: 0.7717126417083718, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 9300, Regression Loss: 2.1918258666992188, Classification AUC: 0.7717155463124599, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 9400, Regression Loss: 2.191802740097046, Classification AUC: 0.771709737104284, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 9500, Regression Loss: 2.1917800903320312, Classification AUC: 0.771703927896108, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 9600, Regression Loss: 2.1917552947998047, Classification AUC: 0.771709737104284, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 9700, Regression Loss: 2.1917319297790527, Classification AUC: 0.7717213555206356, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 9800, Regression Loss: 2.191709041595459, Classification AUC: 0.7717126417083718, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 9900, Regression Loss: 2.1916849613189697, Classification AUC: 0.7717068325001961, Learning Rate: 1.0000000000000002e-07\n",
      "Test Regression MAE: 2.391089916229248\n",
      "Test Classification AUC: 0.6226301369863014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(2.39109, dtype=float32), np.float64(0.6226301369863014))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from weather_prediction import WeatherPredictionNetwork\n",
    "\n",
    "layers = [X_train.shape[1] * X_train.shape[2],512, 512, 2]\n",
    "activations = [\"sigmoid\", \"relu\"]\n",
    "model = WeatherPredictionNetwork(layers, activations, binary_output=binary_output, seed=42)\n",
    "\n",
    "train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=10000, learning_rate=0.0001, rate = [2500, 6000, 7500], binary_output=binary_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority class percentage in training data: 72.76%\n",
      "Class distribution in training data: {0: np.int64(959), 1: np.int64(359)}\n",
      "Majority class percentage in test data: 64.60%\n",
      "Class distribution in test data: {0: np.int64(365), 1: np.int64(200)}\n"
     ]
    }
   ],
   "source": [
    "def compute_majority_class_percentage(y, binary_output=True):\n",
    "    if binary_output:\n",
    "        binary_labels = y[:, 1]\n",
    "    else:\n",
    "        binary_labels = y[:, 1] >=6\n",
    "    \n",
    "    unique, counts = np.unique(binary_labels, return_counts=True)\n",
    "    class_counts = dict(zip(unique, counts))\n",
    "    \n",
    "    majority_class_count = max(class_counts.values())\n",
    "    total_samples = len(binary_labels)\n",
    "    majority_class_percentage = (majority_class_count / total_samples) * 100\n",
    "    \n",
    "    return majority_class_percentage, class_counts\n",
    "\n",
    "majority_percentage_train, train_class_counts = compute_majority_class_percentage(y_train, binary_output=binary_output)\n",
    "majority_percentage_test, test_class_counts = compute_majority_class_percentage(y_test, binary_output=binary_output)\n",
    "\n",
    "print(f\"Majority class percentage in training data: {majority_percentage_train:.2f}%\")\n",
    "print(f\"Class distribution in training data: {train_class_counts}\")\n",
    "print(f\"Majority class percentage in test data: {majority_percentage_test:.2f}%\")\n",
    "print(f\"Class distribution in test data: {test_class_counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7752,
     "sourceId": 11375,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "weather",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
