{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install numpy\n",
    "# %pip install scikit-learn\n",
    "# %pip install kagglehub\n",
    "# %pip install cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:24.574362Z",
     "iopub.status.busy": "2024-12-29T16:54:24.573960Z",
     "iopub.status.idle": "2024-12-29T16:54:25.867960Z",
     "shell.execute_reply": "2024-12-29T16:54:25.866721Z",
     "shell.execute_reply.started": "2024-12-29T16:54:24.574325Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\patry\\anaconda3\\envs\\weather\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "import cupy as cp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patry\\AppData\\Local\\Temp\\ipykernel_25316\\263827200.py:30: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  combined_data = combined_data.ffill().bfill().interpolate()\n"
     ]
    }
   ],
   "source": [
    "historical_hourly_weather_data_path = kagglehub.dataset_download('selfishgene/historical-hourly-weather-data')\n",
    "\n",
    "city_attributes = pd.read_csv(f\"{historical_hourly_weather_data_path}/city_attributes.csv\")\n",
    "humidity = pd.read_csv(f\"{historical_hourly_weather_data_path}/humidity.csv\")\n",
    "pressure = pd.read_csv(f\"{historical_hourly_weather_data_path}/pressure.csv\")\n",
    "temperature = pd.read_csv(f\"{historical_hourly_weather_data_path}/temperature.csv\")\n",
    "weather_description = pd.read_csv(f\"{historical_hourly_weather_data_path}/weather_description.csv\")\n",
    "wind_speed = pd.read_csv(f\"{historical_hourly_weather_data_path}/wind_speed.csv\")\n",
    "wind_direction = pd.read_csv(f\"{historical_hourly_weather_data_path}/wind_direction.csv\")\n",
    "\n",
    "data_frames = []\n",
    "for city in city_attributes['City']:\n",
    "    city_data = pd.DataFrame({\n",
    "        'datetime': pd.to_datetime(humidity['datetime']),\n",
    "        'humidity': humidity[city],\n",
    "        'pressure': pressure[city],\n",
    "        'temperature': temperature[city],\n",
    "        'weather_description': weather_description[city],\n",
    "        'wind_speed': wind_speed[city],\n",
    "        'wind_direction': wind_direction[city],\n",
    "        'latitude': city_attributes.loc[city_attributes['City'] == city, 'Latitude'].values[0],\n",
    "        'longitude': city_attributes.loc[city_attributes['City'] == city, 'Longitude'].values[0],\n",
    "        'city': city\n",
    "    })\n",
    "    city_data.set_index('datetime', inplace=True)\n",
    "    data_frames.append(city_data)\n",
    "\n",
    "combined_data = pd.concat(data_frames)\n",
    "\n",
    "combined_data = combined_data.ffill().bfill().interpolate()\n",
    "\n",
    "aggregated_data = combined_data.groupby(['city']).resample('D').agg({\n",
    "    'temperature': 'mean',\n",
    "    'humidity': 'mean',\n",
    "    'wind_speed': ['max', 'mean'],\n",
    "    'pressure': 'mean',\n",
    "    'weather_description': lambda x: x.mode()[0] if not x.mode().empty else np.nan,\n",
    "    'wind_direction': 'mean',\n",
    "    'latitude': 'mean',\n",
    "    'longitude': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "aggregated_data.columns = [\n",
    "    '_'.join(col).strip('_') if isinstance(col, tuple) else col for col in aggregated_data.columns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_weather_description = pd.get_dummies(\n",
    "    aggregated_data['weather_description_<lambda>'],\n",
    "    prefix='weather_desc',\n",
    "    drop_first=False\n",
    ")\n",
    "\n",
    "aggregated_data = pd.concat([aggregated_data, one_hot_weather_description], axis=1)\n",
    "aggregated_data.drop(columns=['weather_description_<lambda>'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_output = True # True better\n",
    "two_models = False # True = 2.165, Flase = 2.151\n",
    "standarize_2nd_data = False # False better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_weather_data(data, binary_output=True, window_size=3, two_models=False, standarize_2nd_data=False):\n",
    "    X, y, X_2, y_2 = [], [], [], []\n",
    "    for i in range(window_size, len(data) - 1):\n",
    "        X_window = data.iloc[i-window_size:i][[\n",
    "            'temperature_mean', 'humidity_mean', 'wind_speed_max',\n",
    "            'wind_speed_mean', 'pressure_mean', 'wind_direction_mean',\n",
    "            'weather_desc_broken clouds', 'weather_desc_dust',\n",
    "            'weather_desc_few clouds', 'weather_desc_fog',\n",
    "            'weather_desc_freezing rain', 'weather_desc_haze',\n",
    "            'weather_desc_heavy intensity rain', 'weather_desc_light rain',\n",
    "            'weather_desc_mist', 'weather_desc_moderate rain',\n",
    "            'weather_desc_overcast clouds', 'weather_desc_scattered clouds',\n",
    "            'weather_desc_sky is clear', 'weather_desc_smoke', 'weather_desc_snow'\n",
    "        ]].values\n",
    "        if two_models is False:\n",
    "            y_target = data.iloc[i + 1][['temperature_mean', 'wind_speed_max']].values\n",
    "        else:\n",
    "            y_target = data.iloc[i][['temperature_mean', 'wind_speed_max']].values\n",
    "            y_target2 = data.iloc[i + 1][['temperature_mean', 'wind_speed_max']].values\n",
    "\n",
    "        if binary_output:\n",
    "            # encode wind speed > 6 as binary\n",
    "            y_target[1] = 1 if y_target[1] >= 6 else 0\n",
    "            if two_models:\n",
    "                y_target2[1] = 1 if y_target2[1] >= 6 else 0\n",
    "        \n",
    "        X.append(X_window)\n",
    "        y.append(y_target)\n",
    "        if two_models:\n",
    "            X_2.append(y_target)\n",
    "            y_2.append(y_target2)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X_2 = np.array(X_2)\n",
    "    y_2 = np.array(y_2)\n",
    "\n",
    "    # split into train/test (0.7 or 0.8)\n",
    "    train_size = int(0.8 * len(X))\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    continuous_indices = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "    X_train_continuous = X_train[:, :, continuous_indices].astype(float) \n",
    "    X_test_continuous = X_test[:, :, continuous_indices].astype(float)\n",
    "\n",
    "    X_mean = X_train_continuous.mean(axis=(0, 1), keepdims=True)\n",
    "    X_std = X_train_continuous.std(axis=(0, 1), keepdims=True)\n",
    "\n",
    "    X_train[:, :, continuous_indices] = (X_train_continuous - X_mean) / (X_std + 1e-9)\n",
    "    X_test[:, :, continuous_indices] = (X_test_continuous - X_mean) / (X_std + 1e-9)\n",
    "\n",
    "    if two_models:\n",
    "        X_train_2, X_test_2 = X_2[:train_size], X_2[train_size:]\n",
    "        y_train_2, y_test_2 = y_2[:train_size], y_2[train_size:]\n",
    "\n",
    "        X_train_2_continuous = X_train_2[:, 0].astype(float) \n",
    "        X_test_2_continuous = X_test_2[:, 0].astype(float)\n",
    "\n",
    "        if standarize_2nd_data:\n",
    "            X_2_mean = X_train_2_continuous.mean(keepdims=True)\n",
    "            X_2_std = X_train_2_continuous.std(keepdims=True)\n",
    "\n",
    "            X_train_2[:, 0] = (X_train_2_continuous - X_2_mean) / (X_2_std + 1e-9)\n",
    "            X_test_2[:, 0] = (X_test_2_continuous - X_2_mean) / (X_2_std + 1e-9)\n",
    "        else:\n",
    "            X_2_mean = None\n",
    "            X_2_std = None\n",
    "    else:\n",
    "        X_train_2 = None\n",
    "        X_test_2 = None\n",
    "        y_train_2 = None\n",
    "        y_test_2 = None\n",
    "        X_2_mean = None\n",
    "        X_2_std = None\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, X_train_2, X_test_2, y_train_2, y_test_2, X_2_mean, X_2_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, X_train_2, X_test_2, y_train_2, y_test_2, X_2_mean, X_2_std = preprocess_weather_data(\n",
    "    aggregated_data, binary_output=binary_output, window_size=3, two_models=two_models, standarize_2nd_data=standarize_2nd_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54342, 3, 21)\n",
      "(54342, 2)\n",
      "(13586, 3, 21)\n",
      "(13586, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "if two_models:\n",
    "    print(X_train_2.shape)\n",
    "    print(y_train_2.shape)\n",
    "    \n",
    "    print(X_test_2.shape)\n",
    "    print(y_test_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=1000, learning_rate=0.001, rate = [500], \n",
    "                       batch_size=256, binary_output=True, model2=None, X_train2=None, X_test2=None, y_train2=None, y_test2=None,\n",
    "                       X_2_std=None, X_2_mean=None, standarize_2nd_data=False, add_noise=False):\n",
    "    X_train_cp = cp.array(X_train.reshape(X_train.shape[0], -1), dtype=cp.float32)\n",
    "    y_train_cp = cp.array(y_train, dtype=cp.float32)\n",
    "    X_test_cp = cp.array(X_test.reshape(X_test.shape[0], -1), dtype=cp.float32)\n",
    "    y_test_cp = cp.array(y_test, dtype=cp.float32)\n",
    "\n",
    "    model.train(X_train_cp, y_train_cp, X_test_cp, y_test_cp, epochs, learning_rate, rate, batch_size=batch_size)\n",
    "\n",
    "    predictions = model.predict(X_test_cp)\n",
    "    \n",
    "    mae = cp.mean(cp.abs(predictions[:, 0] - y_test_cp[:, 0]))\n",
    "\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    if binary_output:\n",
    "        auc = roc_auc_score(cp.asnumpy(y_test_cp[:, 1]), cp.asnumpy(predictions[:, 1]))\n",
    "    else:\n",
    "        auc = roc_auc_score((cp.asnumpy(y_test_cp[:, 1]) >= 6), cp.asnumpy(predictions[:, 1]))\n",
    "\n",
    "    print(f\"Test Regression MAE: {mae}\")\n",
    "    print(f\"Test Classification AUC: {auc}\")\n",
    "\n",
    "    # print(\"Predictions:\" , predictions[:5, :])\n",
    "    # print(\"True values:\", y_test_cp[:5, :])\n",
    "\n",
    "    if model2 is not None:\n",
    "        if add_noise:\n",
    "            predictions_train = model.predict(X_train_cp)\n",
    "            error_mean = cp.mean(cp.abs(predictions_train - y_train_cp), axis=0)\n",
    "            error_std = cp.std(cp.abs(predictions_train - y_train_cp), axis=0)\n",
    "            noise = cp.random.normal(loc=error_mean, scale=error_std, size=X_train_2.shape)\n",
    "\n",
    "        X_train_cp = cp.array(X_train2, dtype=cp.float32)\n",
    "        if add_noise:\n",
    "            X_train_cp = X_train_cp + noise\n",
    "        X_test_cp = cp.array(X_test2, dtype=cp.float32)\n",
    "        y_train_cp = cp.array(y_train2, dtype=cp.float32)\n",
    "        y_test_cp = cp.array(y_test2, dtype=cp.float32)\n",
    "        \n",
    "        model2.train(X_train_cp, y_train_cp, X_test_cp, y_test_cp, epochs, learning_rate * 10, rate, batch_size=batch_size)\n",
    "\n",
    "        X_2_mean_cp = cp.array(X_2_mean, dtype=cp.float32)\n",
    "        X_2_std_cp = cp.array(X_2_std, dtype=cp.float32)\n",
    "        if standarize_2nd_data:\n",
    "            predictions = (predictions - X_2_mean_cp) / (X_2_std_cp + 1e-9)\n",
    "        predictions = model2.predict(cp.array(predictions, dtype=cp.float32))\n",
    "\n",
    "        mae = cp.mean(cp.abs(predictions[:, 0] - y_test_cp[:, 0]))\n",
    "\n",
    "        if binary_output:\n",
    "            auc = roc_auc_score(cp.asnumpy(y_test_cp[:, 1]), cp.asnumpy(predictions[:, 1]))\n",
    "        else:\n",
    "            auc = roc_auc_score((cp.asnumpy(y_test_cp[:, 1]) >= 6), cp.asnumpy(predictions[:, 1]))\n",
    "    \n",
    "        print(f\"Test Regression MAE: {mae}\")\n",
    "        print(f\"Test Classification AUC: {auc}\")\n",
    "    \n",
    "    return mae, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Regression Loss: 267.98236083984375, Classification AUC: 0.4458195911984758, Learning Rate: 0.0001\n",
      "Test Regression MAE: 267.33221435546875, Test Classification AUC: 0.4251809703502041\n",
      "Test Regression MAE: 2.1461613178253174\n",
      "Test Classification AUC: 0.7360711438224595\n"
     ]
    }
   ],
   "source": [
    "from weather_prediction import WeatherPredictionNetwork\n",
    "# layers=[63, 256, 128, 2], activations=['sigmoid', 'relu'], learning_rate=0.0001, batch_size=256, epochs=1000\n",
    "# layers = [X_train.shape[1] * X_train.shape[2], 512, 512, 2]\n",
    "layers = [X_train.shape[1] * X_train.shape[2], 256, 128, 2]\n",
    "activations = [\"sigmoid\", \"relu\"]\n",
    "model = WeatherPredictionNetwork(layers, activations, binary_output=binary_output, seed=42)\n",
    "\n",
    "if two_models:\n",
    "    model2 = WeatherPredictionNetwork([X_train_2.shape[1], 64, 32, 2], [\"linear\", \"linear\"], binary_output=binary_output, seed=42)\n",
    "    train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=1000, learning_rate=0.0001, rate = [500, 750, 900], \n",
    "                    batch_size=245, binary_output=binary_output, \n",
    "                    model2=model2, X_train2=X_train_2, X_test2=X_test_2, y_train2=y_train_2, y_test2=y_test_2, \n",
    "                    X_2_std=X_2_std, X_2_mean=X_2_mean, standarize_2nd_data=standarize_2nd_data, add_noise=False)\n",
    "else:\n",
    "    # train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=1000, learning_rate=0.0001, rate = [500, 750, 900], \n",
    "    #                 batch_size=245, binary_output=binary_output)\n",
    "    train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=1000, learning_rate=0.0001, rate = [500, 750, 900], \n",
    "                    batch_size=256, binary_output=binary_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import cupy as cp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def hyperparameter_optimization(train_and_evaluate, X_train, y_train, X_test, y_test, binary_output):\n",
    "    # Zakres hiperparametrów do optymalizacji\n",
    "    layer_options = [\n",
    "        [X_train.shape[1] * X_train.shape[2], 128, 64, 2],\n",
    "    ]\n",
    "    activation_options = [\n",
    "        [\"sigmoid\", \"relu\"]\n",
    "    ]\n",
    "    learning_rate_options = [0.001, 0.0001]\n",
    "    batch_size_options = [64, 128, 256, 512]\n",
    "    epochs_options = [100, 200, 500, 1000, 1500, 2000]\n",
    "\n",
    "    # Lista wszystkich kombinacji hiperparametrów\n",
    "    search_space = list(product(layer_options, activation_options, learning_rate_options, batch_size_options, epochs_options))\n",
    "\n",
    "    best_mae = float(\"inf\")\n",
    "    best_auc = 0\n",
    "    best_params = None\n",
    "\n",
    "    # Iteracja po wszystkich kombinacjach hiperparametrów\n",
    "    for layers, activations, learning_rate, batch_size, epochs in search_space:\n",
    "        print(f\"Testing layers={layers}, activations={activations}, learning_rate={learning_rate}, batch_size={batch_size}, epochs={epochs}\")\n",
    "        \n",
    "        # Tworzenie modelu\n",
    "        model = WeatherPredictionNetwork(layers, activations, binary_output=binary_output, seed=42)\n",
    "\n",
    "        # Trening i ewaluacja\n",
    "        mae, auc = train_and_evaluate(\n",
    "            model, X_train, y_train, X_test, y_test,\n",
    "            epochs=epochs, learning_rate=learning_rate, rate=[500, 750, 1000, 1500],\n",
    "            batch_size=batch_size, binary_output=binary_output\n",
    "        )\n",
    "\n",
    "        # Aktualizacja najlepszych hiperparametrów\n",
    "        if mae < best_mae or (mae == best_mae and auc > best_auc):\n",
    "            best_mae = mae\n",
    "            best_auc = auc\n",
    "            best_params = (layers, activations, learning_rate, batch_size, epochs)\n",
    "\n",
    "        print(f\"Finished testing with MAE: {mae}, AUC: {auc}\\n\")\n",
    "\n",
    "    print(f\"Best params: layers={best_params[0]}, activations={best_params[1]}, learning_rate={best_params[2]}, batch_size={best_params[3]}, epochs={best_params[4]}\")\n",
    "    print(f\"Best MAE: {best_mae}, Best AUC: {best_auc}\")\n",
    "\n",
    "    return best_params, best_mae, best_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.001, batch_size=64, epochs=100\n",
      "Epoch 0, Regression Loss: 4.574060916900635, Classification AUC: 0.6006473312337299, Learning Rate: 0.001\n",
      "Test Regression MAE: 4.118930339813232, Test Classification AUC: 0.590231940857816\n",
      "Test Regression MAE: 2.55393123626709\n",
      "Test Classification AUC: 0.7290414036497035\n",
      "Finished testing with MAE: 2.55393123626709, AUC: 0.7290414036497035\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.001, batch_size=64, epochs=200\n",
      "Epoch 0, Regression Loss: 4.574060916900635, Classification AUC: 0.6006473312337299, Learning Rate: 0.001\n",
      "Test Regression MAE: 4.118930339813232, Test Classification AUC: 0.590231940857816\n",
      "Test Regression MAE: 2.2750892639160156\n",
      "Test Classification AUC: 0.7358210157296391\n",
      "Finished testing with MAE: 2.2750892639160156, AUC: 0.7358210157296391\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.001, batch_size=64, epochs=500\n",
      "Epoch 0, Regression Loss: 4.574060916900635, Classification AUC: 0.6006473312337299, Learning Rate: 0.001\n",
      "Test Regression MAE: 4.118930339813232, Test Classification AUC: 0.590231940857816\n",
      "Test Regression MAE: 2.7743027210235596\n",
      "Test Classification AUC: 0.7192807373689514\n",
      "Finished testing with MAE: 2.7743027210235596, AUC: 0.7192807373689514\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.001, batch_size=64, epochs=1000\n",
      "Epoch 0, Regression Loss: 4.574060916900635, Classification AUC: 0.6006473312337299, Learning Rate: 0.001\n",
      "Test Regression MAE: 4.118930339813232, Test Classification AUC: 0.590231940857816\n",
      "Test Regression MAE: 2.5048987865448\n",
      "Test Classification AUC: 0.7222684570832898\n",
      "Finished testing with MAE: 2.5048987865448, AUC: 0.7222684570832898\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.001, batch_size=64, epochs=1500\n",
      "Epoch 0, Regression Loss: 4.574060916900635, Classification AUC: 0.6006473312337299, Learning Rate: 0.001\n",
      "Test Regression MAE: 4.118930339813232, Test Classification AUC: 0.590231940857816\n",
      "Test Regression MAE: 2.50118088722229\n",
      "Test Classification AUC: 0.7223106056787434\n",
      "Finished testing with MAE: 2.50118088722229, AUC: 0.7223106056787434\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.001, batch_size=64, epochs=2000\n",
      "Epoch 0, Regression Loss: 4.574060916900635, Classification AUC: 0.6006473312337299, Learning Rate: 0.001\n",
      "Test Regression MAE: 4.118930339813232, Test Classification AUC: 0.590231940857816\n",
      "Test Regression MAE: 2.501096487045288\n",
      "Test Classification AUC: 0.7223092018309536\n",
      "Finished testing with MAE: 2.501096487045288, AUC: 0.7223092018309536\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.001, batch_size=128, epochs=100\n",
      "Epoch 0, Regression Loss: 3.8202707767486572, Classification AUC: 0.6157449523601044, Learning Rate: 0.001\n",
      "Test Regression MAE: 3.590301513671875, Test Classification AUC: 0.6112271146280852\n",
      "Test Regression MAE: 2.397179365158081\n",
      "Test Classification AUC: 0.7221879845485659\n",
      "Finished testing with MAE: 2.397179365158081, AUC: 0.7221879845485659\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.001, batch_size=128, epochs=200\n",
      "Epoch 0, Regression Loss: 3.8202707767486572, Classification AUC: 0.6157449523601044, Learning Rate: 0.001\n",
      "Test Regression MAE: 3.590301513671875, Test Classification AUC: 0.6112271146280852\n",
      "Test Regression MAE: 2.3326256275177\n",
      "Test Classification AUC: 0.7357485904483877\n",
      "Finished testing with MAE: 2.3326256275177, AUC: 0.7357485904483877\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.001, batch_size=128, epochs=500\n",
      "Epoch 0, Regression Loss: 3.8202707767486572, Classification AUC: 0.6157449523601044, Learning Rate: 0.001\n",
      "Test Regression MAE: 3.590301513671875, Test Classification AUC: 0.6112271146280852\n",
      "Test Regression MAE: 2.613853693008423\n",
      "Test Classification AUC: 0.7336152281803143\n",
      "Finished testing with MAE: 2.613853693008423, AUC: 0.7336152281803143\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.001, batch_size=128, epochs=1000\n",
      "Epoch 0, Regression Loss: 3.8202707767486572, Classification AUC: 0.6157449523601044, Learning Rate: 0.001\n",
      "Test Regression MAE: 3.590301513671875, Test Classification AUC: 0.6112271146280852\n",
      "Test Regression MAE: 2.2619214057922363\n",
      "Test Classification AUC: 0.7330097497339544\n",
      "Finished testing with MAE: 2.2619214057922363, AUC: 0.7330097497339544\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.001, batch_size=128, epochs=1500\n",
      "Epoch 0, Regression Loss: 3.8202707767486572, Classification AUC: 0.6157449523601044, Learning Rate: 0.001\n",
      "Test Regression MAE: 3.590301513671875, Test Classification AUC: 0.6112271146280852\n",
      "Test Regression MAE: 2.263394355773926\n",
      "Test Classification AUC: 0.7330188360558695\n",
      "Finished testing with MAE: 2.263394355773926, AUC: 0.7330188360558695\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.001, batch_size=128, epochs=2000\n",
      "Epoch 0, Regression Loss: 3.8202707767486572, Classification AUC: 0.6157449523601044, Learning Rate: 0.001\n",
      "Test Regression MAE: 3.590301513671875, Test Classification AUC: 0.6112271146280852\n",
      "Test Regression MAE: 2.2630441188812256\n",
      "Test Classification AUC: 0.7330063230188768\n",
      "Finished testing with MAE: 2.2630441188812256, AUC: 0.7330063230188768\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.001, batch_size=256, epochs=100\n",
      "Epoch 0, Regression Loss: 52.831764221191406, Classification AUC: 0.6040574921463344, Learning Rate: 0.001\n",
      "Test Regression MAE: 52.418853759765625, Test Classification AUC: 0.6003573643776918\n",
      "Test Regression MAE: 2.9894473552703857\n",
      "Test Classification AUC: 0.6631945310334814\n",
      "Finished testing with MAE: 2.9894473552703857, AUC: 0.6631945310334814\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.001, batch_size=256, epochs=200\n",
      "Epoch 0, Regression Loss: 52.831764221191406, Classification AUC: 0.6040574921463344, Learning Rate: 0.001\n",
      "Test Regression MAE: 52.418853759765625, Test Classification AUC: 0.6003573643776918\n",
      "Test Regression MAE: 2.9383785724639893\n",
      "Test Classification AUC: 0.6784811843717738\n",
      "Finished testing with MAE: 2.9383785724639893, AUC: 0.6784811843717738\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.001, batch_size=256, epochs=500\n",
      "Epoch 0, Regression Loss: 52.831764221191406, Classification AUC: 0.6040574921463344, Learning Rate: 0.001\n",
      "Test Regression MAE: 52.418853759765625, Test Classification AUC: 0.6003573643776918\n",
      "Test Regression MAE: 2.557860851287842\n",
      "Test Classification AUC: 0.7263140369038201\n",
      "Finished testing with MAE: 2.557860851287842, AUC: 0.7263140369038201\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.001, batch_size=256, epochs=1000\n",
      "Epoch 0, Regression Loss: 52.831764221191406, Classification AUC: 0.6040574921463344, Learning Rate: 0.001\n",
      "Test Regression MAE: 52.418853759765625, Test Classification AUC: 0.6003573643776918\n",
      "Test Regression MAE: 2.2245523929595947\n",
      "Test Classification AUC: 0.7331372677505168\n",
      "Finished testing with MAE: 2.2245523929595947, AUC: 0.7331372677505168\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.001, batch_size=256, epochs=1500\n",
      "Epoch 0, Regression Loss: 52.831764221191406, Classification AUC: 0.6040574921463344, Learning Rate: 0.001\n",
      "Test Regression MAE: 52.418853759765625, Test Classification AUC: 0.6003573643776918\n",
      "Test Regression MAE: 2.2249724864959717\n",
      "Test Classification AUC: 0.733264343610295\n",
      "Finished testing with MAE: 2.2249724864959717, AUC: 0.733264343610295\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.001, batch_size=256, epochs=2000\n",
      "Epoch 0, Regression Loss: 52.831764221191406, Classification AUC: 0.6040574921463344, Learning Rate: 0.001\n",
      "Test Regression MAE: 52.418853759765625, Test Classification AUC: 0.6003573643776918\n",
      "Test Regression MAE: 2.224623203277588\n",
      "Test Classification AUC: 0.7332695942221075\n",
      "Finished testing with MAE: 2.224623203277588, AUC: 0.7332695942221075\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.001, batch_size=512, epochs=100\n",
      "Epoch 0, Regression Loss: 214.6090087890625, Classification AUC: 0.6444690792907918, Learning Rate: 0.001\n",
      "Test Regression MAE: 213.9212646484375, Test Classification AUC: 0.6532152734925416\n",
      "Test Regression MAE: 3.394099473953247\n",
      "Test Classification AUC: 0.6572581451027688\n",
      "Finished testing with MAE: 3.394099473953247, AUC: 0.6572581451027688\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.001, batch_size=512, epochs=200\n",
      "Epoch 0, Regression Loss: 214.6090087890625, Classification AUC: 0.6444690792907918, Learning Rate: 0.001\n",
      "Test Regression MAE: 213.9212646484375, Test Classification AUC: 0.6532152734925416\n",
      "Test Regression MAE: 3.3284692764282227\n",
      "Test Classification AUC: 0.6624524703564828\n",
      "Finished testing with MAE: 3.3284692764282227, AUC: 0.6624524703564828\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.001, batch_size=512, epochs=500\n",
      "Epoch 0, Regression Loss: 214.6090087890625, Classification AUC: 0.6444690792907918, Learning Rate: 0.001\n",
      "Test Regression MAE: 213.9212646484375, Test Classification AUC: 0.6532152734925416\n",
      "Test Regression MAE: 3.099762439727783\n",
      "Test Classification AUC: 0.7285958643661072\n",
      "Finished testing with MAE: 3.099762439727783, AUC: 0.7285958643661072\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.001, batch_size=512, epochs=1000\n",
      "Epoch 0, Regression Loss: 214.6090087890625, Classification AUC: 0.6444690792907918, Learning Rate: 0.001\n",
      "Test Regression MAE: 213.9212646484375, Test Classification AUC: 0.6532152734925416\n",
      "Test Regression MAE: 2.258305072784424\n",
      "Test Classification AUC: 0.7326047451735492\n",
      "Finished testing with MAE: 2.258305072784424, AUC: 0.7326047451735492\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.001, batch_size=512, epochs=1500\n",
      "Epoch 0, Regression Loss: 214.6090087890625, Classification AUC: 0.6444690792907918, Learning Rate: 0.001\n",
      "Test Regression MAE: 213.9212646484375, Test Classification AUC: 0.6532152734925416\n",
      "Test Regression MAE: 2.257842779159546\n",
      "Test Classification AUC: 0.7325910051514803\n",
      "Finished testing with MAE: 2.257842779159546, AUC: 0.7325910051514803\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.001, batch_size=512, epochs=2000\n",
      "Epoch 0, Regression Loss: 214.6090087890625, Classification AUC: 0.6444690792907918, Learning Rate: 0.001\n",
      "Test Regression MAE: 213.9212646484375, Test Classification AUC: 0.6532152734925416\n",
      "Test Regression MAE: 2.258737564086914\n",
      "Test Classification AUC: 0.7325912814994704\n",
      "Finished testing with MAE: 2.258737564086914, AUC: 0.7325912814994704\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.0001, batch_size=64, epochs=100\n",
      "Epoch 0, Regression Loss: 238.66114807128906, Classification AUC: 0.5944442337374828, Learning Rate: 0.0001\n",
      "Test Regression MAE: 238.03094482421875, Test Classification AUC: 0.6063581172822639\n",
      "Test Regression MAE: 2.2104294300079346\n",
      "Test Classification AUC: 0.7251891016134655\n",
      "Finished testing with MAE: 2.2104294300079346, AUC: 0.7251891016134655\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.0001, batch_size=64, epochs=200\n",
      "Epoch 0, Regression Loss: 238.66114807128906, Classification AUC: 0.5944442337374828, Learning Rate: 0.0001\n",
      "Test Regression MAE: 238.03094482421875, Test Classification AUC: 0.6063581172822639\n",
      "Test Regression MAE: 2.189819097518921\n",
      "Test Classification AUC: 0.7322010339217379\n",
      "Finished testing with MAE: 2.189819097518921, AUC: 0.7322010339217379\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.0001, batch_size=64, epochs=500\n",
      "Epoch 0, Regression Loss: 238.66114807128906, Classification AUC: 0.5944442337374828, Learning Rate: 0.0001\n",
      "Test Regression MAE: 238.03094482421875, Test Classification AUC: 0.6063581172822639\n",
      "Test Regression MAE: 2.218623399734497\n",
      "Test Classification AUC: 0.7321891067424842\n",
      "Finished testing with MAE: 2.218623399734497, AUC: 0.7321891067424842\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.0001, batch_size=64, epochs=1000\n",
      "Epoch 0, Regression Loss: 238.66114807128906, Classification AUC: 0.5944442337374828, Learning Rate: 0.0001\n",
      "Test Regression MAE: 238.03094482421875, Test Classification AUC: 0.6063581172822639\n",
      "Test Regression MAE: 2.210683584213257\n",
      "Test Classification AUC: 0.7324132360163937\n",
      "Finished testing with MAE: 2.210683584213257, AUC: 0.7324132360163937\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.0001, batch_size=64, epochs=1500\n",
      "Epoch 0, Regression Loss: 238.66114807128906, Classification AUC: 0.5944442337374828, Learning Rate: 0.0001\n",
      "Test Regression MAE: 238.03094482421875, Test Classification AUC: 0.6063581172822639\n",
      "Test Regression MAE: 2.211313247680664\n",
      "Test Classification AUC: 0.7324275066266037\n",
      "Finished testing with MAE: 2.211313247680664, AUC: 0.7324275066266037\n",
      "\n",
      "Testing layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.0001, batch_size=64, epochs=2000\n",
      "Epoch 0, Regression Loss: 238.66114807128906, Classification AUC: 0.5944442337374828, Learning Rate: 0.0001\n",
      "Test Regression MAE: 238.03094482421875, Test Classification AUC: 0.6063581172822639\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Best params: layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.0001, batch_size=256, epochs=1000\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Best MAE: 2.1387107372283936, Best AUC: 0.7361937317908783\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m best_params, best_mae, best_auc \u001b[38;5;241m=\u001b[39m \u001b[43mhyperparameter_optimization\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbinary_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary_output\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 32\u001b[0m, in \u001b[0;36mhyperparameter_optimization\u001b[1;34m(train_and_evaluate, X_train, y_train, X_test, y_test, binary_output)\u001b[0m\n\u001b[0;32m     29\u001b[0m model \u001b[38;5;241m=\u001b[39m WeatherPredictionNetwork(layers, activations, binary_output\u001b[38;5;241m=\u001b[39mbinary_output, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Trening i ewaluacja\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m mae, auc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m750\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1500\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary_output\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Aktualizacja najlepszych hiperparametrów\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mae \u001b[38;5;241m<\u001b[39m best_mae \u001b[38;5;129;01mor\u001b[39;00m (mae \u001b[38;5;241m==\u001b[39m best_mae \u001b[38;5;129;01mand\u001b[39;00m auc \u001b[38;5;241m>\u001b[39m best_auc):\n",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(model, X_train, y_train, X_test, y_test, epochs, learning_rate, rate, batch_size, binary_output, model2, X_train2, X_test2, y_train2, y_test2, X_2_std, X_2_mean, standarize_2nd_data, add_noise)\u001b[0m\n\u001b[0;32m      6\u001b[0m X_test_cp \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39marray(X_test\u001b[38;5;241m.\u001b[39mreshape(X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mcp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      7\u001b[0m y_test_cp \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39marray(y_test, dtype\u001b[38;5;241m=\u001b[39mcp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_cp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_cp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_cp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_cp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_cp)\n\u001b[0;32m     13\u001b[0m mae \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mmean(cp\u001b[38;5;241m.\u001b[39mabs(predictions[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m y_test_cp[:, \u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\patry\\Documents\\GitHub\\Weather-prediction\\weather_prediction.py:125\u001b[0m, in \u001b[0;36mWeatherPredictionNetwork.train\u001b[1;34m(self, X, y, X_test, y_test, epochs, learning_rate, lower_rate, batch_size)\u001b[0m\n\u001b[0;32m    123\u001b[0m X_batch \u001b[38;5;241m=\u001b[39m X_shuffled[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m    124\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_shuffled[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m--> 125\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward(X_batch, y_batch, output, learning_rate)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# self.clip_weights()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\patry\\Documents\\GitHub\\Weather-prediction\\weather_prediction.py:51\u001b[0m, in \u001b[0;36mWeatherPredictionNetwork.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_values \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m---> 51\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivations_values\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases[i]\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_values\u001b[38;5;241m.\u001b[39mappend(z)\n\u001b[0;32m     53\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_activation(z, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations[i])\n",
      "File \u001b[1;32mc:\\Users\\patry\\anaconda3\\envs\\weather\\lib\\site-packages\\cupy\\linalg\\_product.py:63\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(a, b, out)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a dot product of two arrays.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03mFor arrays with more than one axis, it computes the dot product along the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# TODO(okuta): check type\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Best params: layers=[63, 128, 64, 2], activations=['sigmoid', 'relu'], learning_rate=0.0001, batch_size=256, epochs=1000\n",
    "# Best MAE: 2.1387107372283936, Best AUC: 0.7361937317908783\n",
    "\n",
    "best_params, best_mae, best_auc = hyperparameter_optimization(\n",
    "    train_and_evaluate,\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    binary_output=binary_output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scikit-optimize\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "def bayesian_optimization(train_and_evaluate, X_train, y_train, X_test, y_test, binary_output):\n",
    "    # Przestrzeń wyszukiwania dla hiperparametrów\n",
    "    space = [\n",
    "        Categorical([\n",
    "            (X_train.shape[1] * X_train.shape[2], 256, 128, 2),\n",
    "            (X_train.shape[1] * X_train.shape[2], 512, 512, 2),\n",
    "            (X_train.shape[1] * X_train.shape[2], 1024, 512, 2)\n",
    "        ], name=\"layers\"),\n",
    "        Categorical([(\"sigmoid\", \"relu\"), (\"relu\", \"relu\"), (\"tanh\", \"relu\")], name=\"activations\"),\n",
    "        Real(1e-4, 1e-2, prior=\"log-uniform\", name=\"learning_rate\"),\n",
    "        Integer(32, 256, name=\"batch_size\"),\n",
    "        Integer(500, 1000, name=\"epochs\")\n",
    "    ]\n",
    "\n",
    "    # Funkcja celu dla optymalizacji\n",
    "    @use_named_args(space)\n",
    "    def objective(layers, activations, learning_rate, batch_size, epochs):\n",
    "        print(f\"Testing: layers={layers}, activations={activations}, learning_rate={learning_rate}, batch_size={batch_size}, epochs={epochs}\")\n",
    "\n",
    "        # Tworzenie modelu\n",
    "        model = WeatherPredictionNetwork(list(layers), list(activations), binary_output=binary_output, seed=42)\n",
    "\n",
    "        # Trening i ewaluacja\n",
    "        mae, auc = train_and_evaluate(\n",
    "            model, X_train, y_train, X_test, y_test,\n",
    "            epochs=epochs, learning_rate=learning_rate, rate=[500, 750, 900],\n",
    "            batch_size=batch_size, binary_output=binary_output\n",
    "        )\n",
    "\n",
    "        print(f\"MAE: {mae}, AUC: {auc}\")\n",
    "\n",
    "        # Zwróć MAE jako wartość skalarną\n",
    "        return float(mae)\n",
    "\n",
    "    # Optymalizacja przy użyciu Bayesian Optimization\n",
    "    result = gp_minimize(\n",
    "        func=objective,\n",
    "        dimensions=space,\n",
    "        n_calls=20,  # Liczba iteracji optymalizacji\n",
    "        random_state=42,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Najlepsze hiperparametry\n",
    "    best_params = result.x\n",
    "    print(f\"Best parameters: layers={best_params[0]}, activations={best_params[1]}, learning_rate={best_params[2]}, batch_size={best_params[3]}, epochs={best_params[4]}\")\n",
    "    print(f\"Best MAE: {result.fun}\")\n",
    "\n",
    "    return best_params, result.fun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params, best_mae = bayesian_optimization(\n",
    "    train_and_evaluate,\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    binary_output=binary_output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_majority_class_percentage(y, binary_output=True):\n",
    "    if binary_output:\n",
    "        binary_labels = y[:, 1]\n",
    "    else:\n",
    "        binary_labels = y[:, 1] >=6\n",
    "    \n",
    "    unique, counts = np.unique(binary_labels, return_counts=True)\n",
    "    class_counts = dict(zip(unique, counts))\n",
    "    \n",
    "    majority_class_count = max(class_counts.values())\n",
    "    total_samples = len(binary_labels)\n",
    "    majority_class_percentage = (majority_class_count / total_samples) * 100\n",
    "    \n",
    "    return majority_class_percentage, class_counts\n",
    "\n",
    "majority_percentage_train, train_class_counts = compute_majority_class_percentage(y_train, binary_output=binary_output)\n",
    "majority_percentage_test, test_class_counts = compute_majority_class_percentage(y_test, binary_output=binary_output)\n",
    "\n",
    "print(f\"Majority class percentage in training data: {majority_percentage_train:.2f}%\")\n",
    "print(f\"Class distribution in training data: {train_class_counts}\")\n",
    "print(f\"Majority class percentage in test data: {majority_percentage_test:.2f}%\")\n",
    "print(f\"Class distribution in test data: {test_class_counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7752,
     "sourceId": 11375,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "weather",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
