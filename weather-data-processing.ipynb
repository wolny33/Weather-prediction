{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install numpy\n",
    "# %pip install scikit-learn\n",
    "# %pip install kagglehub\n",
    "# %pip install cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:24.574362Z",
     "iopub.status.busy": "2024-12-29T16:54:24.573960Z",
     "iopub.status.idle": "2024-12-29T16:54:25.867960Z",
     "shell.execute_reply": "2024-12-29T16:54:25.866721Z",
     "shell.execute_reply.started": "2024-12-29T16:54:24.574325Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\patry\\anaconda3\\envs\\weather\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "import cupy as cp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:25.869990Z",
     "iopub.status.busy": "2024-12-29T16:54:25.869419Z",
     "iopub.status.idle": "2024-12-29T16:54:29.158623Z",
     "shell.execute_reply": "2024-12-29T16:54:29.157504Z",
     "shell.execute_reply.started": "2024-12-29T16:54:25.869950Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "historical_hourly_weather_data_path = kagglehub.dataset_download('selfishgene/historical-hourly-weather-data')\n",
    "\n",
    "city = \"Portland\"\n",
    "\n",
    "city_attributes = pd.read_csv(f\"{historical_hourly_weather_data_path}/city_attributes.csv\")\n",
    "humidity = pd.read_csv(f\"{historical_hourly_weather_data_path}/humidity.csv\")\n",
    "pressure = pd.read_csv(f\"{historical_hourly_weather_data_path}/pressure.csv\")\n",
    "temperature = pd.read_csv(f\"{historical_hourly_weather_data_path}/temperature.csv\")\n",
    "weather_description = pd.read_csv(f\"{historical_hourly_weather_data_path}/weather_description.csv\")\n",
    "wind_speed = pd.read_csv(f\"{historical_hourly_weather_data_path}/wind_speed.csv\")\n",
    "wind_direction = pd.read_csv(f\"{historical_hourly_weather_data_path}/wind_direction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patry\\AppData\\Local\\Temp\\ipykernel_25192\\263827200.py:30: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  combined_data = combined_data.ffill().bfill().interpolate()\n"
     ]
    }
   ],
   "source": [
    "historical_hourly_weather_data_path = kagglehub.dataset_download('selfishgene/historical-hourly-weather-data')\n",
    "\n",
    "city_attributes = pd.read_csv(f\"{historical_hourly_weather_data_path}/city_attributes.csv\")\n",
    "humidity = pd.read_csv(f\"{historical_hourly_weather_data_path}/humidity.csv\")\n",
    "pressure = pd.read_csv(f\"{historical_hourly_weather_data_path}/pressure.csv\")\n",
    "temperature = pd.read_csv(f\"{historical_hourly_weather_data_path}/temperature.csv\")\n",
    "weather_description = pd.read_csv(f\"{historical_hourly_weather_data_path}/weather_description.csv\")\n",
    "wind_speed = pd.read_csv(f\"{historical_hourly_weather_data_path}/wind_speed.csv\")\n",
    "wind_direction = pd.read_csv(f\"{historical_hourly_weather_data_path}/wind_direction.csv\")\n",
    "\n",
    "data_frames = []\n",
    "for city in city_attributes['City']:\n",
    "    city_data = pd.DataFrame({\n",
    "        'datetime': pd.to_datetime(humidity['datetime']),\n",
    "        'humidity': humidity[city],\n",
    "        'pressure': pressure[city],\n",
    "        'temperature': temperature[city],\n",
    "        'weather_description': weather_description[city],\n",
    "        'wind_speed': wind_speed[city],\n",
    "        'wind_direction': wind_direction[city],\n",
    "        'latitude': city_attributes.loc[city_attributes['City'] == city, 'Latitude'].values[0],\n",
    "        'longitude': city_attributes.loc[city_attributes['City'] == city, 'Longitude'].values[0],\n",
    "        'city': city\n",
    "    })\n",
    "    city_data.set_index('datetime', inplace=True)\n",
    "    data_frames.append(city_data)\n",
    "\n",
    "combined_data = pd.concat(data_frames)\n",
    "\n",
    "combined_data = combined_data.ffill().bfill().interpolate()\n",
    "\n",
    "aggregated_data = combined_data.groupby(['city']).resample('D').agg({\n",
    "    'temperature': 'mean',\n",
    "    'humidity': 'mean',\n",
    "    'wind_speed': ['max', 'mean'],\n",
    "    'pressure': 'mean',\n",
    "    'weather_description': lambda x: x.mode()[0] if not x.mode().empty else np.nan,\n",
    "    'wind_direction': 'mean',\n",
    "    'latitude': 'mean',\n",
    "    'longitude': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "aggregated_data.columns = [\n",
    "    '_'.join(col).strip('_') if isinstance(col, tuple) else col for col in aggregated_data.columns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_weather_description = pd.get_dummies(\n",
    "    aggregated_data['weather_description_<lambda>'],\n",
    "    prefix='weather_desc',\n",
    "    drop_first=False\n",
    ")\n",
    "\n",
    "aggregated_data = pd.concat([aggregated_data, one_hot_weather_description], axis=1)\n",
    "aggregated_data.drop(columns=['weather_description_<lambda>'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_output = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_weather_data(data, binary_output=True, window_size=3):\n",
    "    X, y = [], []\n",
    "    for i in range(window_size, len(data) - 1):\n",
    "        X_window = data.iloc[i-window_size:i][[\n",
    "            'temperature_mean', 'humidity_mean', 'pressure_mean', 'wind_speed_max', 'wind_speed_mean', 'wind_direction_mean'\n",
    "        ]].values\n",
    "        y_target = data.iloc[i + 1][['temperature_mean', 'wind_speed_max']].values\n",
    "\n",
    "        if binary_output:\n",
    "            # encode wind speed > 6 as binary\n",
    "            y_target[1] = 1 if y_target[1] >= 6 else 0\n",
    "        X.append(X_window)\n",
    "        y.append(y_target)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # split into train/test (0.7 or 0.8)\n",
    "    train_size = int(0.8 * len(X))\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    X_mean = X_train.mean(axis=(0, 1), keepdims=True)\n",
    "    X_std = X_train.std(axis=(0, 1), keepdims=True)\n",
    "    X_train = (X_train - X_mean) / (X_std + 1e-9)\n",
    "    X_test = (X_test - X_mean) / (X_std + 1e-9)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_weather_data(aggregated_data, binary_output=binary_output, window_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54342, 3, 6)\n",
      "(54342, 2)\n",
      "(13586, 3, 6)\n",
      "(13586, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=1000, learning_rate=0.001, rate = [500], binary_output=True):\n",
    "    X_train_cp = cp.array(X_train.reshape(X_train.shape[0], -1), dtype=cp.float32)\n",
    "    y_train_cp = cp.array(y_train, dtype=cp.float32)\n",
    "    X_test_cp = cp.array(X_test.reshape(X_test.shape[0], -1), dtype=cp.float32)\n",
    "    y_test_cp = cp.array(y_test, dtype=cp.float32)\n",
    "\n",
    "    model.train(X_train_cp, y_train_cp, X_test_cp, y_test_cp, epochs, learning_rate, rate, batch_size=256)\n",
    "\n",
    "    predictions = model.predict(X_test_cp)\n",
    "    \n",
    "    mae = cp.mean(cp.abs(predictions[:, 0] - y_test_cp[:, 0]))\n",
    "\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    if binary_output:\n",
    "        auc = roc_auc_score(cp.asnumpy(y_test_cp[:, 1]), cp.asnumpy(predictions[:, 1]))\n",
    "    else:\n",
    "        auc = roc_auc_score((cp.asnumpy(y_test_cp[:, 1]) >= 6), cp.asnumpy(predictions[:, 1]))\n",
    "\n",
    "    print(f\"Test Regression MAE: {mae}\")\n",
    "    print(f\"Test Classification AUC: {auc}\")\n",
    "\n",
    "    # print(\"Predictions:\" , predictions[:5, :])\n",
    "    # print(\"True values:\", y_test_cp[:5, :])\n",
    "\n",
    "    return mae, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Regression Loss: 203.7012939453125, Classification AUC: 0.6260297550227674, Learning Rate: 0.0001\n",
      "Test Regression MAE: 203.1527557373047, Test Classification AUC: 0.6383322650825426\n",
      "Epoch 10, Regression Loss: 2.8277246952056885, Classification AUC: 0.7156746827566846, Learning Rate: 0.0001\n",
      "Epoch 20, Regression Loss: 2.953594207763672, Classification AUC: 0.7246373631478213, Learning Rate: 0.0001\n",
      "Epoch 30, Regression Loss: 2.8154170513153076, Classification AUC: 0.729930595545895, Learning Rate: 0.0001\n",
      "Epoch 40, Regression Loss: 2.85482120513916, Classification AUC: 0.7306526462446785, Learning Rate: 0.0001\n",
      "Epoch 50, Regression Loss: 2.877514123916626, Classification AUC: 0.7322341861717538, Learning Rate: 0.0001\n",
      "Epoch 60, Regression Loss: 2.8303844928741455, Classification AUC: 0.7320802965553279, Learning Rate: 0.0001\n",
      "Epoch 70, Regression Loss: 2.779317617416382, Classification AUC: 0.7320862727710451, Learning Rate: 0.0001\n",
      "Epoch 80, Regression Loss: 2.9619429111480713, Classification AUC: 0.732242972500864, Learning Rate: 0.0001\n",
      "Epoch 90, Regression Loss: 2.7683589458465576, Classification AUC: 0.734158909049353, Learning Rate: 0.0001\n",
      "Epoch 100, Regression Loss: 2.768601417541504, Classification AUC: 0.735299760795231, Learning Rate: 0.0001\n",
      "Test Regression MAE: 2.1529645919799805, Test Classification AUC: 0.7316082011594058\n",
      "Epoch 110, Regression Loss: 2.761087656021118, Classification AUC: 0.7368876775973796, Learning Rate: 0.0001\n",
      "Epoch 120, Regression Loss: 2.8392536640167236, Classification AUC: 0.7377938591035288, Learning Rate: 0.0001\n",
      "Epoch 130, Regression Loss: 2.8244307041168213, Classification AUC: 0.7381273485716828, Learning Rate: 0.0001\n",
      "Epoch 140, Regression Loss: 2.7458441257476807, Classification AUC: 0.7390745629563757, Learning Rate: 0.0001\n",
      "Epoch 150, Regression Loss: 2.859165668487549, Classification AUC: 0.7398804277439839, Learning Rate: 0.0001\n",
      "Epoch 160, Regression Loss: 2.746389150619507, Classification AUC: 0.7403224395442554, Learning Rate: 0.0001\n",
      "Epoch 170, Regression Loss: 2.900777816772461, Classification AUC: 0.740543844729273, Learning Rate: 0.0001\n",
      "Epoch 180, Regression Loss: 2.815768003463745, Classification AUC: 0.7408242478635634, Learning Rate: 0.0001\n",
      "Epoch 190, Regression Loss: 2.731919765472412, Classification AUC: 0.7416863156062722, Learning Rate: 0.0001\n",
      "Epoch 200, Regression Loss: 2.742581367492676, Classification AUC: 0.7417347486562559, Learning Rate: 0.0001\n",
      "Test Regression MAE: 2.1673362255096436, Test Classification AUC: 0.734448141022546\n",
      "Epoch 210, Regression Loss: 2.738077163696289, Classification AUC: 0.742492199636525, Learning Rate: 0.0001\n",
      "Epoch 220, Regression Loss: 2.8239595890045166, Classification AUC: 0.7426488890577843, Learning Rate: 0.0001\n",
      "Epoch 230, Regression Loss: 2.726635217666626, Classification AUC: 0.7427134728719776, Learning Rate: 0.0001\n",
      "Epoch 240, Regression Loss: 2.7163140773773193, Classification AUC: 0.7426722963608343, Learning Rate: 0.0001\n",
      "Epoch 250, Regression Loss: 2.731137990951538, Classification AUC: 0.7435731242882764, Learning Rate: 0.0001\n",
      "Epoch 260, Regression Loss: 2.7740139961242676, Classification AUC: 0.7435759055377023, Learning Rate: 0.0001\n",
      "Epoch 270, Regression Loss: 2.721290349960327, Classification AUC: 0.743887821939208, Learning Rate: 0.0001\n",
      "Epoch 280, Regression Loss: 2.7274978160858154, Classification AUC: 0.744017865795175, Learning Rate: 0.0001\n",
      "Epoch 290, Regression Loss: 2.7165021896362305, Classification AUC: 0.7445632779478333, Learning Rate: 0.0001\n",
      "Epoch 300, Regression Loss: 2.7953643798828125, Classification AUC: 0.7449910907378783, Learning Rate: 0.0001\n",
      "Test Regression MAE: 2.245553731918335, Test Classification AUC: 0.7358182743575771\n",
      "Epoch 310, Regression Loss: 2.8079116344451904, Classification AUC: 0.7449395458773188, Learning Rate: 0.0001\n",
      "Epoch 320, Regression Loss: 2.7196061611175537, Classification AUC: 0.744972519523835, Learning Rate: 0.0001\n",
      "Epoch 330, Regression Loss: 2.8699400424957275, Classification AUC: 0.7453376136617677, Learning Rate: 0.0001\n",
      "Epoch 340, Regression Loss: 2.7019646167755127, Classification AUC: 0.7460975380091442, Learning Rate: 0.0001\n",
      "Epoch 350, Regression Loss: 2.7349467277526855, Classification AUC: 0.745967595177063, Learning Rate: 0.0001\n",
      "Epoch 360, Regression Loss: 2.7298178672790527, Classification AUC: 0.7462004002547111, Learning Rate: 0.0001\n",
      "Epoch 370, Regression Loss: 2.7143452167510986, Classification AUC: 0.746359548611094, Learning Rate: 0.0001\n",
      "Epoch 380, Regression Loss: 2.7104508876800537, Classification AUC: 0.7469814935731929, Learning Rate: 0.0001\n",
      "Epoch 390, Regression Loss: 2.7288854122161865, Classification AUC: 0.7471225765221985, Learning Rate: 0.0001\n",
      "Epoch 400, Regression Loss: 2.6915698051452637, Classification AUC: 0.7477677514801195, Learning Rate: 0.0001\n",
      "Test Regression MAE: 2.1462349891662598, Test Classification AUC: 0.7386192602080295\n",
      "Epoch 410, Regression Loss: 2.754894256591797, Classification AUC: 0.7472951809852673, Learning Rate: 0.0001\n",
      "Epoch 420, Regression Loss: 2.6965456008911133, Classification AUC: 0.7477614955588141, Learning Rate: 0.0001\n",
      "Epoch 430, Regression Loss: 2.7002358436584473, Classification AUC: 0.747802109222594, Learning Rate: 0.0001\n",
      "Epoch 440, Regression Loss: 2.7656140327453613, Classification AUC: 0.7484726638546675, Learning Rate: 0.0001\n",
      "Epoch 450, Regression Loss: 2.6762144565582275, Classification AUC: 0.7489471256266188, Learning Rate: 0.0001\n",
      "Epoch 460, Regression Loss: 2.693584680557251, Classification AUC: 0.748985528447947, Learning Rate: 0.0001\n",
      "Epoch 470, Regression Loss: 2.691985845565796, Classification AUC: 0.749448685853255, Learning Rate: 0.0001\n",
      "Epoch 480, Regression Loss: 2.67966365814209, Classification AUC: 0.748820698013419, Learning Rate: 0.0001\n",
      "Epoch 490, Regression Loss: 2.778954267501831, Classification AUC: 0.7497867433878616, Learning Rate: 0.0001\n",
      "Epoch 500, Regression Loss: 2.6658437252044678, Classification AUC: 0.7499488351816774, Learning Rate: 1e-05\n",
      "Test Regression MAE: 2.144928216934204, Test Classification AUC: 0.7401136948690181\n",
      "Epoch 510, Regression Loss: 2.665415048599243, Classification AUC: 0.7501408877736072, Learning Rate: 1e-05\n",
      "Epoch 520, Regression Loss: 2.6745996475219727, Classification AUC: 0.7502524621266539, Learning Rate: 1e-05\n",
      "Epoch 530, Regression Loss: 2.6656079292297363, Classification AUC: 0.7502393860622049, Learning Rate: 1e-05\n",
      "Epoch 540, Regression Loss: 2.662959337234497, Classification AUC: 0.7503654363821171, Learning Rate: 1e-05\n",
      "Epoch 550, Regression Loss: 2.673764944076538, Classification AUC: 0.7504275220886596, Learning Rate: 1e-05\n",
      "Epoch 560, Regression Loss: 2.6622605323791504, Classification AUC: 0.7504277942346375, Learning Rate: 1e-05\n",
      "Epoch 570, Regression Loss: 2.6620724201202393, Classification AUC: 0.7504246645558914, Learning Rate: 1e-05\n",
      "Epoch 580, Regression Loss: 2.6710264682769775, Classification AUC: 0.7505209808660046, Learning Rate: 1e-05\n",
      "Epoch 590, Regression Loss: 2.661040782928467, Classification AUC: 0.750628314277562, Learning Rate: 1e-05\n",
      "Epoch 600, Regression Loss: 2.66372013092041, Classification AUC: 0.7506220775989014, Learning Rate: 1e-05\n",
      "Test Regression MAE: 2.1414635181427, Test Classification AUC: 0.7401920782129368\n",
      "Epoch 610, Regression Loss: 2.6637299060821533, Classification AUC: 0.7507449975528304, Learning Rate: 1e-05\n",
      "Epoch 620, Regression Loss: 2.6660401821136475, Classification AUC: 0.7507848126468468, Learning Rate: 1e-05\n",
      "Epoch 630, Regression Loss: 2.6692581176757812, Classification AUC: 0.7507980941952538, Learning Rate: 1e-05\n",
      "Epoch 640, Regression Loss: 2.6588568687438965, Classification AUC: 0.7508600005328563, Learning Rate: 1e-05\n",
      "Epoch 650, Regression Loss: 2.6637649536132812, Classification AUC: 0.7508978982347556, Learning Rate: 1e-05\n",
      "Epoch 660, Regression Loss: 2.6609442234039307, Classification AUC: 0.7509421886181866, Learning Rate: 1e-05\n",
      "Epoch 670, Regression Loss: 2.6736998558044434, Classification AUC: 0.7509528456072764, Learning Rate: 1e-05\n",
      "Epoch 680, Regression Loss: 2.6804537773132324, Classification AUC: 0.7510027816452745, Learning Rate: 1e-05\n",
      "Epoch 690, Regression Loss: 2.677518129348755, Classification AUC: 0.7510687873534786, Learning Rate: 1e-05\n",
      "Epoch 700, Regression Loss: 2.6568455696105957, Classification AUC: 0.7510989742526704, Learning Rate: 1e-05\n",
      "Test Regression MAE: 2.1386125087738037, Test Classification AUC: 0.7401290377094297\n",
      "Epoch 710, Regression Loss: 2.6579980850219727, Classification AUC: 0.7511269104496459, Learning Rate: 1e-05\n",
      "Epoch 720, Regression Loss: 2.6662564277648926, Classification AUC: 0.7511757908911234, Learning Rate: 1e-05\n",
      "Epoch 730, Regression Loss: 2.659841299057007, Classification AUC: 0.7511904888356429, Learning Rate: 1e-05\n",
      "Epoch 740, Regression Loss: 2.655812978744507, Classification AUC: 0.7512004737066356, Learning Rate: 1e-05\n",
      "Epoch 750, Regression Loss: 2.6634836196899414, Classification AUC: 0.7511773021259858, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 760, Regression Loss: 2.659684181213379, Classification AUC: 0.7511875261555652, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 770, Regression Loss: 2.6614301204681396, Classification AUC: 0.7511977824852984, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 780, Regression Loss: 2.6601064205169678, Classification AUC: 0.7512128501634947, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 790, Regression Loss: 2.6625869274139404, Classification AUC: 0.7512361207063185, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 800, Regression Loss: 2.659137487411499, Classification AUC: 0.7512452575264609, Learning Rate: 1.0000000000000002e-06\n",
      "Test Regression MAE: 2.145785331726074, Test Classification AUC: 0.7403772645280781\n",
      "Epoch 810, Regression Loss: 2.6607892513275146, Classification AUC: 0.7512573192286285, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 820, Regression Loss: 2.6584384441375732, Classification AUC: 0.7512666684051021, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 830, Regression Loss: 2.659662961959839, Classification AUC: 0.7512617876456701, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 840, Regression Loss: 2.6604983806610107, Classification AUC: 0.7512530652296303, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 850, Regression Loss: 2.6622190475463867, Classification AUC: 0.7512459543851013, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 860, Regression Loss: 2.6579644680023193, Classification AUC: 0.7512530315550018, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 870, Regression Loss: 2.6587913036346436, Classification AUC: 0.7512483466582052, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 880, Regression Loss: 2.6639959812164307, Classification AUC: 0.751261757407228, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 890, Regression Loss: 2.6580538749694824, Classification AUC: 0.7512640252903773, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 900, Regression Loss: 2.65946888923645, Classification AUC: 0.751272909894424, Learning Rate: 1.0000000000000002e-07\n",
      "Test Regression MAE: 2.1458280086517334, Test Classification AUC: 0.7403575885511813\n",
      "Epoch 910, Regression Loss: 2.65903377532959, Classification AUC: 0.7512737943688521, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 920, Regression Loss: 2.6597788333892822, Classification AUC: 0.7512729951118513, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 930, Regression Loss: 2.658566951751709, Classification AUC: 0.7512736459255915, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 940, Regression Loss: 2.6591689586639404, Classification AUC: 0.7512738747756185, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 950, Regression Loss: 2.6587777137756348, Classification AUC: 0.7512732280853023, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 960, Regression Loss: 2.6588430404663086, Classification AUC: 0.7512740177209806, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 970, Regression Loss: 2.6587703227996826, Classification AUC: 0.7512746444814145, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 980, Regression Loss: 2.6591544151306152, Classification AUC: 0.7512754265574824, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 990, Regression Loss: 2.6589815616607666, Classification AUC: 0.7512750465152456, Learning Rate: 1.0000000000000002e-07\n",
      "Test Regression MAE: 2.144796848297119\n",
      "Test Classification AUC: 0.7403726882053617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(2.1447968, dtype=float32), np.float64(0.7403726882053617))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from weather_prediction import WeatherPredictionNetwork\n",
    "\n",
    "layers = [X_train.shape[1] * X_train.shape[2],512, 512, 2]\n",
    "activations = [\"sigmoid\", \"relu\"]\n",
    "model = WeatherPredictionNetwork(layers, activations, binary_output=binary_output, seed=42)\n",
    "\n",
    "train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=1000, learning_rate=0.0001, rate = [500, 750, 900], binary_output=binary_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority class percentage in training data: 56.02%\n",
      "Class distribution in training data: {0: np.int64(30444), 1: np.int64(23898)}\n",
      "Majority class percentage in test data: 57.03%\n",
      "Class distribution in test data: {0: np.int64(7748), 1: np.int64(5838)}\n"
     ]
    }
   ],
   "source": [
    "def compute_majority_class_percentage(y, binary_output=True):\n",
    "    if binary_output:\n",
    "        binary_labels = y[:, 1]\n",
    "    else:\n",
    "        binary_labels = y[:, 1] >=6\n",
    "    \n",
    "    unique, counts = np.unique(binary_labels, return_counts=True)\n",
    "    class_counts = dict(zip(unique, counts))\n",
    "    \n",
    "    majority_class_count = max(class_counts.values())\n",
    "    total_samples = len(binary_labels)\n",
    "    majority_class_percentage = (majority_class_count / total_samples) * 100\n",
    "    \n",
    "    return majority_class_percentage, class_counts\n",
    "\n",
    "majority_percentage_train, train_class_counts = compute_majority_class_percentage(y_train, binary_output=binary_output)\n",
    "majority_percentage_test, test_class_counts = compute_majority_class_percentage(y_test, binary_output=binary_output)\n",
    "\n",
    "print(f\"Majority class percentage in training data: {majority_percentage_train:.2f}%\")\n",
    "print(f\"Class distribution in training data: {train_class_counts}\")\n",
    "print(f\"Majority class percentage in test data: {majority_percentage_test:.2f}%\")\n",
    "print(f\"Class distribution in test data: {test_class_counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7752,
     "sourceId": 11375,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "weather",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
